{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhGuhbZ6M5tl"
   },
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 14 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 13 April 2020</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/keras-hellboy.png\" width=250 />\n",
    "</left>\n",
    "\n",
    "Minimalist, modular, Open Source Neural Networks API library, written in Python and capable of running on top of either TensorFlow or Theano. Written by [François Chollet](https://fchollet.com/) and adopted by Google's brain team and a number of other frameworks for its simplicity.\n",
    "\n",
    "- Developed with a focus on going from idea to result with the least possible delay\n",
    "- Fast prototyping (modularity, minimalism, extensibility)\n",
    "- Supports both convolutional networks and recurrent networks, as well as combinations\n",
    "- Supports arbitrary connectivity schemes (including multi-input and multi-output training)\n",
    "- Runs seamlessly on CPU and GPU\n",
    "- Initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System)\n",
    "\n",
    "### References\n",
    "- http://keras.io/ \n",
    "- http://keras.io/documentation/ \n",
    "- http://robotfuture.net \n",
    "- https://github.com/fchollet/keras \n",
    "\n",
    "### Install (already installed if you have tensorflow)\n",
    "```(python)\n",
    "pip install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras (κέρας) means [horn](https://en.wikipedia.org/wiki/Horn_(anatomy)) in Greek, and it's a play on the greek words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/odyssey.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "Reference to a literary image from ancient Greek literature, first found in the [Odyssey](https://en.wikipedia.org/wiki/Odyssey), where dream spirits (Oneiroi, singular [Oneiros](https://en.wikipedia.org/wiki/Oneiros)) are divided between those who deceive with false visions and arrive to Earth through a gate of ivory ([blue pill](https://en.wikipedia.org/wiki/Red_pill_and_blue_pill) in [the Matrix®](https://en.wikipedia.org/wiki/The_Matrix)), and those who announce a future that will come to pass and arrive through a gate of horn ([red pill](https://en.wikipedia.org/wiki/Red_pill_and_blue_pill) in [the Matrix®](https://en.wikipedia.org/wiki/The_Matrix)).\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/odyssey3.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "These spirits reappear in [Harry Potter](https://en.wikipedia.org/wiki/Magical_creatures_in_Harry_Potter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homer's Odyssey\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/odyssey2.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    ">*Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them*. \n",
    "- [Homer](https://en.wikipedia.org/wiki/Homer), [Odyssey](https://en.wikipedia.org/wiki/Odyssey) 19 ([Shewring](https://en.wikipedia.org/wiki/English_translations_of_Homer) translation)\n",
    "\n",
    "The Greek [Odyssey](https://en.wikipedia.org/wiki/Odyssey) (circa 800 BC) is an epic poem alike India's [Bhagavad Gita](https://en.wikipedia.org/wiki/Bhagavad_Gita) or the [Mahabharata](https://en.wikipedia.org/wiki/Mahabharata), a Sanskrit epic narrating the struggle between two groups of cousins, the Kaurava and the Pāṇḍava princes (circa 400 BC), and like China's [Hēi Àn Zhuàn](https://en.wikipedia.org/wiki/Epic_of_Darkness) or 黑暗传, myths relating to the creation of the world from the birth of Pangu, the first living being and the creator of all in some versions of Chinese mythology, written in the [Tang](https://en.wikipedia.org/wiki/Tang_dynasty) Dynasty (600 to 900 AD). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras by [François Chollet](https://www.linkedin.com/in/fchollet) is now the main API of Tensorflow 2.0.\n",
    "\n",
    "We will use Keras to showcase how neural network build models: Going from ***bunny clouds*** to ***bunny meshes***. For simplicity, we will start with a *single* neuron and 1D data.\n",
    "\n",
    "# Activation functions recap\n",
    "\n",
    "Let's start with a recap on activation functions, in 1 and 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linear activation functions help us build classifiers that are impossible to build with using linear brains like our caterpilar one. This single fact is one of the major factors for the dark ages in AI from the 1960s to the late 1990s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaviside(z):\n",
    "    return (z >= 0).astype(z.dtype)\n",
    "\n",
    "def mlp_xor(x1, x2, activation=heaviside):\n",
    "    return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1s = np.linspace(-0.2, 1.2, 100)\n",
    "x2s = np.linspace(-0.2, 1.2, 100)\n",
    "x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "z1 = mlp_xor(x1, x2, activation=heaviside)\n",
    "z2 = mlp_xor(x1, x2, activation=sigmoid)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contourf(x1, x2, z1)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: heaviside\", fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contourf(x1, x2, z2)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: sigmoid\", fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Neuron\n",
    "\n",
    "Let's build ourselves a neuron. Let's start with some ridiculously simple data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 200\n",
    "x = np.linspace(0, 2, n_points)\n",
    "y = np.array([0] * int(n_points / 2) + list(x[:int(n_points / 2)])) * 2\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(x, y, linewidth=2)\n",
    "plt.title('ridiculously simple data')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE**: The data we're going to model with in this notebook consists of **point-clouds** (`x = np.linspace(0, 2, n_points)`). It is ***not*** continuous, even though it *looks* continuous when we plot it. The model however (the mesh bunny), ***will be*** continuous!\n",
    "\n",
    "Before **training** a model, you **configure** the learning process with 3 parameters. This is called **compiling** a model. In computer programming, compilation is the translation of **source code** into **object code** by a **compiler**. We also did this with `conx`.\n",
    "\n",
    "These are the 3 parameters:\n",
    "- Optimizer: This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class \n",
    "- Loss function: This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function\n",
    "- List of metrics: For any classification problem you will want to set this to metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=1, input_dim=1, init=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print initial weigths\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net initialized with weigths w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models are trained on NumPy arrays of input **data** and **labels**.\n",
    "\n",
    "This is an artificial **neuron** with **many** inputs and outputs, and then one with only **one** input and output:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/neuron-one-many.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "Let's save our training history to ***see*** how Keras trains. Just to pacify Elon Musk who thinks that Neural nets are the work of the [devil](https://www.vox.com/future-perfect/2018/11/2/18053418/elon-musk-artificial-intelligence-google-deepmind-openai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.predictions = []\n",
    "        self.i = 0\n",
    "        self.save_every = 50\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.i += 1        \n",
    "        if self.i % self.save_every == 0:        \n",
    "            pred = model.predict(X_train)\n",
    "            self.predictions.append(pred)\n",
    "            \n",
    "history = TrainingHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training API is reminiscent of `sklearn`'s training API. Let's train our **single** neuron on our ***ridiculously simple data*** that we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "# print trained weights\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net weigths after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "To build animations:\n",
    "- Download ffmpeg (install at C:\\ffmpeg)\n",
    "\n",
    ">For Windows: https://ffmpeg.zeranoe.com/builds/\n",
    "Directions: http://www.wikihow.com/Install-FFmpeg-on-Windows \n",
    "\n",
    ">For OSX: http://www.renevolution.com/ffmpeg/2013/03/16/how-to-install-ffmpeg-on-mac-os-x.html \n",
    "\n",
    "To reset your PATH environment variable without killing and restarting your command console:\n",
    "```(python)\n",
    "SET PATH=%PATH%;C:\\ffmpeg\\bin\n",
    "```\n",
    "\n",
    "But you want to add `ffmpeg\\bin` in your PATH. It's good juju.\n",
    "\n",
    "***Also***, create subfolders `videos` and `images` in your `C:\\Users\\<username>` folder\n",
    "\n",
    ">**Pit Stop**: This will take 5 minutes, so let's take a pit stop and park our F1 for gas and a change of tires. Meet back in 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the ***animation*** of our training process, which is also called **auto-encoding** (a generative model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the animation\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "def update_line(num):\n",
    "    plt.title('iteration: {0}'.format((history.save_every * (num + 1))))\n",
    "    line.set_xdata(x)\n",
    "    line.set_ydata(history.predictions[num])\n",
    "    return []\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_line, len(history.predictions),\n",
    "                                   interval=50, blit=True)\n",
    "ani.save('keras_videos/neuron.mp4', fps=30, extra_args=['-vcodec', 'libx264', '-pix_fmt','yuv420p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uh-oh..\n",
    "\n",
    "gulp (hard swallow)...\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/oopsie.gif\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does our bunny mesh match our bunny cloud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y, label='data')\n",
    "plt.plot(x, history.predictions[0], label='prediction')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('iteration: 0')\n",
    "plt.savefig('images/neuron_start.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really..\n",
    "\n",
    "What's our **training error**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... Not good! We're ***not*** learning! \n",
    "\n",
    "Do you remember how I mentioned [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) during our discussion on activation functions? This is exactly *what's happening here*!\n",
    "\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/bad-robot.jpg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('neural net weigths after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*No* change in weights!\n",
    "\n",
    "### Back to the drawing board: Modify model hyperparameters\n",
    "\n",
    "Let's use batch normalization, also called [mini-batch](https://en.wikipedia.org/wiki/Batch_normalization), a technique for improving speed, performance, and stability of artificial neural networks, introduced in 2015 paper.It normalizes the input layer by adjusting and scaling activations., thereyby smoothing the objective function to improve performance.\n",
    "\n",
    "In Keras, it's the `batch-size` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6: larger batch size\n",
    "history = TrainingHistory()\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=1, input_dim=1, init=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=200,\n",
    "          nb_epoch=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哇！ Now look at that training error going to zero!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/good-robot.jpg\" width=200 />\n",
    "</center>\n",
    "\n",
    "And if we plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update_line, len(history.predictions),\n",
    "                                   interval=50, blit=True)\n",
    "ani.save('keras_videos/neuron-large_batch.mp4', fps=30, extra_args=['-vcodec', 'libx264', '-pix_fmt','yuv420p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is we start with zero weights?\n",
    "\n",
    "Let's give training another shot, initializing our weights to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 7: Dying ReLu problem\n",
    "\n",
    "np.random.seed(2)\n",
    "history = TrainingHistory()\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=1, input_dim=1, init=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net initialized with weigths w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0][0][0] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=200,\n",
    "          nb_epoch=2000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])\n",
    "\n",
    "weights = model.layers[0].get_weights()\n",
    "w0 = weights[0][0][0]\n",
    "w1 = weights[1][0]\n",
    "print('neural net weigths after training w0: {w0:.2f}, w1: {w1:.2f}'.format(**locals()))\n",
    "print('dying ReLU problem!')\n",
    "print('http://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks')\n",
    "\n",
    "fig = plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(x, y,  label='data')\n",
    "line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh-oh! And we did use mini-batch! This is called the [dying ReLu](https://arxiv.org/abs/1903.06733) problem. Read the [URL](http://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks) below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dying ReLU problem!')\n",
    "print('http://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 2.5))\n",
    "plt.plot(history.losses)\n",
    "plt.ylabel('error')\n",
    "plt.xlabel('iteration')\n",
    "plt.title('training error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty strange indeed! Looks like we won at step 0. But not really..\n",
    "\n",
    "This problem is usually fixed using a ***leaky ReLU***, which we mentioned during our discussion on activation functions. Let's move on to more neurons now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixty neurons\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/brain.gif\" width=200 />\n",
    "</center>\n",
    "\n",
    "Let's try increasing our level of intelligence by augmenting our brain from ***one*** to ***sixty*** neurons (still only ***one*** layer).\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/neurons-60.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "Let's work with slightly more complicated data: Let's model a *noisy* **double-sine** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.linspace(0, 2 * math.pi, 200)\n",
    "sine = np.sin(x)\n",
    "err = np.random.normal(0, 0.2, len(sine))\n",
    "y = sine + err\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(x, y)\n",
    "plt.xlim([0, 2 * math.pi])\n",
    "plt.title('A noisy sine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **compile** a Keras model with a single layer of 60 neurons. This should remind you of our caterpilar brain, slightly larger, with non-linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "n_conn = 60\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=n_conn, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TrainingHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.predictions = []\n",
    "        self.i = 0\n",
    "        self.save_every = 50\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.i += 1        \n",
    "        if self.i % self.save_every == 0:        \n",
    "            pred = model.predict(X_train)\n",
    "            self.predictions.append(pred)\n",
    "            \n",
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **train** our model in 5000 timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = TrainingHistory()\n",
    "res = model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's animate our learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "def visualize_training(history, name):\n",
    "    fig = plt.figure(figsize=(5, 2.5))\n",
    "    plt.plot(x, y, label='data')\n",
    "    line, = plt.plot(x, history.predictions[0],  label='prediction')\n",
    "    plt.legend()\n",
    "\n",
    "    def update_line(num):\n",
    "        plt.title('iteration: {0}'.format((history.save_every * (num + 1))))\n",
    "        line.set_xdata(x)\n",
    "        line.set_ydata(history.predictions[num])\n",
    "        return []\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update_line, len(history.predictions),\n",
    "                                       interval=50, blit=True)\n",
    "    ani.save('keras_videos/{0}.mp4'.format(name), dpi=100, extra_args=['-vcodec', 'libx264', '-pix_fmt','yuv420p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training(history, 'noisy-sine-one-layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh... We were able to model the first \"*kink*\" in our 1D mesh cloud, but ***not the second***!\n",
    "\n",
    ">**DISCUSSION**: Why do you think?\n",
    "\n",
    "This is what we started with at step 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(5, 2.5))\n",
    "    plt.plot(x, y, label='data')\n",
    "    plt.plot(x, history.predictions[0], label='prediction')\n",
    "    plt.legend()\n",
    "    plt.title('iteration: 0')\n",
    "    plt.savefig('images/{0}.png'.format('noisy-sine-one-layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our training error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.plot(history.losses)\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylim([0, 0.5])\n",
    "    plt.title('training error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Better model?\n",
    "\n",
    ">**Challenge**: Can you come up with a better neural architecture that retains the same \"*brainpower*\", i.e. number of neurons (60 neurons) and weights, but *increases* modeling intelligence, Dr. Frankenstein?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/give-you-life.gif\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "      <summary>Can you come up with this \"*clever-er*\" architecture?</summary>\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/neurons-10-10.png\" width=200 />\n",
    "    Count the number of weights and compare to the 60 neuron single layer!\n",
    "</center>\n",
    "</details>\n",
    "\n",
    "Let's compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conn = 10\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=n_conn, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=n_conn))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "history = TrainingHistory()\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training(history, 'tiny-sine-two-layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">哇！他真棒！\n",
    "\n",
    "Is adding a second layer the only possible way to increase our learning power? No, we can also watch an episode of GoT... Kidding! We can also use  optimizers such as [Adam](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c). Let's ***go back*** to our single layer of 60 neurons and leverage Adam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "n_conn = 60\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=n_conn, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "adam = Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "history = TrainingHistory()\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training(history, 'tiny-sine-one-layer-adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! It ***worked***， with a *single* layer!\n",
    "\n",
    "And what if we combine our two-layer model ***and*** use Adam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 8\n",
    "n_conn = 10\n",
    "model = Sequential()\n",
    "model.add(Dense(output_dim=n_conn, input_dim=1))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=n_conn))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=1))\n",
    "adam = Adam()\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(x, ndmin=2).T\n",
    "Y_train = np.array(y, ndmin=2).T\n",
    "history = TrainingHistory()\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          nb_epoch=5000,\n",
    "          verbose=0,\n",
    "          callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_training(history, 'tiny-sine-two-layer-adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE** Professor, so many hyperparameters to play with! Yup, just like with Bayesian modeling, modeling is an ***art*** *and* a science1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "Now that we know our Keras API and how to build ANNs, let's apply this knowledge to Tensorflow, which uses the `tf.keras` API, see [this guide](https://www.tensorflow.org/guide/keras)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "Let's use it ti predict house prices in Boston."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHp3M9ZmrIxj"
   },
   "source": [
    "In a *regression* problem, we aim to predict the output of a continuous value, like a price or a probability. In a *classification* problem (discriminative model), we aim to predict a discrete label. \n",
    "\n",
    "Let's predict the median price of homes in a Boston suburb in the mid-1970s. We'll provide the model with some data points about the suburb, such as the crime rate and the local property tax rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_72b0LCNbjx"
   },
   "source": [
    "## The Boston Housing Prices dataset\n",
    "\n",
    "The [dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) is accessible directly in TensorFlow. \n",
    "\n",
    "Let's download and **shuffle** the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9kxxgzvzlyz"
   },
   "outputs": [],
   "source": [
    "boston_housing = keras.datasets.boston_housing\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()\n",
    "\n",
    "# Shuffle the training set\n",
    "order = np.argsort(np.random.random(train_labels.shape))\n",
    "train_data = train_data[order]\n",
    "train_labels = train_labels[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwEKwRJgsgJ6"
   },
   "source": [
    "### Examples and features\n",
    "\n",
    "This dataset is small: it has 506 total examples, split between 404 training examples and 102 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ujqcgkipr65P"
   },
   "outputs": [],
   "source": [
    "print(\"Training set: {}\".format(train_data.shape))  # 404 examples, 13 features\n",
    "print(\"Testing set:  {}\".format(test_data.shape))   # 102 examples, 13 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LRPXE3Oz3Nq"
   },
   "source": [
    "The dataset contains 13 different features:\n",
    "\n",
    "1.   Per capita crime rate.\n",
    "2.   The proportion of residential land zoned for lots over 25,000 square feet.\n",
    "3.   The proportion of non-retail business acres per town.\n",
    "4.   Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "5.   Nitric oxides concentration (parts per 10 million).\n",
    "6.   The average number of rooms per dwelling.\n",
    "7.   The proportion of owner-occupied units built before 1940.\n",
    "8.   Weighted distances to five Boston employment centers.\n",
    "9.   Index of accessibility to radial highways.\n",
    "10.  Full-value property-tax rate per $10,000.\n",
    "11.  Pupil-teacher ratio by town.\n",
    "12.  1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n",
    "13.  Percentage lower status of the population.\n",
    "\n",
    "Each one of these input data features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on. This is often the case with real-world data, and understanding how to explore and clean such data is an important skill to develop.\n",
    "\n",
    "> **DISCUSSION**: As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about [ML fairness](https://developers.google.com/machine-learning/fairness-overview/).\n",
    "\n",
    "Let's display sample features and notice the different scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tYsm8Gs03J4"
   },
   "outputs": [],
   "source": [
    "print(train_data[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7muNf-d1-ne"
   },
   "source": [
    "Let's use [pandas](https://pandas.pydata.org) to display the first few rows of the dataset in a nicely formatted table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYVyGhdyCpIM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
    "                'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "df = pd.DataFrame(train_data, columns=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wb9S7Mia2lpf"
   },
   "source": [
    "### Labels\n",
    "\n",
    "The labels are the house prices in thousands of dollars. (You may notice mid-1970s prices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8NwI2ND2t4Y"
   },
   "outputs": [],
   "source": [
    "print(train_labels[0:10])  # Display first 10 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRklxK5s388r"
   },
   "source": [
    "## Normalize features\n",
    "\n",
    "It's ***always*** recommended to normalize features that use different scales and ranges. \n",
    "\n",
    "For each feature, subtract the mean of the feature and divide by the standard deviation (or use `sklearn`'s `preprocessing.MinMaxScaler` API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ze5WQP8R1TYg"
   },
   "outputs": [],
   "source": [
    "# Test data is *not* used when calculating the mean and std\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "train_data = (train_data - mean) / std\n",
    "test_data = (test_data - mean) / std\n",
    "\n",
    "print(train_data[0])  # First training sample, normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuiClDk45eS4"
   },
   "source": [
    "Although the model *might* converge without feature normalization, it makes training more ***difficult***, and it makes the resulting model more dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmjdzxKzEu1-"
   },
   "source": [
    "## Create the model\n",
    "\n",
    "Let's build our model. Here, we'll use a Keras `Sequential` model with two densely connected hidden layers (since these worked so well for us in the noisy sine), and an output layer that returns a **single**, **continuous** value. The model building steps are wrapped in a function, `build_model`, since we'll create a second model, later on.\n",
    "\n",
    "`model.summary()`gives us a nice visual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c26juK7ZG8j-"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu,\n",
    "                       input_shape=(train_data.shape[1],)),\n",
    "    keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae'])\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-qWCsh6DlyH"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Let's train for 500 epochs and record the training and validation accuracy in the `history2` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD7qHCmNIOY0"
   },
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 100 == 0: print('')\n",
    "    print('.', end='')\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "# Store training stats\n",
    "history2 = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=0,\n",
    "                    callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQm3pc0FYPQB"
   },
   "source": [
    "Let's visualize the model's training progress using the stats stored in the `history` object. We want to use this data to determine how long to train *before* the model stops making progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6XriGbVPh2t"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [1000$]')\n",
    "  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']),\n",
    "           label='Train Loss')\n",
    "  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
    "           label = 'Val loss')\n",
    "  plt.legend()\n",
    "  plt.ylim([0, 5])\n",
    "\n",
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqsuANc11FYv"
   },
   "source": [
    "This graph shows little improvement in the model after about 200 epochs. Let's update the `model.fit` method to automatically **stop training** when the validation score doesn't improve. We'll use a *callback* that tests a training condition for  every epoch. If a set amount of epochs elapses without showing improvement, then automatically stop the training.\n",
    "\n",
    "You can learn more about this callback [here](https://www.tensorflow.org/versions/master/api_docs/python/tf/keras/callbacks/EarlyStopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdMZuhUgzMZ4"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history2 = model.fit(train_data, train_labels, epochs=EPOCHS,\n",
    "                    validation_split=0.2, verbose=0,\n",
    "                    callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3St8-DmrX8P4"
   },
   "source": [
    "The graph shows the average error is about \\\\$2,500 dollars. \n",
    "\n",
    "Let's see how did the model performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl_yNr5n1kms"
   },
   "outputs": [],
   "source": [
    "[loss, mae] = model.evaluate(test_data, test_labels, verbose=0)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft603OzXuEZC"
   },
   "source": [
    "## Predict\n",
    "\n",
    "Finally, let's predict some housing prices using data in the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU"
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_data).flatten()\n",
    "\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [1000$]')\n",
    "plt.ylabel('Predictions [1000$]')\n",
    "plt.axis('equal')\n",
    "plt.xlim(plt.xlim())\n",
    "plt.ylim(plt.ylim())\n",
    "_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-OHX4DiXd8x"
   },
   "outputs": [],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins = 50)\n",
    "plt.xlabel(\"Prediction Error [1000$]\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgGQuV-yqYZH"
   },
   "source": [
    "What does this histogram look like?\n",
    "\n",
    "## Some advice, not nearly enough\n",
    "\n",
    "* Mean Squared Error (MSE) is a common loss function used for regression problems (different than classification problems).\n",
    "* Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\n",
    "* When input data features have values with different ranges, each feature should be scaled independently.\n",
    "* If there is not much training data, prefer a small network with few hidden layers to avoid overfitting.\n",
    "* Early stopping is a useful technique to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FhGuhbZ6M5tl"
   ],
   "name": "basic-regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
