{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 6 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 20 March 2020, with material from Thomas Wiecki</div>\n",
    "\n",
    "# Metropolis lab 1\n",
    "\n",
    "We worked with estimating the **mean** of a model in our first notebook, so let's estimate **standard deviation** in this notebook. You will write your own [Metropolis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm). You will also improve on a little bit of sloppy work by professor: He did not take the **log** of probabilities in his lab, resulting in underflows in his simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import matplotlib as mpl   \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data generation\n",
    "\n",
    "We generate 30,000 samples from a normal distribution with $\\mu$ = 10, and $\\sigma$= 3, but let's say we can only observe 1000 of them. \n",
    "\n",
    "Lab goal: We'll use Bayesian estimation to build a model from the 1000 observations, then we'll use the model to reconstruct (a simulation of) the 30,000 samples.\n",
    "\n",
    "Let's start by plotting the histogram of these observed 1000 datapoints. Our generating model is a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=lambda mu, sig, t: np.random.normal(mu, sig, t)\n",
    "\n",
    "#Form a population of 30,000 individuals, with average=10 and sigma=3\n",
    "population = model(10, 3, 30000)\n",
    "\n",
    "#Assume we are only able to observe 1,000 of these individuals, sampled randomly amongst the 30000.\n",
    "observation = population[np.random.randint(0, 30000, 1000)]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist( observation,bins=35 ,)\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Figure 1: Distribution of 1000 observations sampled from a population of 30,000 with $\\mu$=10, $\\sigma$=3\")\n",
    "mu_obs=observation.mean()\n",
    "mu_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Which parameter to model\n",
    "\n",
    "Our $\\theta$ is made up of two values: $[\\mu,\\sigma]$. Let's assume  $\\mu$ is a constant, $\\mu = \\mu_{obs}$. What you computed above.\n",
    "\n",
    "We would like to find a distribution for $\\sigma_{obs}$ using the 1000 observed samples. \n",
    "\n",
    "Those with a math background will say... Professor! ***There is a formula for computing the standard deviation $\\sigma$ of gaussian-looking data***! Actually, ***y'all need to know how to compute standard deviation from $n$ observations $d_i$ with mean $\\mu$***):\n",
    "\n",
    "$$\\sigma=\\sqrt{\\frac{1}{n}\\sum_i^n(d_i-\\mu)^2}$$\n",
    "\n",
    "Note however, we are not trying to find *a* value for $\\sigma$, but rather, we are trying to ***compute a distribution of the possible values of $\\sigma$***.\n",
    "\n",
    "## Step 3: Define the pdf for the prior for model parameter(s), and the pdf for the likelihood of the data\n",
    "From the figure above, we can see that the data is **normally distributed**. The mean can be easily computed by taking the average of the values of the 1000 samples. By doing that, we get for example $\\mu_{obs}=9.8$.\n",
    "\n",
    "Now, let's start picking the pdf's for our model parameters. For the standard deviation pdf, let's pick a simple one: the normal distribution!\n",
    "\n",
    "\\begin{equation} \\sigma_{new} \\sim N(\\mu=\\sigma_{current},\\; \\sigma'=1) \\end{equation}\n",
    "\n",
    "Note that $\\sigma'$ is *unrelated* to $\\sigma_{new}$ and $\\sigma_{current}$. It simply specifies the standard deviation of the parameter's pdf. It can be *any* value desired. It only affects the convergence time of the algorithm.\n",
    "\n",
    "We don't have any preferences values that $\\sigma_{new}$ and $\\sigma_{current}$ can take, but they should be positive! Why? Intuitively, the standard deviation measures **dispersion**. Dispersion is a distance, and distances cannot be negative.\n",
    "\n",
    "Mathematically, $\\sigma=\\sqrt{\\dfrac{1}{n}\\sum_i^n(d_i-\\mu)^2}$, and the square root of a number cannot be negative. We should strictly enforce this in the prior. Enforcing constraints is important in the model because otherwise we may oscillate between realistic and unrealistic solutions.\n",
    "\n",
    "Since likelihood $f$ should be proportional to the posterior, we choose $f$ to be the following pdf, for each data point $d_i$ in the data D:\n",
    "\n",
    "\\begin{equation} f(d_i\\;|\\; \\mu,\\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\dfrac{(d_i-\\mu)^2}{2\\sigma^2}} \\end{equation}\n",
    "\n",
    "#### Question 1) Compute the standard deviation of the 30,000 samples, and then the standard deviation of the 1,000 observations in the cell below. Same or different?\n",
    "\n",
    "Note that the standard deviation of a non-gaussian distribution is ***nonsense***! Thankfully here, we have a distribution that ***is*** gaussian-distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define when we accept or reject $\\sigma_{new}$: \n",
    "From our lab, we know to accept $\\sigma_{new}$ from $\\sigma_{current}$ if:\n",
    "\n",
    "$\\dfrac{\\text{Likelihood}(D \\;|\\; \\mu_{obs},\\sigma_{new})\\; * \\; \\text{prior}(\\mu_{obs},\\sigma_{new})}{\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{current})\\;*\\;\\text{prior}(\\mu_{obs},\\sigma_{current})} \\;\\;>\\;\\; 1     \\quad \\quad \\quad \\quad \\quad      (1)$\n",
    "\n",
    "Otherwise, if this ratio is smaller or equal to 1, then we compare it to a uniformly generated random number in the closed set [0,1]. If the ratio is larger than the random number, we accept $\\sigma_{new}$, otherwise we reject it.\n",
    "\n",
    "*Note: Since we will be computing this ratio to decide which parameters should be accepted, make sure that the adopted likelihood is proportional to the posterior itself, $P(\\sigma\\;|\\; D,\\mu)$, which in our case is true.*\n",
    "\n",
    "\n",
    "## Step 5: Acceptance condition derivation:\n",
    "\n",
    "Just like the likelihood of flipping a coin 10 times and obsering 10 tails is $(0.5)^{10}$, the total likelihood for a set of observations $D$ when we have the probability of each observation $f(d_i\\;|\\;\\mu,\\sigma)$ is: $\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) = \\prod_i^n f(d_i\\;|\\;\\mu_{obs},\\sigma_{new})  \\quad \\quad \\quad \\quad \\quad      (2)$\n",
    "\n",
    "In our case, we will take the **log** of both the prior and the likelihood function. Why the log? Simply because it helps with **numerical stability**. Multiplying thousands of small values (probabilities, likelihoods, etc..) can cause an **underflow** in system memory, and the **log** is a perfect solution because it transforms multiplications to **additions** and small positive numbers into not-so-small negative numbers.\n",
    "\n",
    "Therefore our acceptance condition from equation $(1)$ which is:\n",
    "Accept $\\sigma_{new}$ if:\n",
    "\n",
    "$$\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) * \\text{prior}(\\mu_{obs},\\sigma_{new})) > \\\\ \\text{Likelihood}(D\\;|\\:\\mu_{obs},\\sigma_{current}) *  \\text{prior}(\\mu_{obs},\\sigma_{current}))$$\n",
    "\n",
    "After taking the **log** of equation (1), which we can since **log** is a [monotonic](https://en.wikipedia.org/wiki/Monotonic_function) function, equation (1) becomes:\n",
    "\n",
    "$$Log[\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) * \\text{prior}(\\mu_{obs},\\sigma_{new}))] > \\\\ Log[\\text{Likelihood}(D\\;|\\:\\mu_{obs},\\sigma_{current}) *  \\text{prior}(\\mu_{obs},\\sigma_{current}))]$$\n",
    "\n",
    "But since $Log(a * b) = Log(a) + Log(b)$, our condition becomes:\n",
    "\n",
    "$$Log(\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new})) + Log(\\text{prior}(\\mu_{obs},\\sigma_{new})) - (Log(\\text{Likelihood}(D\\;|\\:\\mu_{obs},\\sigma_{current})) + Log(prior(\\mu_{obs},\\sigma_{current})))\\;>\\;0$$\n",
    "\n",
    "By plugging in $\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) = \\prod_i^n f(d_i\\;|\\;\\mu_{obs},\\sigma_{new}) $, our condition becomes:\n",
    "\n",
    " $$\\sum_i^nLog(f(d_i\\;|\\;\\mu_{obs},\\sigma_{new})) \\;\\; + Log(\\text{prior}(\\mu_{obs},\\sigma_{new})) - \\sum_i^nLog(f(d_i\\;|\\;\\mu_{obs},\\sigma_{current})) \\;\\; -Log(\\text{prior}(\\mu_{obs},\\sigma_{current}))>0$$\n",
    " \n",
    " $\\quad$\n",
    " \n",
    " \n",
    "By plugging in \\begin{equation} f(d_i\\;|\\; \\mu,\\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\dfrac{(d_i-\\mu)^2}{2\\sigma^2}} \\end{equation}\n",
    "\n",
    "Our condition becomes: \n",
    "  \n",
    "$$\\sum_i^n -Log(\\sigma_{new}\\sqrt{2\\pi})-\\dfrac{(d_i-\\mu_{obs})^2}{2\\sigma_{new}^2} \\;\\;+\\;\\; Log(prior(\\mu_{obs},\\sigma_{new})) \\quad > \\\\\n",
    "\\quad \\sum_i^n -Log(\\sigma_{current}\\sqrt{2\\pi})-\\dfrac{(d_i-\\mu_{obs})^2}{2\\sigma_{current}^2} \\;\\;+\\;\\; Log(prior(\\mu_{obs},\\sigma_{current})) \\quad \\quad  (3)$$\n",
    "\n",
    "Note that in our calculus above, we have liberaly used $log(a * b) = log(a) + log(b)$, $log(a^2) = 2 log(a)$, and $log(\\frac{1}{a}) = -log(a)$.\n",
    "\n",
    "So (3) will be our acceptance condition. If true, we will always accept $\\sigma_{new}$ as our next value in the chain. If false, ***we will only accept it according to a probability equal to the ratio of the nominators*** (like in the lab notebook).\n",
    "\n",
    "So let's write some code. Your homework is to replace ellipses (...) with code, to build your own Metropolis algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transition model defines how to move from sigma_current to sigma_new\n",
    "# Our model has two parameters x: x[0] = mu, x[1] = sigma\n",
    "# The first one is fixed, the second one we will model with a normal distribution with mu=x[1] and sigma=0.5\n",
    "# Purpose of the trailing [0] is to unpackage a numpy array of a single number into the number itself. Simpler that way \n",
    "transition_model = lambda x: [x[0], np.random.normal(x[1], 0.5, (1,))[0]]\n",
    "\n",
    "\n",
    "def prior(x):\n",
    "    #x[0] = mu, x[1]=sigma (new or current)\n",
    "    #returns 1 for all valid values of sigma. Log(1)=0, so it does not affect the summation.\n",
    "    #returns 0 for all invalid values of sigma (<=0). Log(0)=-infinity, and Log(negative number) is undefined. Makes the \n",
    "    # new sigma infinitely unlikely.\n",
    "    if(x[1] <= 0):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "# Computes the likelihood of the data given a sigma (new or current) according to equation (3)\n",
    "def manual_log_like_normal(x, data):\n",
    "    #x[0]=mu, x[1]=sigma (new or current)\n",
    "    #data = the observation\n",
    "    return np.sum(-np.log(x[1] * np.sqrt(2* np.pi) )-((data-x[0])**2) / (2*x[1]**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sig = transition_model([mu_obs,0.1])\n",
    "mu, sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2) Please write your own Metropolis algorithm below\n",
    "\n",
    "Note the inputs to your Metropolis algorithm:\n",
    "\n",
    "`likelihood_computation(x,data)` is the **likelihood** that parameters x generated the data. That will be the function `manual_log_likelihood` written above.\n",
    "\n",
    "`transition_model(x)`: is a function that draws a sample from a symmetric distribution and returns it.\n",
    "\n",
    "`param_init` is the starting sample. So you start your Metropolis program with `x = param_init`\n",
    "\n",
    "`iterations` is the number of accepted to generated\n",
    "\n",
    "`data` are the observations we wish to model\n",
    "\n",
    "`acceptance_rule(x, x_new)` is a predicate that decides whether to accept or reject the new sample.\n",
    "\n",
    "So, start with x equal to `param_init`, and with two lists, one are the accepted values `accepted = []`, the other the rejected values `rejected = []`.\n",
    "\n",
    "Then for index i ranging in `range(iterations)`, set `x_new` equal to `transition_model(x)`, and `x_likely` to `likelihood_computation(x, data)`. Then evaluate a new candidate `x_new_likely` in the same way.\n",
    "\n",
    "Then plug in equation (3) as input to your `acceptance_rule`. Equation (3) can simply be written as comparing `(x_likely + np.log(prior(x))` to `(x_new_likely + np.log(prior(x_new))`. If the new value `x_new` is accepted, replace `x` with `x_new` in order to start the next iteration, and add `x_new` to the accepted list. If the new value `x_new` is rejected, then just add it to the rejected list.\n",
    "\n",
    "At the end of `iterations`, return the accepted list and the rejected list as numpy arrays: `return np.array(accepted), np.array(rejected)`.\n",
    "\n",
    "Simple, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines whether to accept or reject the new sample\n",
    "def acceptance(x, x_new):\n",
    "    if x_new > x:\n",
    "        return True\n",
    "    else:\n",
    "        accept=np.random.uniform(0,1)\n",
    "        # Since we did a log likelihood, we need to exponentiate in order to compare to the random number\n",
    "        # less likely x_new are less likely to be accepted\n",
    "        return (accept < (np.exp(x_new - x)))\n",
    "\n",
    "# Your Metropolis algorithm!\n",
    "def metropolis_hastings(likelihood_computation, prior, transition_model, param_init, iterations, data, acceptance_rule):\n",
    "    # likelihood_computer(x,data): returns the likelihood that these parameters generated the data\n",
    "    # transition_model(x): a function that draws a sample from a symmetric distribution and returns it\n",
    "    # param_init: a starting sample\n",
    "    # iterations: number of accepted to generated\n",
    "    # data: the data that we wish to model\n",
    "    # acceptance_rule(x,x_new): decides whether to accept or reject the new sample\n",
    "    ...          \n",
    "                \n",
    "    return np.array(accepted), np.array(rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the algorithm with initial parameters and collect accepted and rejected samples\n",
    "\n",
    "#### Question 3) Run your Metropolis algorithm with the following parameters:\n",
    "\n",
    "- manual_log_likelihood\n",
    "- prior\n",
    "- transition_model\n",
    "- [mu_obs, 0.1]\n",
    "- 50000\n",
    "- observation\n",
    "- acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted, rejected = metropolis_hastings(manual_log_like_normal, prior, transition_model, [mu_obs,0.1], 50000, \n",
    "                                         observation, acceptance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm accepted 8803 samples (which might be different on each new run).\n",
    "\n",
    "The last 10 samples contain the following  values for $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted[-10:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an array of single-point arrays, due to how we wrote our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accepted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4) Plot accepted and rejected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "ax.plot( rejected[0:50,1], 'rx', label='Rejected',alpha=0.5)\n",
    "ax.plot( accepted[0:50,1], 'b.', label='Accepted',alpha=0.5)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"$\\sigma$\")\n",
    "ax.set_title(\"Figure 2: MCMC sampling for $\\sigma$ with Metropolis-Hastings. First 50 samples are shown.\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "to_show=-accepted.shape[0]\n",
    "ax2.plot( rejected[to_show:,1], 'rx', label='Rejected',alpha=0.5)\n",
    "ax2.plot( accepted[to_show:,1], 'b.', label='Accepted',alpha=0.5)\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "ax2.set_ylabel(\"$\\sigma$\")\n",
    "ax2.set_title(\"Figure 3: MCMC sampling for $\\sigma$ with Metropolis-Hastings. All samples are shown.\")\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "accepted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we sample our state space, accepting some samples and rejecting others, and converging to a mean of 3 (the peak of the distribution of our model's sigma parameter).\n",
    "\n",
    "So, starting from an initial σ of 0.1, the algorithm converged pretty quickly to the expected value of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We consider the initial 25% of the values of $\\sigma$ to be \"burn-in\", so we drop them.\n",
    "### Let's visualize the trace of  $\\sigma$ and the histogram of the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show=int(-0.75*accepted.shape[0])\n",
    "hist_show=int(-0.75*accepted.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(accepted[show:,1])\n",
    "ax.set_title(\"Figure 4: Trace for $\\sigma$\")\n",
    "ax.set_ylabel(\"$\\sigma$\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.hist(accepted[hist_show:,1], bins=20,density=True)\n",
    "ax.set_ylabel(\"Frequency (normed)\")\n",
    "ax.set_xlabel(\"$\\sigma$\")\n",
    "ax.set_title(\"Figure 5: Histogram of $\\sigma$\")\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "ax.grid(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most likely value for $\\sigma$ is around 3.1. This is a bit more than the original value of 3.0. \n",
    "\n",
    "The difference is due to us observing only 3.33% of the original population (1,000 out of 30,000) \n",
    "\n",
    "## Predictions:\n",
    "Now that we have a model, a much more powerful engine than just 1000 dataponts (!), let's make a prediction on what 30,000 individuals drawn from the same distribution look like.\n",
    "\n",
    "#### Question 5)  First, average out the last 75% of accepted samples of σ, and then generate 30,000 random individuals from a normal distribution with μ=9.8 and your value of σ. Then compare against the original data of 30,000 individuals (fill in the `???`). Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu=???.mean()\n",
    "sigma=???.mean()\n",
    "\n",
    "print(mu, sigma)\n",
    "\n",
    "model = lambda t,mu,sigma:np.random.normal(mu,sigma,t)\n",
    "\n",
    "observation_gen=model(population.shape[0],mu,sigma)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist( observation_gen,bins=70 ,label=\"Predicted distribution of 30,000 individuals\")\n",
    "ax.hist( population,bins=70 ,alpha=0.5, label=\"Original values of the 30,000 individuals\")\n",
    "ax.set_xlabel(\"Mean\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Figure 6: Posterior distribution of predicitons\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good fit! Our model, generated from a small 1,000 sample of 30,000 observations, suceeds in modelling all 30,000 observations!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The great thing about probabilistic programming is that you only need to write down the model and then run it. The simplest MCMC algorithm, [Metropolis](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm), is very simple. There is no need to compute evidence (denominator), or ensure constraining mathematical properties.\n",
    "\n",
    "You are now a Metropolis wizard! You can predict the past from the future :-) After linear algebgra and graphs, you will be able to do the same thing in multiple dimensions, with a bit of help from Artificial Neural Networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
