{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 5 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 02 02 2020, with material from Chris Fonnesbeck and Cyrille Rossant</div>\n",
    "<div style=\"text-align: right\"><i>please browse all URLs in this notebook</i></div>\n",
    "\n",
    "Last week, we learned **probability theory**. Basically, *if you can count* by building your random-variable sets and the predicates you want to apply on the datasets with python, you do not need to know the math behind probability theory (but knowing some of the math is helpful). We even did a quick intro to **Bayes' formula**, which allows you to invert the precondition and the postcondition on a conditional probability to make probabilities easier to evaluate. That is very useful for data science interviews that involve probability questions. But Bayes' formula will open up a new universe of statistics for us, which we'll get into next class. For this class, let's learn **classical statistics** based on *classical probability theory*.\n",
    "\n",
    "# Palindrome dates\n",
    "\n",
    "Read today's date backwards. **Homework exercise**: When's the *last* time a date was a palindrome? When's the *next* time? Do you feel special?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"02022020\"[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Statistical Data Analysis with analytic distributions\n",
    "\n",
    ">**DEFINITION**: What is a **random variable**? Find the math definition [here](https://en.wikipedia.org/wiki/Random_variable). I prefer *this* definition: it's an Excel column of data. The $x$s are the *names* of each row. The $y$s are the cell *values*. $x$ is called the random variable, and $y$ is called its **range** $f(x)$. $f$ is the function we're after, to figure out from the data. Often, all we can get to is its *approximation*. The function $f$ is revelatory of the *mechanism that produced the data*, called the **model**. That is **data science** in a nutshell.\n",
    "\n",
    "A common question then follows: Wait a minute, the data may not be **random**. Why do you use the word **random**?\n",
    "\n",
    "Well, according to [Ramsey theory](https://en.wikipedia.org/wiki/Ramsey_theory), ideal randomness is *impossible* especially for large structures. While disorder is more probable in general, complete disorder is *impossible*. Misunderstanding of this can lead to numerous [conspiracy theories](https://en.wikipedia.org/wiki/Conspiracy_theory). Saying randomness is impossible is tantamount to saying that a Uniform distribution (which we'll examine further below) for the random variable is *not possible*. In other words, it's not like throwing the dice, or flipping a coin. There must be *some order* in the data if it's produced by a natural phenomenon, and a histogram can help detect this order. For example, did you know that the number of emails you receive a day is ***not random***, meaning that if you count the humber of emails, all possible numbers are not equally probably? It's not like throwing the dice!\n",
    "\n",
    "The corollary of this of course is that if data is not random, you can model it and predict it!\n",
    "\n",
    "Machine models like regression forests are *sexy* because the computer builds the model (the function approximation) for you, but there are many cases where ***you can build the model for you*** (i.e. find the $f$), and before powerful computers, statisticians did not know about ML so they found the $f$ using math. So let's go back in time a couple of decades at least and look at what Neanderthal data scientists did. Actually, there is good stuff to learn.\n",
    "\n",
    "The primary goal of statistical data analysis is to create a **model** for the data so you can throw away the data and use the model instead. After all, we already know *that is what intelligence is all about*. The more (low-dimensional) models you have, the less storage you need, the more models you can accomodate, the cleverer you are!\n",
    "\n",
    "A model's goal is usually to relate the influence of one variable on another. For example, how different medical interventions influence the incidence or duration of disease, how baseball players' performance varies as a function of age, how infectious diseases like the Coronavirus influence the stock market. The independent variable(s) $x$ is(are) called the predictor variable(s). The variable(s) $y$ to predict is(are) called the dependent variable(s), or target(s). We build the function approximation that gives us $y$ from $x$. *That* ***is*** data science.\n",
    "\n",
    ">**WHY** am I repeating myself? Because this is **so** important!\n",
    "\n",
    "**Bayesian models**, which we're going to look at soon, involve advanced probabilistic programming. We only *now* have the computer power to run these programs. What did Neanderthal man do when he tried to do statistical data analysis?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/caveman.png width = 400 />\n",
    "</center>\n",
    "\n",
    "In this notebook we'll take a look at the two most popular statistical data science methods prior to the resounding success of modern Bayesian estimation: **Method Of Moments** (MOM) and **Maximum Likelihood Estimation** (MLE). We'll skip over **Maximum A Posteriori** (MAP) estimation.\n",
    "\n",
    "But first, we learn more **statistics**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More about *distributions*\n",
    "\n",
    "Statistical **point** ***features*** is probably the most used statistics concept in data science. It is often\n",
    "the first statistics technique you would apply when exploring a column of a dataset (i.e. a *random variable*)\n",
    "and includes concepts such as **bias, variance, mean, median, percentiles**, and others. It’s all fairly easy to\n",
    "understand and implement in code, and `pandas`'s `.describe()` dataframe API gives you most of them!\n",
    "\n",
    "A **box (or whisker) plot** perfectly illustrates what we can do with basic statistical features of a random variable:\n",
    "\n",
    "- The line in the middle is the median value of the data. Median is used over the mean\n",
    "since it is more robust to outlier values. The first quartile is essentially the 25th\n",
    "percentile; i.e 25% of the points in the data fall below that value. The third quartile is the\n",
    "75th percentile; i.e 75% of the points in the data fall below that value. The min and max\n",
    "values represent the upper and lower ends of our data range.\n",
    "\n",
    "- When the box plot is short it implies that much of your data points are similar, since\n",
    "there are many values in a small range\n",
    "\n",
    "- When the box plot is tall it implies that much of your data points are quite different,\n",
    "since the values are spread over a wide range\n",
    "\n",
    "- If the median value is closer to the bottom then we know that most of the data has\n",
    "lower values. If the median value is closer to the top then we know that most of the\n",
    "data has higher values. Basically, if the median line is not in the middle of the box\n",
    "then it is an indication of skewed data.\n",
    "\n",
    "- Are the whiskers very long? That means your data has a high standard deviation\n",
    "and variance i.e the values are spread out and highly varying. If you have long\n",
    "whiskers on one side of the box but not the other, then your data may be highly\n",
    "varying only in one direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function, division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"data/500_Person_Gender_Height_Weight_Index.csv\")\n",
    "df.head()\n",
    "\n",
    "f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)})\n",
    "mean=df['Height'].mean()\n",
    "median=df['Height'].median()\n",
    "mode=df['Height'].mode().get_values()[0]\n",
    "\n",
    "sns.boxplot(df[\"Height\"], ax=ax_box)\n",
    "ax_box.axvline(mean, color='r', linestyle='--')\n",
    "ax_box.axvline(median, color='g', linestyle='-')\n",
    "ax_box.axvline(mode, color='b', linestyle='-')\n",
    "\n",
    "sns.distplot(df[\"Height\"], ax=ax_hist)\n",
    "ax_hist.axvline(mean, color='r', linestyle='--')\n",
    "ax_hist.axvline(median, color='g', linestyle='-')\n",
    "ax_hist.axvline(mode, color='b', linestyle='-')\n",
    "\n",
    "plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n",
    "\n",
    "ax_box.set(xlabel='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **histogram**, which we plotted here above below the whisker plot is super-important: It tells us *how* the data is distributed and gives us the opportunity to find the function that approximates our data: It needs to produce (fake) data that has the same histogram as the real data. The histogram is a **probability distribution**.\n",
    "\n",
    ">**DEFINITION**: A **probability distribution** is a function which represents the probabilities of all possible values in an experiment\n",
    "\n",
    "Notice how we went from *frequencies* (of the value of a random variable), which is what you get in a histogram, to *probabilities* of a random variable. Does this surprise you? Shouldn't, we used our `p` function to easily go from 2-child Danish B/G frequencies to probability distributions for BB, BG, GB, GG. Probabilities is nothing more than renormalized frequencies!\n",
    "\n",
    ">**DEFINITION**: The **expected value** of a discrete random variable is the probability-weighted average of all its possible values. In other words, each possible value the random variable can assume is multiplied by its probability of occurring, and the resulting products are summed to produce the expected value. Intuitively, a random variable's expected value represents the mean of a large number of independent realizations of the random variable. The expected value is also known as the **expectation**, mathematical expectation, **mean**, or **first moment**.\n",
    "\n",
    "Here are some classic well-known theoretical distributions whose histograms are very well-defined and which happen to model data that usually occurs on planet Earth (much like the Fibonacci numbers and the Golden ratio appear in nature):\n",
    "\n",
    "- A [Bernoulli Distribution]() has only two possible outcomes and a single trial. A simple example can be a single toss of a biased/unbiased coin. In this example, the probability that the outcome might be heads can be considered equal to p and (1 - p) for tails (the probabilities of mutually exclusive events that encompass all possible outcomes needs to sum up to one).\n",
    "\n",
    "\n",
    "- A [binomial Distribution]() can be thought as the sum of outcomes of an event following a Bernoulli distribution. The Binomial Distribution is therefore used in binary outcome events and the probability of success and failure is the same in all the successive trials. This distribution takes two parameters as inputs: the number of times on event takes place and the probability assigned to one of the two classes. The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for N much larger than n, the binomial distribution remains a good approximatio. A simple example of a Binomial Distribution in action can be the toss of a biased/unbiased coin repeated a certain amount of times, or picking balls from an urn (with replacement) that contains balls of two different colors. The main characteristics of a Binomial Distribution are:\n",
    " - Given multiple trials, each of them is independent of each other (the outcome  of one trial doesn't affect another one).\n",
    " - Each trial can lead to just two possible results (eg. winning or losing), which have probabilities p and (1 - p).\n",
    "\n",
    "\n",
    "- A [Uniform Distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)) is the most basic of the seven we show here. It has a *single value* which only occurs in a certain range while anything outside that range is just 0. It’s very much an *on or off* distribution. We can also think of it as an indication of a\n",
    "categorical variable with 2 categories: 0 or the value. Your categorical variable might\n",
    "have multiple values other than 0 but we can still visualize it in the same was as a\n",
    "piecewise function of multiple uniform distributions.\n",
    "\n",
    "\n",
    "- A [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution), commonly referred to as a **Gaussian Distribution**, is specifically defined by its **mean** and **standard deviation**. The mean value shifts the\n",
    "distribution spatially (measure of *centrality*) and the standard deviation controls the *spread*. The import\n",
    "distinction from other distributions (e.g Poisson) is that the standard deviation is the\n",
    "*same in all directions*. Thus with a Gaussian distribution we know the average value\n",
    "of our dataset as well as the spread of the data i.e is it spread over a wide range or is\n",
    "it highly concentrated around a few values.\n",
    "\n",
    "\n",
    "- A [Poisson Distribution](https://en.wikipedia.org/wiki/Poisson_distribution) is similar to the Normal but with an added factor of skewness. With a low value for the skewness a poisson distribution will have\n",
    "relatively uniform spread in all directions just like the Normal. But when the\n",
    "skewness value is high in magnitude then the spread of our data will be different in\n",
    "one direction: It will be very *spread out* in one direction and in the other it will be\n",
    "*highly concentrated*. Poisson Processes are used to model a series of discrete events in which we know the average time between the occurrence of different events but we don’t know exactly *when* each of these events might take place. For example, events that can be counted (e.g. emails or text messages). A process can be considered to belong to the class of Poisson Processes if it can meet the following criterias:\n",
    " - The events are independent of each other (if an event happens, this does not alter the probability that another event can take place).\n",
    " - Two events can’t take place simultaneously.\n",
    " - The average rate between events occurrence is constant.\n",
    "\n",
    "\n",
    "- A [Gamma Distribution](https://en.wikipedia.org/wiki/Gamma_distribution) is a distribution that arises naturally in processes for which the ***waiting times between events are relevant***. It can be thought of as a **waiting time** between **Poisson distributed events**, such as in when you wait for the \"T\" (i.e. queuing models), climatology, and financial services. Examples of events that may be modeled by a gamma distribution include but are not limited to:\n",
    " \n",
    " - Public transportation\n",
    " - Amount of [rainfall](http://journals.tubitak.gov.tr/engineering/issues/muh-00-24-6/muh-24-6-7-9909-13.pdf). accumulated in a reservoir\n",
    " - The size of loan defaults or aggregate [insurance claims](https://www.crcpress.com/Statistical-and-Probabilistic-Methods-in-Actuarial-Science/Boland/p/book/9781584886952)\n",
    " - The flow of items through manufacturing and distribution processes\n",
    " - Visitors to a website\n",
    " - Customers calling a help center \n",
    " - Radioactive decay in atoms\n",
    " - Movements in a stock price\n",
    "\n",
    "\n",
    "- A [Random walk](https://en.wikipedia.org/wiki/Random_walk) (or [Brownian motion](https://en.wikipedia.org/wiki/Brownian_motion)) can be any sequence of discrete steps (of always the same length) moving in **random directions**. Random Walks can take place in any type of dimensional space (eg. 1D, 2D, nD).\n",
    "Specfically, a **Random Walk** is used to describe a *discrete-time process* while **Brownian Motion** can be used to describe a continuous-time random walk. Some examples of random walks applications are: tracing the path taken by molecules when moving through a gas during the diffusion process, sports events predictions etc…\n",
    "\n",
    "\n",
    "There are more theoretical distributions we'll learn in class but these seven already give\n",
    "us a lot of value. We can quickly see and interpret our categorical variables with a\n",
    "Uniform Distribution. There are many algorithms that by default will perform well specifically \n",
    "with Gaussian so we should use Gaussian Distribution as much as possible. And the Poisson and the Gamma/Brown \n",
    "model a lot of natural processes, respectively for discrete and continuous events.\n",
    "\n",
    "In our last notebook, when we examined a Celtics team, we saw how when we add a lot of \n",
    "distributions together we tend to get a nice gaussian distribution, and for good reason,\n",
    "there is a great theorem behind this fact!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Uniform distribution\n",
    "\n",
    "Simplest of all: same probability $y$ for all possible values of the random variable $x$. Becomes apparent when we plot a lot of experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Uniform\n",
    "\n",
    "y = Uniform.dist()\n",
    "samples = y.random(size=1000)\n",
    "print(samples.mean())\n",
    "print(samples.std())\n",
    "plt.plot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = y.random(size=10000)\n",
    "print(samples.mean())\n",
    "print(samples.std())\n",
    "plt.hist(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the latter histogram *flatter*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bernoulli Distribution\n",
    "\n",
    "Here we use `Scipy` to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# size = number of experiments\n",
    "# p = probability of success\n",
    "bern = stats.bernoulli.rvs(size=10000,p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= sns.distplot(bern,\n",
    "                 kde=False)\n",
    "ax.set(xlabel='Bernoulli Distribution', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_bern = stats.bernoulli.rvs(size=1000,p=0.7)\n",
    "\n",
    "ax= sns.distplot(biased_bern,\n",
    "                 kde=False)\n",
    "ax.set(xlabel='Bernoulli Distribution', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a **model** for a fair coin toss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.full((2), 1/2)\n",
    "face = [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(face, probs)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Coin Toss Outcome', fontsize=12)\n",
    "plt.title('Coin Toss Bernulli Distribution', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a **model** for a biased coin toss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.array([0.75, 0.25])\n",
    "face = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(face, probs)\n",
    "plt.title('Loaded coin Bernoulli Distribution', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Loaded coin Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The binomial Distribution\n",
    "\n",
    "The Binomial Distribution can be thought as the sum of outcomes of an event following a Bernoulli distribution. It takes two parameters as inputs: the number of times on event takes place and the probability assigned to one of the two classes.\n",
    "\n",
    "The probability of getting exactly k successes in n independent Bernoulli trials is given by: \n",
    "\n",
    "$$p(k,n,p) = (^n_k) p^k (1-p)^{n-k}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$(^n_k) = \\frac{n!}{k!\\;(n-k)!}$$\n",
    "\n",
    "The probability of getting exactly k successes in n independent Bernoulli trials can be plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = number of experiments\n",
    "# n = number of trials (number of trials for each experiment)\n",
    "# p = probability that one of two events is going to take place\n",
    "binomial_data = stats.binom.rvs(size=10000, n=10, p=0.5, random_state=0)\n",
    "pd.Series(binomial_data).plot(kind=\"hist\", bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for n=10 and p=0.5, the probably of 5 successes is the highest, and the distribution of the number of successes is very gaussian-like!\n",
    "\n",
    "Changing the probability value changes how the distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prob in range(2, 10, 3):\n",
    "    binomial_data = stats.binom.rvs(size=10000, n=10, p=0.1*prob, \n",
    "                                    random_state=0)\n",
    "    \n",
    "    pd.Series(binomial_data).plot(kind=\"hist\", bins = 20, \n",
    "                              label=\"p = {:f}\".format(0.1*prob))\n",
    "    plt.xlabel('Random Variable', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title(\"Binomial Distribution varying p\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It *skews* the distribution to the left or the right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Normal Distribution\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/ohnonotmath.png\" width=200 />\n",
    "</center>\n",
    "\n",
    "Also known as the **gaussian** distribution.\n",
    "\n",
    "For ***continuous random variables***:\n",
    "\n",
    "$$X \\in [0,1]$$\n",
    "\n",
    "$$Y \\in (-\\infty, \\infty)$$\n",
    "\n",
    "Its **probability *density* function** (pdf): \n",
    "\n",
    "For continuous $X$,\n",
    "\n",
    "$$Pr(x \\le X \\le x + dx) = f(x|\\theta)dx \\, \\text{ as } \\, dx \\rightarrow 0$$\n",
    "\n",
    "![Continuous variable](https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 110%;\">  \n",
    "$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right]$$\n",
    "</div>\n",
    "\n",
    "* $X \\in \\mathbf{R}$\n",
    "* $\\mu \\in \\mathbf{R}$\n",
    "* $\\sigma>0$\n",
    "\n",
    "The **expectation** (or first moment), and the **standard deviation** (or second moment) of a gaussian are:\n",
    "$$\\begin{align}E(X) &= \\mu \\cr\n",
    "\\text{Var}(X) &= \\sigma^2 \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Normal\n",
    "\n",
    "y = Normal.dist(mu=-2, sd=4)\n",
    "samples = y.random(size=10000)\n",
    "print(samples.mean())\n",
    "print(samples.std())\n",
    "plt.plot(samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(samples)\n",
    "ax_hist.axvline(samples.mean(), color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to use `Scipy` to model the normal distribution, you would use the ***r***andom ***v***ariate***s*** API `.rvs()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = number of data points\n",
    "# loc = mean\n",
    "# scale = standard deviation\n",
    "normal_dist = stats.norm.rvs(size=10000, loc=200, scale=9, \n",
    "                             random_state=0) \n",
    "\n",
    "pd.Series(normal_dist).plot(kind=\"hist\", bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot different means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean in range(400, 900, 200):\n",
    "    normal_dist = stats.norm.rvs(size=10000, loc=mean, scale=90, \n",
    "                             random_state=0) \n",
    "    pd.Series(normal_dist).plot(kind=\"hist\", bins=100, \n",
    "                                label=\"Mean = {:f}\".format(mean))\n",
    "    plt.xlabel('Distribution', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title(\"Normal Distribution varying the mean\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Poisson distribution\n",
    "\n",
    "For ***discrete random variables***, the Poisson distribution models **unbounded counts**:\n",
    "\n",
    "$$X = \\{0,1,2,3,\\ldots \\}$$\n",
    "\n",
    "$$Y = \\{\\ldots,-2,-1,0,1,2,\\ldots\\}$$\n",
    "\n",
    "Its **probability *mass* function**: \n",
    "\n",
    "$$Pr(X=x) = f(x\\;|\\; \\theta)$$\n",
    "\n",
    "![Discrete variable](http://upload.wikimedia.org/wikipedia/commons/1/16/Poisson_pmf.svg)\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$Pr(X=x)=\\frac{e^{-\\lambda}\\lambda^x}{x!}$$\n",
    "</div>\n",
    "\n",
    "* $X=\\{0,1,2,\\ldots\\}$\n",
    "* $\\lambda > 0$\n",
    "\n",
    "One useful property of the Poisson distribution is that its **expectation** (or first moment) as well as its **standard deviation** (or second moment) are equal to its *sole* parameter, i.e.:\n",
    "\n",
    "$$E(X) = \\text{Var}(X) = \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take random samples from a Poisson process and plot their histogram. You will see it reproduces a Poisson distribution. That is to say, random samples from a Poisson process yields a Poisson Distribution. This means that the statistics of a Poisson process are not exactly completely random. Rather, they follow a predictable behavior.\n",
    "\n",
    "That is because Poisson processes model **counts**. For example, counts of SMS messages you receive per day. Let's say, if you're a popular student, you receive from 0 to 100 messages a day. On every single day, I can thus expect you receive about 50 messages, right? It's a rare day you receive 0, or 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Poisson\n",
    "\n",
    "x = Poisson.dist(mu=1)\n",
    "samples = x.random(size=100)\n",
    "print(samples.mean())\n",
    "print(samples.std())\n",
    "plt.plot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Poisson.dist(mu=1)\n",
    "samples = x.random(size=1000)\n",
    "print(samples.mean())\n",
    "print(samples.std())\n",
    "plt.plot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples, bins=len(set(samples)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `NumPy` to model a Poisson process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.poisson(5, 10000)\n",
    "plt.hist(s, 14, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.poisson(lam=(100., 500.), size=(5, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see how the cumulative sum of a Poisson random variable is a linear curve, since the expectation is constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # Events\n",
    "lambdas = [0.5, 2, 4]\n",
    "X_T = [np.random.poisson(lam, size=N) for lam in lambdas]\n",
    "S = [[np.sum(X[0:i]) for i in range(N)] for X in X_T]\n",
    "X = np.linspace(0, N, N)\n",
    " \n",
    "# Plot the graph\n",
    "graphs = [plt.step(X, S[i], label=\"λ = %f\"%lambdas[i])[0] \n",
    "          for i in range(len(lambdas))]\n",
    "plt.legend(handles=graphs)\n",
    "plt.title(\"Poisson Process models unbounded counts\")\n",
    "plt.xlabel(\"Events\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gamma distribution\n",
    "\n",
    "This is the [**gamma distribution**](https://en.wikipedia.org/wiki/Gamma_distribution):\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$f(x) = \\frac{\\beta^\\alpha x^{\\alpha -1} e^{- \\beta x}}{\\Gamma (\\alpha)}$$\n",
    "</div>\n",
    "\n",
    "And... what is the $\\Gamma()$ function? It [interpolates](https://en.wikipedia.org/wiki/Gamma_function) the factorial function to non-integer values. Pretty cool.\n",
    "\n",
    "The gamma distribution is a ***two-parameter*** ($\\alpha, \\beta$) family of ***continuous*** probability distributions. There are [three different parametrizations](https://en.wikipedia.org/wiki/Gamma_distribution) in common use! For the one here (more common in Bayesian statistics), the **expectation** (or first moment) of the Gamma is $E[x] = α/β$.\n",
    "\n",
    "In Bayesian statistics, the **gamma distribution** is used as a [**conjugate prior**](https://en.wikipedia.org/wiki/Conjugate_prior) distribution for the **exponential distribution** and the **Poisson distribution**. In Bayesian probability theory (which we'll look into soon), if the posterior distributions $p(θ \\;|\\; x)$ are in the same probability distribution family as the prior probability distribution $p(θ)$, the prior and posterior are then called **conjugate distributions**.\n",
    "\n",
    "Recall Bayes' formula:\n",
    "\n",
    "$$p(θ \\; | \\; x) = \\frac{p(x \\;| \\; θ) \\; p(θ)}{p(x)}$$\n",
    "\n",
    ">**DEFINITIONS**: $p(x \\;| \\; θ)$ is called the **likelihood** function, $p(θ \\; | \\; x)$ is called the **posterior** function, and $p(θ)$ is called the **prior** function.\n",
    "\n",
    "For $x > 0$ and $\\alpha, \\beta > 0$\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Gamma_distribution_pdf.svg\" width=\"500\" />\n",
    "Gamma distribution\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Gamma\n",
    "\n",
    "x = Poisson.dist(mu=1)\n",
    "samples = x.random(size=1000)\n",
    "print(samples.mean())\n",
    "print(samples.std())\n",
    "plt.plot(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples, bins=len(set(samples)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very tempting to confuse a Gamma for a Poisson. Be especially careful about the underlying model. Are you estimating *discrete* or *continuous* random variables?\n",
    "\n",
    "In Bayesian inference, the gamma distribution is the conjugate prior to many likelihood distributions: the Poisson, [exponential](https://en.wikipedia.org/wiki/Exponential_distribution), normal (with known mean), Pareto, gamma itself with known shape parameter, inverse gamma with known shape parameter, and [Gompertz](https://en.wikipedia.org/wiki/Gompertz_distribution) with known scale parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random walk/Brownian motion\n",
    "\n",
    "Let's start in 1D. Doesn't this look very much like a profile of a stock price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for the walk\n",
    "dims = 1\n",
    "step_n = 50000\n",
    "step_set = [-1, 1]\n",
    "origin = np.zeros((1,dims))\n",
    "# Simulate steps in 1D\n",
    "step_shape = (step_n,dims)\n",
    "steps = np.random.choice(a=step_set, size=step_shape)\n",
    "path = np.concatenate([origin, steps]).cumsum(0)\n",
    "start = path[:1] # first element \n",
    "stop = path[-1:] # last element\n",
    "# Plot the path\n",
    "#figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(np.arange(step_n+1), path, c='blue',alpha=0.35, s=0.005);\n",
    "plt.plot(0, start, c='red', marker='o')\n",
    "plt.plot(step_n, stop, c='black', marker='o')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.title('Random Walk in 1D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for the walk\n",
    "dims = 2\n",
    "step_n = 50000\n",
    "step_set = [-1, 1]\n",
    "origin = np.zeros((1,dims))\n",
    "# Simulate steps in 1D\n",
    "step_shape = (step_n,dims)\n",
    "steps = np.random.choice(a=step_set, size=step_shape)\n",
    "path = np.concatenate([origin, steps]).cumsum(0)\n",
    "start = path[:1] # first element \n",
    "stop = path[-1:] # last element \n",
    "# Plot the path\n",
    "#figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(path[:,0], path[:,1],c='blue', alpha=0.35, s=0.005);\n",
    "plt.plot(start[:,0], start[:,1],c='red', marker='o')\n",
    "plt.plot(stop[:,0], stop[:,1],c='black', marker='o')\n",
    "plt.xlabel(\"X Steps\")\n",
    "plt.ylabel(\"Y Steps\")\n",
    "plt.title('Random Walk in 2D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# Defining parameters for the walk\n",
    "dims = 3\n",
    "step_n = 5000\n",
    "step_set = [-1, 1]\n",
    "origin = np.zeros((1,dims))\n",
    "# Simulate steps in 3D\n",
    "step_shape = (step_n,dims)\n",
    "steps = np.random.choice(a=step_set, size=step_shape)\n",
    "path = np.concatenate([origin, steps]).cumsum(0)\n",
    "start = path[:1] # first element \n",
    "stop = path[-1:] # last element\n",
    "# Plot the path\n",
    "fig = figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlabel('X Steps')\n",
    "ax.set_ylabel('Y Steps')\n",
    "ax.set_zlabel('Z Steps')\n",
    "ax.scatter3D(path[:,0], path[:,1], path[:,2], c='blue', alpha=0.35, s=0.3)\n",
    "ax.plot3D(start[:,0], start[:,1], start[:,2], c='red', marker='o')\n",
    "ax.plot3D(stop[:,0], stop[:,1], stop[:,2], c='black', marker='o')\n",
    "plt.title('Random Walk in 3D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you plot it in 4D?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf, cdf, ppf\n",
    "\n",
    "A [probability density function](https://en.wikipedia.org/wiki/Probability_density_function) (pdf) gives us the probability of a random variable assuming a certain value ($y$ from $x$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.pdf(x=168,         # Value to check for the random variable\n",
    "               loc=174,       # mean\n",
    "               scale=8.2)     # standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.pdf(x=174,         # Value to check for the random variable\n",
    "               loc=174,       # mean\n",
    "               scale=8.2)     # standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.pdf(x=180,         # Value to check for the random variable\n",
    "               loc=174,       # mean\n",
    "               scale=8.2)     # standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist = stats.norm.rvs(size=10000, loc=174, scale=8.2, \n",
    "                             random_state=0) \n",
    "\n",
    "f = pd.Series(normal_dist).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for kicks.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "normal_dist_scaled = preprocessing.MinMaxScaler().fit_transform(pd.DataFrame(normal_dist, columns=['Score']))\n",
    "normal_dist_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(normal_dist_scaled).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.Series(np.squeeze(normal_dist_scaled)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.Series(normal_dist).plot(kind=\"hist\", bins=100)\n",
    "f.axvline(180, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate normal random variates, then plot their **histogram**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean in range(400, 900, 200):\n",
    "    normal_dist = stats.norm.rvs(size=10000, loc=mean, scale=90, \n",
    "                             random_state=0) \n",
    "    pd.Series(normal_dist).plot(kind=\"hist\", bins=100, \n",
    "                                label=\"Mean = {:f}\".format(mean))\n",
    "    plt.xlabel('Distribution', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title(\"Normal Distributions using histograms on random variates\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the **pdf** of the same normal random variates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean in range(400, 900, 200):\n",
    "    n = np.arange(0, 1200)\n",
    "    normal = stats.norm.pdf(n, mean, 90)\n",
    "    plt.plot(n, normal, label=\"Mean = {:f}\".format(mean))\n",
    "    plt.xlabel('Distribution', fontsize=12)\n",
    "    plt.ylabel('Probability', fontsize=12)\n",
    "    plt.title(\"Normal Distributions using their pdfs\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the **pdf** is the *theoretical* **histogram** of a known distribution!\n",
    "\n",
    "The [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (CDF) gives us the probability of a certain random observation will have a lower value than the one provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.cdf(x=186,         # Cutoff value (quantile) to check\n",
    "               loc=174,       # Mean\n",
    "               scale=8.2)     # Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [percent point function](https://en.wikipedia.org/wiki/Quantile_function) (PPF), also called [quantile function](https://en.wikipedia.org/wiki/Quantile_function) gives us the quantile using a probability as input (inverse of cumulative distribution function) ($x$ from $y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.ppf(q=0.93,      # Cutoff value (quantile) to check\n",
    "               loc=174,     # Mean\n",
    "               scale=8.2)   # Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Exercise\n",
    "\n",
    "- How to model throwing a die with data science? Is that a perfectly random event, or not? What is the histogram that matches the experiment? Build a **model**.\n",
    "\n",
    "- Build a **weighted die** that matches a gaussian distribution instead (in other words, build a model for it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: none;\">\n",
    "x = [0,1,2,3,4,5,6]\n",
    "counts, bins = np.histogram(x, bins= 100)\n",
    "bins = bins[:-1] + (bins[1] - bins[0])/2\n",
    "probs = counts/float(counts.sum())\n",
    "print(probs.sum()) # 1.0\n",
    "plt.bar(bins, probs)\n",
    "plt.show()\n",
    "\n",
    "probs = np.full((6), 1/6)\n",
    "face = \\[1,2,3,4,5,6]\n",
    "plt.bar(face, probs)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Dice Roll Outcome', fontsize=12)\n",
    "plt.title('Fair Dice Uniform Distribution', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "\n",
    "probs = np.array([0.25, 0.125, 0.05, 0.4, 0.125, 0.05])\n",
    "face = [1,2,3,4,5,6]\n",
    "plt.bar(face, probs)\n",
    "plt.title('Loaded dice (not uniform distribution)', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Dice Roll Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "\n",
    "n = np.arange(1, 7)\n",
    "mean = 3.5\n",
    "gaussian_dice = stats.norm.pdf(n, mean, 0.9)\n",
    "plt.plot(n, gaussian_dice)\n",
    "\n",
    "face = [1,2,3,4,5,6]\n",
    "plt.bar(face, gaussian_dice)\n",
    "plt.title('Loaded dice (gausian distribution)', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xlabel('Dice Roll Outcome', fontsize=12)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"ipynb.images/birds-up.png\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world data can be messy. Some datasets contain attributes that need severe modifications before they can be used to do predictive modeling. What do I mean by that? I mean that ***more reliable predictions can be made if the predictors and the\n",
    "target variable are closer to a normal distribution***. One way to getting them there if they're not is to *apply a transform to them*. Let's look at an example.\n",
    "\n",
    "We're in Boston, so let's use a familiar Boston Housing Prices dataset to explore some techniques of\n",
    "dealing with skewed data. There is no need to download it, as we can import it straight from `Scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "x = boston['data']\n",
    "y = boston['target']\n",
    "cols = boston['feature_names']\n",
    "\n",
    "df = pd.DataFrame(x, columns = cols)\n",
    "df['Price'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How skewed is the `CRIM` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CRIM'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, that's a lot! Let's use the Seaborn library to make a histogram alongside with the KDE plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['CRIM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s explore some methods for handling skewed data.\n",
    "\n",
    "## Log Transform\n",
    "Log transformation is most likely the first thing you should do to remove skewness from the predictor.\n",
    "It can be easily done via `Numpy`, just by calling the log() function on the desired column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crim_log = np.log(df['CRIM'])\n",
    "crim_log.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(crim_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much nicer, right? It’s not normally distributed for sure, but is a lot better than what we had before!\n",
    "\n",
    "As you would expect, the log transformation isn’t the only one you can use. Let’s explore a couple of more options.\n",
    "\n",
    "## Square Root Transform\n",
    "The square root sometimes works great and sometimes isn’t the best suitable option. In this case, I still expect the transformed distribution to look somewhat exponential, but just due to taking a square root the range of the variable will be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crim_sqrt = np.sqrt(df['CRIM'])\n",
    "crim_sqrt.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(crim_sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is pretty much the same, but the range is smaller, as expected.\n",
    "\n",
    "## Box-Cox Transform\n",
    "The [Box-Cox transform](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation), named after statisticians [George Box](http://mathshistory.st-andrews.ac.uk/Biographies/Box.html) and [Sir David Roxbee Cox](https://www.britannica.com/biography/David-Cox-British-statistician) who collaborated on a 1964 paper and developed the technique, is another way of handling skewed data. To use it, your data must be positive — so that can be a bummer sometimes.\n",
    "\n",
    "At the core of the Box Cox transformation is an exponent, $λ$, which varies from -5 to 5. All values of $λ$ are considered and the optimal value for your data is selected; The *optimal value* is the one which results in the best approximation of a normal distribution curve.\n",
    "\n",
    "You can import the Box-Cox transform from `Scipy`, but the check for the skew you’ll need to convert the resulting Numpy array to a Pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "crim_boxcox = stats.boxcox(df['CRIM'])[0]\n",
    "pd.Series(crim_boxcox).skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(crim_boxcox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is pretty similar to the one made by the log transformation, but just a touch less bimodal in dispersion (since the two bumps are closer together), but more bimodal in centrality (since the two bumps are of almost equal magnitude).\n",
    "\n",
    "Skewed data can mess up the power of your predictive model if you don’t address it correctly.\n",
    "\n",
    "Should go without saying, but you should remember what transformation you’ve performed on which attribute, because you’ll *have to reverse it once when making predictions*!\n",
    "\n",
    "In general, conducting transformations on the data before modeling it goes by the name of **kernel methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Of Moments (MOM) \n",
    "\n",
    "We'll illustrate the **Method of Moments** (MOM) with ***an example***. \n",
    "\n",
    "When you take a ***bad*** class in statistics, where you don't udnerstand or tend to fall asleep at the helm, it's the ***fault of the professor, not yours***.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/asleep.jpg width = 400 />\n",
    "</center>\n",
    "\n",
    "A good professor with give you the relevant equations and apply them to an ***interesting problem***. Then you’ll have an appreciation for how statistics helps us to understand the world. There are many **amazing phenomena** in the world, and we use data science as a tool for exploring them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal of Data Science\n",
    "\n",
    "So, Neanderthal data scientists try to match the histogram of datasets to well-known theoretical profiles, and then they can use the theoretical profile (gaussian, gamma, etc.) to simulate data! \n",
    "\n",
    "With computers, ***we can do the same thing***, i.e. find the best approximation to our empirical histogram and then use libraries like `SciPy` to build simulations of (the process underlying) our data.\n",
    "\n",
    "> **IMPORTANT**: Why do we want to produce data that *looks* different and yet follows the same histogram? Because we theorize that by capturing the histogram, we are modelling the *process* that produced the data, which is what we're really after. The process is bound to produce different-looking data every time, as long as it follows the precise statistics of the process. That is what **statistical science** is all about.\n",
    "\n",
    "To understand this better, let's do some labs and try to leverage the parametrized distributions we just learned as potential models to datasets we observe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab \\#1: Nashville Precipitation\n",
    "\n",
    "The dataset `nashville_precip.txt` contains NOAA precipitation data for the city of Nashville, Tennessee, measured since 1871. It is a classical dataset like the iris dataset. Download it from blackboard and put it in the right folder (you know..). The gamma distribution is a good fit to aggregated rainfall data, and will be our candidate distribution in this case. We'll use the gamma distribution model for Nashville precipitation to demonstrate MOM and MLE.\n",
    "\n",
    "Let's peek at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = pd.read_table(\"data/nashville_precip.txt\", index_col=0, na_values='NA', delim_whitespace=True)\n",
    "precip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the year 1971, in the month of January, Nashville saw 2.76 inches of rainfall.\n",
    "\n",
    "### 1.1 Data Exploration: The Histogram\n",
    "\n",
    "Let's do some data exploration with the ***histogram*** of precipitations per month. The histogram tells us, for each month of the year, what's the most frequent number of inches of rain from 1871 onward.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/rainfall.jpeg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = precip.hist(sharex=True, sharey=True, grid=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is recognizing what sort of distribution to fit our data to. A couple of observations:\n",
    "\n",
    "1. The data is *skewed*, with a longer tail to the right than to the left\n",
    "2. The data is positive-valued, since they are measuring rainfall\n",
    "3. The data is continuous\n",
    "\n",
    "Even just by ***eyeballing*** the histograms, ***even if you did not know what the Gamma distribution is usually used for***, a good option appears to be the **gamma distribution**: The curves look like ***Gaussians with long tails***. That's a Gamma!\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$x \\sim \\text{Gamma}(\\alpha, \\beta) = \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-\\beta x}}{\\Gamma(\\alpha)}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Cleansing \n",
    "\n",
    "***Wait***, there's something ***wrong*** with our data.. if you open it in a text editor, you will find a value of **NA** for October of 1963 (take a look). \n",
    "\n",
    "So we have to do some **data cleansing** first. \n",
    "\n",
    "Given what we are trying to do, it is ***sensible*** to fill in the missing value with the average of the available values (another option would have been the average of the months of September and November 1963). Filling in with 0 would be a *bad idea*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.fillna(value={'Oct': precip.Oct.mean()}, inplace=True)\n",
    "precip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Method of Moments\n",
    "\n",
    "The **method of moments** simply assigns the **empirical** (coming from the data) **mean** and **variance** to their **theoretical counterparts** (coming from the *model*, in this case the Gamma!), so that we can ***solve for the parameters*** of the Gamma.\n",
    "\n",
    "So, for the gamma distribution, the mean and variance turn out to be (analytically derived):\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\hat{\\mu} = \\bar{X} = \\alpha \\beta $$\n",
    "$$ \\hat{\\sigma}^2 = S^2 = \\alpha \\beta^2 $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we solve for these parameters, we can use a gamma distribution to describe our data, with parameters:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\alpha = \\frac{\\bar{X}^2}{S^2}, \\, \\beta = \\frac{S^2}{\\bar{X}} $$\n",
    "</div>\n",
    "\n",
    "($\\bar{X}$ is the mean, while $S^2$ is the variance of each column of the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the sample ***moments of interest***: the **means** and **variances** *month by month*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_mean = precip.mean()\n",
    "precip_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_var = precip.var()\n",
    "precip_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use these moments to estimate $\\alpha$ and $\\beta$ for each month:\n",
    "```python\n",
    "alpha_mom = ...\n",
    "beta_mom = ...\n",
    "alpha_mom, beta_mom\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "alpha_mom = precip_mean ** 2 / precip_var\n",
    "beta_mom = precip_var / precip_mean\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_mom = precip_mean ** 2 / precip_var\n",
    "beta_mom = precip_var / precip_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_mom, beta_mom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `gamma.pdf` function in `scipy.stats.distributions` to plot the distributions implied by the calculated alphas and betas. Yes, yes, I know, we have not *officially* introduced scipy yet..\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"https://c402277.ssl.cf1.rackcdn.com/photos/14785/images/story_full_width/shutterstock_532108075.jpg\" width=400 />\n",
    "Lazy Professor\n",
    "</center>\n",
    "\n",
    "For example, here is January (and note that scipy's gamma is the *gamma distribution*, not the $\\Gamma$ function that interpolates $n!$ to real numbers, even though the $\\Gamma$ function figures in the denominator of the gamma distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import gamma\n",
    "\n",
    "precip.Jan.hist(normed=True, bins=20)\n",
    "plt.plot(np.linspace(0, 10), gamma.pdf(np.linspace(0, 10), alpha_mom[0], beta_mom[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for january :-) Looping over all months now, we can create a grid of plots for the distribution of rainfall, using the gamma distribution:\n",
    "\n",
    "```python\n",
    "axs = precip.hist(normed=True, figsize=(12, 8), sharex=True, sharey=True, bins=15, grid=True)\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    \n",
    "    # Get month\n",
    "    m = ax.get_title()\n",
    "    \n",
    "    # Plot fitted distribution\n",
    "    x = np.linspace(*ax.get_xlim())\n",
    "    ax.plot(x, gamma.pdf(x, alpha_mom[m], beta_mom[m]))\n",
    "    \n",
    "    # Annotate with parameter estimates\n",
    "    label = 'alpha = {0:.2f}\\nbeta = {1:.2f}'.format(alpha_mom[m], beta_mom[m])\n",
    "    ax.annotate(label, xy=(10, 0.2))\n",
    "    \n",
    "plt.tight_layout()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "axs = precip.hist(normed=True, figsize=(12, 8), sharex=True, sharey=True, bins=15, grid=True)\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    \n",
    "    # Get month\n",
    "    m = ax.get_title()\n",
    "    \n",
    "    # Plot fitted distribution\n",
    "    x = np.linspace(*ax.get_xlim())\n",
    "    ax.plot(x, gamma.pdf(x, alpha_mom[m], beta_mom[m]))\n",
    "    \n",
    "    # Annotate with parameter estimates\n",
    "    label = 'alpha = {0:.2f}\\nbeta = {1:.2f}'.format(alpha_mom[m], beta_mom[m])\n",
    "    ax.annotate(label, xy=(10, 0.2))\n",
    "    \n",
    "plt.tight_layout()\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job, that was **data science** you did!\n",
    "\n",
    "### 1.4 Conclusion\n",
    "\n",
    "In math, a **moment** is a specific quantitative measure of the ***shape of a set of points***. \n",
    "\n",
    "The *zeroth* moment is the total mass, the *first* moment is the center of mass, the *second* moment is rotational inertia. Oops.. this is not mechanics 101, it's statistics 101...\n",
    "\n",
    "So, the *zeroth* moment is total probability, *first* moment is the [mean](https://en.wikipedia.org/wiki/Mean), *second* moment is the [variance](https://en.wikipedia.org/wiki/Variance), *third* moment is the [skewness](https://en.wikipedia.org/wiki/Skewness), *fourth* moment is the [kurtosis](https://en.wikipedia.org/wiki/Kurtosis). And you can keep on going...\n",
    "\n",
    "For a distribution of mass or probability on a bounded interval, the collection of all the moments (of all orders, from 0 to $\\infty$) ***uniquely determines the distribution***. This is related to [Taylor's approximation theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem).\n",
    "\n",
    "Minimum number of moments we need in the **Method of Moments** equal the ***number of the parameters in the estimator***! For each moment, we equate the moment of the ***model + parameters*** to the moment of the dataset. That gives us ***one*** equation for the parameters. We will need as many equations (thus moments) as there are parameters!\n",
    "\n",
    "The method of Moments is ***easy-peasy***, as long as we have analytic formulas for the moments for the model distribution (which we usually do, *that is why* they are **models**). \n",
    "\n",
    "But they're usually *not* the most efficient estimators.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Maximum likelihood estimation** (MLE) fitting is usually ***more work*** than the method of moments, but is ***preferred*** as the resulting estimator is known to have ***good theoretical properties***. MLE also uses the\n",
    "same math principles as Neural Network models, namely *minimize a function by finding where its derivative is zero*.\n",
    "\n",
    "MLE is a method of estimating the *parameters* of a statistical model, given observations. MLE attempts to find the parameter values that ***maximize the likelihood function, given the observations***. \n",
    "\n",
    "Essentially, we attempt to find the values of the parameters (estimators) which would most likely, from a probability perspective, produce the data that we observed. We go from model to the data. We do not have the model, but we do have the data. So it is an ***inverse problem*** (btw, Inverse problems was the topic of my PhD thesis).\n",
    "\n",
    "[Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) and [Pierre-Simon Laplace](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace) where early users of maximum likelihood. [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) popularized it between 1912 and 1922, but it remained rigorously unproven until [Samuel S. Wilks](https://en.wikipedia.org/wiki/Samuel_S._Wilks) in 1938. [Wilks' theorem](https://en.wikipedia.org/wiki/Wilks%27_theorem) shows that the ***error in the logarithm of likelihood values for estimates from multiple independent samples is asymptotically distributed***. Wilks gave with his most general proof of the theorem in 1962.\n",
    "\n",
    "MLE is used often with **count models** (general rule of thumb for Count models is that it is risky to use ML with samples smaller than 100, while samples over 500 seem adequate, so if you have only a couple hundred datapoints, ML won't work! In which case you need to fall back to theory, which is what we're doing here), **and** when we are interested in a dataset $(x,y)$ but are unable to obtain $y$ for the entire population $x$ and only able to obtain the $y$'s for a ***subset*** of $x$. \n",
    "\n",
    "So we assume all $y$ are **normally (gaussian) distributed** with some unknown **mean** and **variance**. The mean and variance are estimated with MLE while only knowing the $y$'s of the subset of $x$. \n",
    "\n",
    ">**Advice**: In general, when you have no idea about the distribution of data, and there are too few datapoints for a good historgram, start by assuming a gaussian distribution.\n",
    "\n",
    "Here's the secret sauce of the algorithm:\n",
    "\n",
    "- MLE takes the mean and variance as **parameters**, and finds values for these parameters that make the observed results (the subset) the ***most probable given the gaussian model***. The analysis is an **iterative** one, which proceeds until a metric called the **log likelihood** ***converges***.\n",
    "\n",
    "You will see it does sound very much like **variational inference** and **Markov Chain Monte Carlo** (MCMC) methods (when we'll look at these in class), but it's actually ***simpler***. From the point of view of Bayesian inference, MLE is a special case of [**maximum a posteriori estimation**](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP) that assumes a **uniform prior** distribution of the parameters. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model.\n",
    "\n",
    "First, let's start with a practical introduction to the notion of a [derivative](https://en.wikipedia.org/wiki/Derivative).\n",
    "\n",
    ">**DEFINITION**: The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function f(x) and mathematically evaluate f'(x)\n",
    "def f(x):\n",
    "    return (x/5)**3\n",
    "\n",
    "def fp(x):\n",
    "    return (3./5.)*(x/5)**2\n",
    "\n",
    "# Interval x\n",
    "xleft = -10.\n",
    "xright = 10.\n",
    "\n",
    "# Restrict the y range for nicer plots [optional]\n",
    "ybottom = -8.\n",
    "ytop = 8.\n",
    "\n",
    "# plot the curve and derivative\n",
    "X = np.linspace(xleft, xright, 1000)\n",
    "Y = [f(x) for x in X]\n",
    "Z = [fp(x) for x in X]\n",
    "plt.plot(X,Y)\n",
    "plt.plot(X,Z)\n",
    "#slider = plot_tangent(f,fp,xleft,xright,ybottom,ytop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(4/x)+np.cos(x)# function goes here\n",
    "\n",
    "def fp(x):\n",
    "    return (-4*x**-2)*np.cos(4/x)-np.sin(x)# derivative goes here\n",
    "\n",
    "# x interval\n",
    "xleft = 0.3\n",
    "xright = 7\n",
    "\n",
    "# y interval\n",
    "ybottom = -0.5\n",
    "ytop = 2\n",
    "\n",
    "# plot the curve and derivative\n",
    "X = np.linspace(xleft, xright, 1000)\n",
    "Y = [f(x) for x in X]\n",
    "Z = [fp(x) for x in X]\n",
    "plt.plot(X,Y)\n",
    "plt.plot(X,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Example data set\n",
    "\n",
    "Say we have some data $y = y_1,y_2,\\ldots,y_n$ that is distributed according to some distribution:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$Pr(Y_i=y_i \\; | \\;\\lambda)$$\n",
    "</div>\n",
    "\n",
    "Assume the data is drawn from a Poisson distribution with parameter $\\lambda =5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = np.random.poisson(5, size=100)\n",
    "#plt.hist(y, bins=12, normed=True)\n",
    "#plt.xlabel('y'); plt.ylabel('Pr(y)')\n",
    "\n",
    "y = np.random.poisson(5, size=100)\n",
    "\n",
    "print(list(y))\n",
    "plt.figure(0)\n",
    "plt.plot(list(y))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.hist(y, bins=12, normed=True)\n",
    "plt.xlabel('y'); plt.ylabel('Pr(y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The theory\n",
    "\n",
    "The **likelihood function $l$** calculates the joint probability of observing $all$ the values of the dependent variable in our dataset, one after the other, when we evaluate our model. So it's an **intersection** of events (really the intersection of *all possible events*), thus a **product of probabilities**. \n",
    "\n",
    "It assumes that each observation is drawn *randomly* and *independently* from the population. If the values of the dependent variable are random and independent, then you can find the joint probability of observing all the values simultaneously by multiplying the individual density functions:\n",
    "\n",
    "$$l = \\prod_{i=1}^n Pr(y_i \\;| \\;\\lambda)$$ \n",
    "\n",
    "$l$ will give us a measure of how **likely we are** to observe values $y_1,\\ldots,y_n$ given the parameter $\\lambda$. \n",
    "\n",
    "**Maximum likelihood fitting** consists of **maximizing $l$** so that this outcome is **the most likely**. We call this function the *likelihood function*, because it is a measure of ***how likely the observations are if the model is true***.\n",
    "\n",
    "We are essentially doing a kind of Bayesian inference: Instead of saying: ***the evidence are the observations, how do we get to the model?***, we say ***given the evidence of a model with an unknown parameter, how likely are the observations***? \n",
    "\n",
    "We come up with an equation that involves the parameter, which we want to maximize. This in turn yields the right parameter for the model. So, instead of using moments, we find maxima on a function (which is also what ANNs, do by the way). It is also, I think, how ***your*** brain builds models: What are the right parameters so that my model yields my observations most probably?\n",
    "\n",
    "And so we want to find those points where the derivative of the likelihood function is zero.\n",
    "\n",
    "And you know what? The likelihood function can be ***computed analytically***, in closed form, ***for all popular analytic distribution models***! Just like the moments, for all popular distribution models.\n",
    "\n",
    "So, let's recap: \n",
    "\n",
    "- The product $\\prod_{i=1}^n Pr(y_i \\; | \\; \\theta)$ gives us a measure of how **likely** it is to observe values $y_1,\\ldots,y_n$ given the parameters $\\lambda$. MLE consists in choosing the appropriate function $l= Pr(Y|\\theta)$ to maximize for a given set of observations. This function is called the *likelihood function*, because it is a measure of how likely the observations are if the model is true.\n",
    "\n",
    "In the above model, the data were drawn from a Poisson distribution with parameter $\\lambda =5$, That is,\n",
    "\n",
    "$$L(y|\\lambda=5) = \\frac{e^{-5} 5^y}{y!}$$\n",
    "\n",
    "Instead of looking at the function above as a function of $y$, let us look at it as a *function of* $\\lambda$. Let's calculate the likelihood that the underlying process with $\\lambda = 5$ generates any given value of $y$ (note the sexy factorial evaluation below). But let's start with $y$ = 10.\n",
    "\n",
    "For any given value of $y$, we can calculate its likelihood. We will use a python `lambda` function:\n",
    "\n",
    "Please fill in an implementation of the factorial function below to replace the ellipsis ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_like = lambda x, lam: np.exp(-lam) * (lam**x) / ...\n",
    "\n",
    "lam = 5\n",
    "value = 10\n",
    "poisson_like(value, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability above is for $y$ = 10. What is the probability for all possible $y$'s?\n",
    "\n",
    "First of all, *what **are** all possible $y$'s*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [poisson_like(yi, lam) for yi in {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14}]\n",
    "probs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus the probability of observing all our empirical datapoints one after the other, given the assumed model (with two different ways of evaluating the product of a list):\n",
    "\n",
    "(two ways of computing it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "reduce((lambda x, y: x * y), probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the same result for $\\lambda$ = 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 8\n",
    "probs2 = [poisson_like(yi, lam) for yi in {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 14}]\n",
    "probs2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 8\n",
    "np.prod(probs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, both probabilities are ***pretty low*** because we're evaluating a joint probability of ***a lot*** of events, but the probability for $\\lambda = 8$ is ***lower***! Thus, $\\lambda$ = 5 is a ***better*** value, more ***likely*** to lead our model to reproduce our entire dataset (it gives us a ***higher probability to observe our $y$'s***)!\n",
    "\n",
    "(*duh*, of course it is, the data was drawn from a random Poisson process with $\\lambda = 5$, but *we don't know that -well, we do, but we're pretending we don't* :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the likelihood function for any value of $\\lambda$, for a particular $y$ (let's pick $y$ = 5, arbitrarily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,15)\n",
    "_y = 5\n",
    "plt.plot(lambdas, [poisson_like(_y, l) for l in lambdas])\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('L($\\lambda$|x={0})'.format(_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like $\\lambda$ = 5 is about right for $y$ = 5.\n",
    "\n",
    "Try it out for other values of $y$. Does the optimal $\\lambda$ remain the same? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the likelihood function different than the probability distribution function (pdf)? The likelihood is a function of the parameter(s) *given the data*, whereas the pdf returns the probability of the data given a particular parameter value. \n",
    "\n",
    "Here is the pdf of the Poisson for $\\lambda=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 5\n",
    "xvals = np.arange(15)\n",
    "plt.bar(xvals, [poisson_like(x, lam) for x in xvals])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Pr(X|$\\lambda$=5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, both curves look the same (kinda), but that is a coincidence. *It's not always like this*.\n",
    "\n",
    "So we cannot locate one $\\lambda$ that is optimal ***for all $y$'s***. So we have to ***compromise***. Specifically, we want the value of $\\lambda$ which **maximizes the likelihood function**, because it yields a pdf that is the closest to the histogram of the data. \n",
    "\n",
    "In other words, our observations, which are a subset of all possible data which we can observe and record for a specific physical process, yield a histogram that we assume matches the pdf of all the data. So we model all possible data as a parametrized Poisson pdf. The Poisson pdf matches the histogram of the observed data the best when the likelihood function is maximal. And that happens when the area under the curve is maximal.\n",
    "\n",
    "So, inference is reduced to an **optimization problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The theory is actually very intuitive\n",
    "\n",
    "In the 2019 season, before the Singaporean Grand Prix, Lewis Hamilton accrued 284 points out of 1,207 total points awarded. Having this data, we’d like to make a guess at the probability that Lewis Hamilton wins the Singaporean Grand Prix.\n",
    "\n",
    "The simplest guess here would be 284/1,207 = 24%, which is the best possible guess based on the data. \n",
    "\n",
    "For the sake of argument, let's say Lewis Hamilton won 10 out of 20 races.\n",
    "\n",
    "***This is actually an estimation with the MLE method***!\n",
    "\n",
    "Let's simplify that Lewis Hamilton has a single winning probability (let’s call this θ) throughout all races across the season, regardless of the uniqueness of each race and any complex factors like weather. In other words, we’re assuming each of LH's races as a [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial) with a winning probability θ.\n",
    "\n",
    "With this assumption, we can describe the probability that LH wins k times out of n races for any given number k and n (k≤n). More precisely, we assume that the number of race wins for LH follows a [binomial distribution with parameter θ](https://en.wikipedia.org/wiki/Binomial_distribution).\n",
    "The probability that LH wins k times out of n races, given the winning probability θ, is:\n",
    "\n",
    "$$P(\\text{k wins out of n races} \\; | \\; θ) = (^n_k) \\; θ^k (1 - θ)^{n-k}$$\n",
    "\n",
    "This simplification (describing the probability using just a single parameter θ regardless of real world complexity) is our statistical model, and θ is the parameter to be estimated.\n",
    "\n",
    "Since we have observed data for this F1 season, which is 10 out of 20 wins for LH (let’s call this data as D), we can calculate P(D|θ) — the probability that this data D is observed for given θ. Let’s calculate P(D|θ) for $θ=0.1$ and $θ=0.7$ as examples.\n",
    "\n",
    "$$P(\\text{10 wins out of 20} \\; | \\; θ) = (^{20}_{5}) \\; 0.1^{5} (0.9)^{15}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "P_10_wins_out_of_20_and_theta = m.factorial(20) // m.factorial(10) // m.factorial(10) * 0.1**10 * 0.9**10\n",
    "P_10_wins_out_of_20_and_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a ***very low probability***! So, if LH’s winning probability θ is actually 0.1, this data D (10 wins in 20 races) is ***extremely unlikely to be observed***.\n",
    "\n",
    "Then what if θ = 0.7?\n",
    "\n",
    "$$P(\\text{10 wins out of 20} \\; | \\; θ) = (^{20}_{5}) \\; 0.7^{5} (0.3)^{15}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_10_wins_out_of_20_and_theta = m.factorial(20) // m.factorial(10) // m.factorial(10) * 0.7**10 * 0.3**10\n",
    "P_10_wins_out_of_20_and_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a *** higher probability***! So if LH’s winning probability θ is 0.7, this data D is ***much more likely to be observed*** than when θ = 0.1.\n",
    "\n",
    "Based on this comparison, we would be able to say that θ is more likely to be 0.7 than 0.1 considering the actual observed data D. \n",
    "\n",
    "Here, we’ve been calculating the probability that D is observed for each θ, but at the same time, we can also say that we’ve been checking likelihood of each value of θ based on the observed data. Because of this, P(D|θ) is also considered as Likelihood of θ. \n",
    "\n",
    "The next question here is, what is the exact value of θ which maximizes the likelihood P(D|θ)? This is Maximum Likelihood Estimation!\n",
    "\n",
    "The value of θ maximizing the likelihood can be obtained by having derivative of the likelihood function with respect to θ, and setting it to zero.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/mle-math.png width = 600 />\n",
    "</center>\n",
    "\n",
    "Since likelihood goes to zero when θ= 0 or 1, the value of θ that maximizes the likelihood is k/n.\n",
    "\n",
    "$$ θ = \\frac{k}{n}$$\n",
    "\n",
    "In other words, the estimated value of θ, LH's winning percentage per race, is 10/20 = 50% when estimated with MLE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Nashville rainfall\n",
    "\n",
    "Now let's put MLE to use. Going back to our Nashville rainfall data, where we used a gamma distribution instead of a Poisson distribution, we need to maximize:\n",
    "\n",
    "$$\\prod_{i=1}^n \\beta^{\\alpha} x^{\\alpha-1} e^{-x/\\beta}\\Gamma(\\alpha)^{-1}$$ with respect to $(\\alpha, \\beta)$\n",
    "\n",
    "In order to make the likelihood function more manageable (this is legit since `log` is monotonic with respect to its argument), the optimization is performed ***using a natural log transformation of the likelihood function***. And since the *log of a product is the sum of the logs*, we want to maximize:\n",
    "\n",
    "$$\\begin{align}l(\\alpha,\\beta) &= \\sum_{i=1}^n \\log[\\beta^{\\alpha} x^{\\alpha-1} e^{-x/\\beta}\\Gamma(\\alpha)^{-1}] \\cr \n",
    "&= n[(\\alpha-1)\\overline{\\log(x)} - \\bar{x}\\beta + \\alpha\\log(\\beta) - \\log\\Gamma(\\alpha)]\\end{align}$$\n",
    "\n",
    "(*so much easier to work in the log scale!*)\n",
    "\n",
    "where $n = 2012 − 1871 = 141$ and the bar indicates an average over all *i*. We want to choose $\\alpha$ and $\\beta$ to maximize $l(\\alpha,\\beta)$.\n",
    "\n",
    "Notice $l$ is infinite if any $x$ is zero. We do not have any zeros, but we do have an NA value for one of the October data, which we dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Solving the equation\n",
    "\n",
    "To find the maximum of any function, we typically take the *derivative* with respect to the variable to be maximized, set it to zero and solve for that variable. \n",
    "\n",
    "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\beta} = n\\left(\\frac{\\alpha}{\\beta} - \\bar{x}\\right) = 0$$\n",
    "\n",
    "Which can be solved as $\\beta = \\alpha\\; / \\; \\bar{x}$. However, plugging this into the derivative with respect to $\\alpha$ yields:\n",
    "\n",
    "$$\\frac{\\partial l(\\alpha,\\beta)}{\\partial \\alpha} = \\log(\\alpha) + \\overline{\\log(x)} - \\log(\\bar{x}) - \\frac{\\Gamma(\\alpha)'}{\\Gamma(\\alpha)} = 0$$\n",
    "\n",
    "This has ***no closed form solution***! We must use ***numerical optimization***!\n",
    "\n",
    "Numerical optimization algorithms take an initial \"guess\" at the solution, and iteratively improve the guess until it gets \"close enough\" to the answer.\n",
    "\n",
    "Here, we will use [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method) algorithm:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "</div>\n",
    "\n",
    "Which is available to us via `SciPy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a graphical example of how Newton-Raphson converges on a solution, using an arbitrary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function\n",
    "func = lambda x: 3./(1 + 400*np.exp(-2*x)) - 1\n",
    "xvals = np.linspace(0, 6)\n",
    "plt.plot(xvals, func(xvals))\n",
    "plt.text(5.3, 2.1, '$f(x)$', fontsize=16)\n",
    "\n",
    "# zero line\n",
    "plt.plot([0,6], [0,0], 'k-')\n",
    "\n",
    "# value at step n\n",
    "plt.plot([4,4], [0,func(4)], 'k:')\n",
    "plt.text(4, -.2, '$x_n$', fontsize=16)\n",
    "\n",
    "# tangent line\n",
    "tanline = lambda x: -0.858 + 0.626*x\n",
    "plt.plot(xvals, tanline(xvals), 'r--')\n",
    "\n",
    "# point at step n+1\n",
    "xprime = 0.858/0.626\n",
    "plt.plot([xprime, xprime], [tanline(xprime), func(xprime)], 'k:')\n",
    "plt.text(xprime+.1, -.2, '$x_{n+1}$', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_n$ be our current estimate. Then the next estimate $x_{n+1}$ is obtained as follows: Draw the tangent line at $(x_n,f(x_n))$. Then $x_{n+1}$ is the point where the tangent line meets the x-axis. That tangent line meets the x-axis at a point often much closer to the root of the curve than $x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the Newton-Raphson algorithm, we need a function that returns a vector containing the **first and second derivatives** of the function with respect to the variable of interest. In our case, this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import psi, polygamma\n",
    "\n",
    "dlgamma = lambda m, log_mean, mean_log: np.log(m) - psi(m) - log_mean + mean_log\n",
    "dl2gamma = lambda m, *args: 1./m - polygamma(1, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `log_mean` and `mean_log` are $\\log{\\bar{x}}$ and $\\overline{\\log(x)}$, respectively. `psi` and `polygamma` are complex functions of the Gamma function that result when you take first and second derivatives of that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "log_mean = precip.mean().apply(np.log)\n",
    "mean_log = precip.apply(np.log).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to optimize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha MLE for December\n",
    "alpha_mle = newton(dlgamma, 2, dl2gamma, args=(log_mean[-1], mean_log[-1]))\n",
    "alpha_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now plug this back into the solution for beta:\n",
    "\n",
    "<div style=\"font-size: 120%;\">  \n",
    "$$ \\beta  = \\frac{\\alpha}{\\bar{X}} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_mle = alpha_mle/precip.mean()[-1]\n",
    "beta_mle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the fit of the estimates derived from MLE to those from the method of moments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = precip.Dec\n",
    "dec.hist(normed=True, bins=10, grid=False)\n",
    "x = np.linspace(0, dec.max())\n",
    "plt.plot(x, gamma.pdf(x, alpha_mom[-1], beta_mom[-1]), 'm-')\n",
    "plt.plot(x, gamma.pdf(x, alpha_mle, beta_mle), 'r--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red dotted curve is a *better model* of our data than the continuous mauve curve. So MLE gives us better models than MOM.\n",
    "\n",
    "For common distributions, `SciPy` includes methods for fitting via MLE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "\n",
    "gamma.fit(precip.Dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit is not directly comparable to our estimates, however, because SciPy's `gamma.fit` method fits an odd 3-parameter version of the gamma distribution. But don't let the three parameters  (alpha, loc, beta) provided throw you off! There's `alpha` and `beta` that you know of, and the other one is `loc`, the location at which the gamma distribution ***starts growing***, since you can liberally move the gamma along the x-axis.\n",
    "\n",
    "In general, the gamma function has three parametrizations:\n",
    "\n",
    "- With a shape parameter k and a scale parameter θ.\n",
    "- With a shape parameter α = k and an inverse scale parameter β = 1/θ, called a rate parameter.\n",
    "- With a shape parameter k and a mean parameter μ = k/β.\n",
    "\n",
    "It's possible to shift and/or scale the distribution using the loc and scale parameters. Specifically, gamma.pdf(x, alfa, loc, scale) is identically equivalent to gamma.pdf(y, alfa) / scale with y = (x - loc) / scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Another dataset example\n",
    "\n",
    "Let's try our hands at another dataset that better illustrates the modeling process because it incorporates an imprpvement on the model.\n",
    "\n",
    "We'll use `statsmodels`, a Python package for statistical data analyses, which will help us avoid the math!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/happy-cat.jpg width = 400 />\n",
    "    No math baby!\n",
    "</center>\n",
    "\n",
    "## Lab \\#2: Hearts dataset\n",
    "\n",
    "`statsmodels` just like `R`, also contains real-world datasets that you can use to experiment with new methods. Let's load the **heart dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import statsmodels.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data = statsmodels.datasets.heart.load_pandas().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains censored and uncensored data: a censor of 0 means that the patient was ***alive at the end of the study***, and thus we don't know the exact survival time. We only know that the patient survived at least the indicated number of days. \n",
    "\n",
    "Let's only keep uncensored data (thus introduce a [bias](https://en.wikipedia.org/wiki/Bias) toward patients that ***did not survive very long*** after their transplant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.censors == 1]\n",
    "survival = data.survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data graphically, by plotting the raw survival data and the histogram. Notice how the data is 2D, however the histogran is really 1D: It gives us the possible $y$'s and their ***distribution***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.plot(sorted(survival)[::-1], 'o')\n",
    "ax1.set_xlabel('Patient')\n",
    "ax1.set_ylabel('Survival time (days)')\n",
    "\n",
    "ax2.hist(survival, bins=15)\n",
    "ax2.set_xlabel('Survival time (days)')\n",
    "ax2.set_ylabel('Number of patients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the histogram is decreasing very rapidly! Patients died fast!\n",
    "\n",
    "Eyeballing the data, let's try to fit an [exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution) to the data. Isn't that would *you* would try?\n",
    "\n",
    "According to the exponential model, S (number of days of survival) is an exponential random variable with the parameter λ, and the observations $s_i$ are sampled from this distribution. Let the sample **mean** be:\n",
    "\n",
    "$$\\overline s = \\frac 1 n \\sum s_i$$\n",
    "\n",
    "The likelihood function of an exponential distribution is as follows:\n",
    "\n",
    "$$\\mathcal{L}(\\lambda, \\{s_i\\}) = P(\\{s_i\\} \\mid \\lambda) = \\lambda^n \\exp\\left(-\\lambda n \\overline s\\right)$$\n",
    "\n",
    "Here's the proof:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\lambda, \\{s_i\\}) &= P(\\{s_i\\} \\mid \\lambda) &\\\\\n",
    "&= \\prod_{i=1}^n P(s_i \\mid \\lambda) & \\textrm{(by independence of the $s_i$)}\\\\\n",
    "&= \\prod_{i=1}^n \\lambda \\exp(-\\lambda s_i) &\\\\\n",
    "&= \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^n s_i\\right) &\\\\\n",
    "&= \\lambda^n \\exp\\left(-\\lambda n \\overline s\\right) &\n",
    "\\end{align*}\n",
    "\n",
    "Here, $\\overline s$ is the sample mean.\n",
    "\n",
    "To find the maximum of this function, let's compute its derivative function with respect to $λ$:\n",
    "\n",
    "$$\\frac{d\\mathcal{L}(\\lambda, \\{s_i\\})}{d\\lambda} = \\lambda^{n-1} \\exp\\left(-\\lambda n \\overline s \\right) \\left( n - n \\lambda \\overline s \\right)$$\n",
    "\n",
    "The root of this derivative is therefore $λ=1\\;/\\;\\overline s$. We're lucky here, the exponential is simple to diffferentiate.  In more complex situations, we would require numerical optimization methods, like Newton-Raphson, to maximize the likelihood function.\n",
    "\n",
    "Let's compute this parameter ***numerically***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smean = survival.mean()\n",
    "rate = 1. / smean\n",
    "rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the fitted exponential distribution to the data. We first need to generate linearly spaced values for the x-axis (days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smax = survival.max()\n",
    "days = np.linspace(0., smax, 1000)\n",
    "# bin size: interval between two consecutive values in `days`\n",
    "dt = smax / 999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the probability density function of the exponential distribution with` SciPy`. \n",
    "\n",
    "The parameter is the scale, the inverse of the estimated rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_exp = st.expon.pdf(days, scale=1. / rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram and the obtained distribution. \n",
    "\n",
    "We need to rescale the theoretical distribution to the histogram (depending on the bin size and the total number of data points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 30\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.hist(survival, nbins)\n",
    "ax.plot(days, dist_exp * len(survival) * smax / nbins,\n",
    "        '-r', lw=3)\n",
    "ax.set_xlabel(\"Survival time (days)\")\n",
    "ax.set_ylabel(\"Number of patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmmm... \n",
    "\n",
    "What do you think?\n",
    "\n",
    "The fit between the model and our data is ***ok-ish***, but not ***perfect***.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.8 Avoiding the math, and a better model\n",
    "\n",
    "Just like with our probability function `p`, you can avoid the math when you can write python! \n",
    "\n",
    "You can also avoid the math if you know how to write python and you know where to find the useful libraries!\n",
    "\n",
    "`SciPy` actually integrates numerical maximum likelihood routines for a large number of distributions. Let's leverage `SciPy` to estimate the parameter of the exponential distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = st.expon\n",
    "args = dist.fit(survival)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we plot, let's perform a **goodness of fit test**. A good statistical goodness of fit test is the [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test), also known as **KS** test. it is sensitive to differences in *both* location and shape of the empirical [cumulative distribution functions](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (cdf) of the two samples.\n",
    "\n",
    "The cdf is the area under the pdf: the cdf evaluated at $x$, is the probability that $X$ will take a value less than or equal to $x$. In the case of scalar continuous distributions, it gives the area under the pdf from minus infinity to $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.kstest(survival, dist.cdf, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the p-value is very low: the null hypothesis (stating that the observed data stems from an exponential distribution with a maximum likelihood rate parameter) can be rejected with high confidence!\n",
    "\n",
    "- We'll talk about p-values and null-hypotheses in our ***next lecture***. They're very important point estimators in classical statistical inference. Still very much used in industry (but your professor thinks they suck, since Bayesian inference, which yields pdfs instead of point estimates, is much better).\n",
    "\n",
    "Oh no...! The exponential distribution is thus ***not a good fit for the data***, and the **KS** test told us so!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another distribution, the [Birnbaum-Sanders distribution](https://en.wikipedia.org/wiki/Birnbaum-Saunders_distribution), which is typically used to model **failure times**. You wouldn't know about this model distribution ***without experience in data science***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = st.fatiguelife\n",
    "args = dist.fit(survival)\n",
    "st.kstest(survival, dist.cdf, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the p-value is about 0.073, so that we would ***not reject the null hypothesis*** with a five percent confidence level!\n",
    "\n",
    "Ok, let's plot now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_fl = dist.pdf(days, *args)\n",
    "nbins = 30\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "ax.hist(survival, nbins)\n",
    "ax.plot(days, dist_exp * len(survival) * smax / nbins,\n",
    "        '-r', lw=3, label='exp')\n",
    "ax.plot(days, dist_fl * len(survival) * smax / nbins,\n",
    "        '--g', lw=3, label='BS')\n",
    "ax.set_xlabel(\"Survival time (days)\")\n",
    "ax.set_ylabel(\"Number of patients\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Birnbaum-Sanders (BS) fits the data a lot better than the exp distribution!\n",
    "\n",
    "Once again, \n",
    "\n",
    "- The maximum likelihood estimate (MLE) for the rate parameter(s) is, by definition, the value of the parameters that maximizes the likelihood function. It is the parameter(s) that maximize the probability of observing the data, assuming that the observations are actually sampled from the distribution we picked as a model.\n",
    "\n",
    "We then either verify by plotting the data and the model with the MLE parameter(s) and see if it's a good match, or use goodness of fit tests like the **KS** test to get a more objective estimate. There are [many](https://en.wikipedia.org/wiki/Goodness_of_fit) different goodness of fit tests! Which we pick is part of the art!\n",
    "If we are wrong and the fit is not very good, back to the drawing board for another model!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/frustration.png width = 400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Once you have a correct model..\n",
    "\n",
    "You can keep the model and its parameters, and ***throw away the data***. \n",
    "\n",
    "To generate a new value of the pseudorandom variable, use a series of invocations of the `.rvs()` method for your model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Density Estimates\n",
    "\n",
    "You can also estimate a probability distribution ***nonparametrically*** using [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation.) (KDE). That is actually how `seaborne` plots the function\n",
    "best matching a histogram.\n",
    "\n",
    "In some instances, you may not be interested in the parameters of a particular distribution of data, but just a smoothed representation of the data at hand. In this case, you can estimate the disribution *non-parametrically* (i.e. making no assumptions about the form of the underlying distribution) using [kernel density estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE). \n",
    "\n",
    "KDE is a fundamental data smoothing problem where inferences about the population are made based on a finite data sample.\n",
    "\n",
    "The kernel density estimator of a set of n points ${x_i}$ is given as:\n",
    "\n",
    "$$\\hat{f}_h(x) = \\frac{1}{nh} \\sum_{i=1}^n K\\Big(\\frac{x-x_i}{h}\\Big)$$\n",
    "\n",
    "Here, $h>0$ is a scaling parameter (the bandwidth) and $K(u)$ is the kernel, a symmetric function that integrates to 1. This estimator is to be compared with a classical histogram, where the kernel would be a top-hat function (a rectangle function taking its values in ${0,1}$), but the blocks would be located on a regular grid instead of the data points.\n",
    "\n",
    "Multiple kernels can be chosen. Here, we chose a Gaussian kernel, so that the KDE is the superposition of Gaussian functions centered on all the data points. It is an estimation of the density.\n",
    "\n",
    "The choice of the bandwidth is not trivial; there is a [tradeoff](https://en.wikipedia.org/wiki/Bias-variance_dilemma.) between a too low value (small bias, high variance: overfitting) and a too high value (high bias, small variance: underfitting). \n",
    "\n",
    "There are several methods to automatically choose a sensible bandwidth. SciPy uses a rule of thumb called Scott's Rule: $h = n^{\\frac{-1}{d + 4}}$\n",
    "\n",
    "The following figure illustrates the KDE. The sample dataset contains four points in [0,1] (black lines). The estimated density is a smooth curve, represented here with different bandwidth values.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/kde.png width = 600 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random data\n",
    "y = np.random.random(15) * 10\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=16, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import norm\n",
    "\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "# Smoothing parameter\n",
    "s = 0.4\n",
    "\n",
    "# Calculate the kernels\n",
    "kernels = np.transpose([norm.pdf(x, yi, s) for yi in y])\n",
    "\n",
    "#plt.plot(x, y)\n",
    "plt.plot(x, kernels, 'k:')\n",
    "plt.plot(x, kernels.sum(1))\n",
    "plt.plot(y, np.zeros(len(y)), 'ro', ms=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy implements a Gaussian KDE that automatically chooses an appropriate bandwidth. \n",
    "\n",
    "## 3.2 Bimodal distribution\n",
    "\n",
    "Let's create a bi-modal distribution of data that is not easily summarized by a parametric distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bi-modal distribution with a mixture of Normals.\n",
    "x1 = np.random.normal(0, 3, 50)\n",
    "x2 = np.random.normal(4, 1, 50)\n",
    "\n",
    "# Append by row\n",
    "x = np.r_[x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x, bins=8, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kde\n",
    "\n",
    "density = kde.gaussian_kde(x)\n",
    "xgrid = np.linspace(x.min(), x.max(), 100)\n",
    "plt.hist(x, bins=8, normed=True)\n",
    "plt.plot(xgrid, density(xgrid), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spatial distribution\n",
    "\n",
    "We look at the geographical locations of tropical cyclones from 1848 to 2013, based on data provided by the NOAA, the US' National Oceanic and Atmospheric Administration.\n",
    "\n",
    "We use a kernel density estimation (KDE) to estimate that pdf.\n",
    "\n",
    "First, install a [map server](https://pypi.org/project/geos/):\n",
    "```(python)\n",
    "pip install geos\n",
    "```\n",
    "\n",
    "If you want a full geometry engine (*optional*), look [here](https://trac.osgeo.org/geos).\n",
    "\n",
    "Then, import the [cartopy](http://scitools.org.uk/cartopy/) library:\n",
    "```(python)\n",
    "conda install cartopy\n",
    "```\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/cartopy-install.png width = 900 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import cartopy.crs as ccrs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Open the data with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data\n",
    "df = pd.read_csv('data/Allstorms.ibtracs_wmo.v03r05.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains information about most storms since 1848. A single storm may appear multiple times across several consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[[0, 1, 3, 8, 9]]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas' `groupby()` function to obtain the average location of every storm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.groupby('Serial_Num')\n",
    "pos = dfs[['Latitude', 'Longitude']].mean()\n",
    "x = pos.Longitude.values\n",
    "y = pos.Latitude.values\n",
    "pos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the storms on a map with cartopy. \n",
    "\n",
    "This toolkit allows us to easily project the geographical coordinates on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a simple equirectangular projection,\n",
    "# also called Plate Carree.\n",
    "geo = ccrs.Geodetic()\n",
    "crs = ccrs.PlateCarree()\n",
    "# We create the map plot.\n",
    "ax = plt.axes(projection=crs)\n",
    "# We display the world map picture.\n",
    "ax.stock_img()\n",
    "# We display the storm locations.\n",
    "ax.scatter(x, y, color='r', s=.5, alpha=.25, transform=geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before performing the kernel density estimation, we transform the storms' positions from the **geodetic coordinate system** (longitude and latitude) into the map's coordinate system, called **plate carrée**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = crs.transform_points(geo, x, y)[:, :2].T\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform the kernel density estimation on our (2, N) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = st.gaussian_kde(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gaussian_kde()` routine returned a Python function. To see the results on a map, we need to evaluate this function on a 2D grid spanning the entire map. We create this grid with meshgrid(), and we pass the x and y values to the kde() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "# Coordinates of the four corners of the map.\n",
    "x0, x1, y0, y1 = ax.get_extent()\n",
    "# We create the grid.\n",
    "tx, ty = np.meshgrid(np.linspace(x0, x1, 2 * k),\n",
    "                     np.linspace(y0, y1, k))\n",
    "# We reshape the grid for the kde() function.\n",
    "mesh = np.vstack((tx.ravel(), ty.ravel()))\n",
    "# We evaluate the kde() function on the grid.\n",
    "v = kde(mesh).reshape((k, 2 * k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before displaying the KDE heatmap on the map, we need to use a special colormap with a transparent channel. This will allow us to superimpose the heatmap on the stock image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [https://stackoverflow.com/a/37334212/1595060](https://stackoverflow.com/a/37334212/1595060)\n",
    "cmap = plt.get_cmap('Reds')\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "my_cmap[:, -1] = np.linspace(0, 1, cmap.N)\n",
    "my_cmap = ListedColormap(my_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we display the estimated density with imshow():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=crs)\n",
    "ax.stock_img()\n",
    "ax.imshow(v, origin='lower',\n",
    "          extent=[x0, x1, y0, y1],\n",
    "          interpolation='bilinear',\n",
    "          cmap=my_cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of MLE and classical statistical estimation\n",
    "\n",
    "What do you ***need to know***?\n",
    "\n",
    "- MOM equates the empirical and theoretical moments to yield the parameters of your model. \n",
    "\n",
    "- MLE gives you the value which maximises the Likelihood P(D|θ). \n",
    "\n",
    "- [Maximum a Posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) (MAP), which we skipped over, yields the value which maximises the posterior probability P(θ\\;|\\;D). As both methods give you a single fixed value, they’re considered to be **point estimators**.\n",
    "\n",
    "- How to numerically evaluate the parameters of your model using the first two methods. You *do not need to know the math* (but if you understand it, that is a good thing!)\n",
    "\n",
    "**Bayesian inference**, as we will see next week, ***fully calculates the posterior probability distribution***, as Bayes' formula below. \n",
    "\n",
    "$$p(θ \\; | \\; D) = \\frac{p(D \\; | \\; θ) \\; p(θ)}{p(D)}$$\n",
    "\n",
    "Hence the output is not a single value, i.e. a **point estimate** for the parameters of your model, but a **probability density function** (when θ is a continuous variable) or a ***probability mass function*** (when θ is a discrete variable) ***for the parameters of your model***. That way, you know what the most likely value is, ***but also the amount of error you might be making***!\n",
    "\n",
    "- MLE and MAP return a single fixed value(s) for the model parameter(s), Bayesian inference returns functions (pdfs)  instead!\n",
    "\n",
    "Assume you’re in a casino with full of slot machines with 50% winning probability. After playing for a while, you hear a rumour that there’s ***one special slot machine*** with 67% winning probability!\n",
    "\n",
    "Now, you’re observing people playing 2 suspicious slot machines (you’re sure that one of those is the special slot machine!) and get the following data.\n",
    "\n",
    "Machine A: 3 wins out of 4 plays\n",
    "Machine B: 81 wins out of 121 plays\n",
    "\n",
    "By intuition, you would think machine B is the special one! Because 3 wins out of 4 plays on machine A could just happen by chance. But machine B’s data doesn’t look like it's happening by chance!\n",
    "\n",
    "The posterior probability distribution P(θ|D), calculated as a Gamma function, is plotted below for the two machines:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/tricked-slot-machines.png width = 400 />\n",
    "</center>\n",
    "\n",
    "Although both distributions have their mode on θ=0.666 (MLE estimate), the shapes of the distributions are quite different. Density around the mode is much higher in the distribution of machine B than the one of machine A.\n",
    "\n",
    "So a pdf yields much more information than a point estimate. In particular, it tells us about errors in the estimation. And that is very important when a model is making a potentially dangerous prediction (e.g. ***drive 120 mph on this road***).\n",
    "\n",
    "***Ummm... computer, what's an estimation of the error you might be making in your prediction?***\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/crazy-robot.jpg width = 200 />\n",
    "</center>\n",
    "\n",
    "So we will now turn our attention to Bayesian estimation. Bayes' formula is actually pretty complex. Specifically, the term in the denominator, a **marginal probability**,  needs to be calculated for every possible θ:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src =ipynb.images/bayes-complex.png width = 350 />\n",
    "</center>\n",
    "\n",
    "That integral sum (for the continuous case, a simple sun for discrete cases) is the reason why we had to wait for powerful laptops before we could actually put Bayesian estimation to practice. \n",
    "\n",
    "When the model is ***analytic***, like the ones in this notebook, solutions are forthcoming, albeit with a bit of math. But that is rarely the case in real-world applications. We then need to use ***Monte Carlo*** and other probabilistic programming methodologies as substitute for direct integral computation. That is what we'll look at next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "To be announced.\n",
    "\n",
    "***You may work in teams of 2***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
