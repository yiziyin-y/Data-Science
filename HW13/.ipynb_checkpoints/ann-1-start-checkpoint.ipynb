{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 13 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 6 April 2020, with material by James Loy, Simeon Kostandinov, and Avinash Sharma</div>\n",
    "\n",
    "\n",
    "# A 2-layer Artificial Neural Network\n",
    "\n",
    "<img src=\"https://c4.staticflickr.com/3/2656/4253587827_9723c3ffd3_z.jpg\" width=400/>\n",
    "\n",
    "*Photo courtesy of GE Healthcare, http://www.flickr.com/photos/gehealthcare/4253587827/ *\n",
    "\n",
    "<img src=\"https://askabiologist.asu.edu/sites/default/files/resources/articles/neuron_anatomy.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Most introductory texts to Neural Networks brings up brain analogies when describing them. Without delving into brain analogies, we can simply describe Neural Networks as a mathematical function that maps a given input to a desired output, kust like our winnow.\n",
    "\n",
    "This is what we worked on last, our good little caterpillar brain, or *winnow*:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"images/winnow-ann.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "Remember how simple the learning algorithm was? We start with a random weights matrix $W$, we evaluate $\\hat{y} = W$@$x$ where x is a row of observations, and we compare $\\hat{y}$ to the label $y$. If $\\hat{y}$ is too large compared to $y$ we halve the weights that multiply non-zero components of $x$. If $\\hat{y}$ is too small compared to $y$, we double the weights that multiply the non-zero components of $x$.\n",
    "\n",
    "In general, Neural Networks are a little bit *deeper* than one layer and consist of the following components:\n",
    "- An input layer, x\n",
    "- ***An arbitrary amount*** of hidden layers, the more the deeper the network\n",
    "- An output layer, ŷ\n",
    "- A set of weights and biases between each layer, W and b\n",
    "- And also, ***new new new***: A choice of activation function for each hidden layer, σ. In this notebook, we’ll use a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation function.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"images/2-layer-ann.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "What is an **activation function**? We don't just look at the *linear output* of the combined output of all neurons from the upstream layer, but a function $\\sigma()$ of that, which usually **discounts** small inputs, and **enhances** larger inputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, are we ready for a bit of *linear algebra*?\n",
    "\n",
    "Here is our constructor class: We randomize the weights of our two layers in the constructor. You can think of the weights as either stored on the nodes, or the edges of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input is $x$, the output ŷ of a simple 2-layer Neural Network like the one above is:\n",
    "\n",
    "$$\\hat{y_1} = \\sigma(W_1x + b_1)$$\n",
    "\n",
    "$$\\hat{y} = \\sigma\\;(W_2\\hat{y_1} + b_2)$$\n",
    "\n",
    "Like in our Winnow lab, $W$ is a matrix, $x$ and $\\hat{y}$ are vectors, and $b_i$ is a vector that is called a **bias**: Even if the input $x = 0$, then the output $\\hat{y}$ is the bias. Our output is never 0. You may think of the bias as *long-term memory*.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\hat{y} = \\sigma\\;(W_2\\sigma(W_1x + b_1) + b_2)$$\n",
    "\n",
    "You might notice that in the equation above, the weights $W$ and the biases $b$ are the only variables that affect the output $\\hat{y}$.\n",
    "\n",
    "## The activation function and why it cannot be linear\n",
    "\n",
    "So what does an activation function $\\sigma$ do? Why does a neuron, artifical or not, need it? The input is a *weighted sum* of the output of the previous neuronal layers, plus a bias and then decides whether that input is enough to *excite* the neuron to fire.\n",
    "\n",
    ">**QUESTION**: How did we decide to \"fire\" the neurons in our Caterpillar brain?\n",
    "\n",
    "This is a step function:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/step-function.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "The associated activation function A = is **activated** if Y > threshold, else not. This is a **binary** activation.\n",
    "\n",
    "There are certain drawbacks with this. We are wasting away a lot of potential since we have a reservoir of neurotransmitters, and we are essentially saying they're either completley on, or completely off. Like black or white with no colors in between. So there's waste.\n",
    "\n",
    "Can we do better? How about a linear function like $y = cx$?. This is a straight line function where activation is proportional to input. This way, it gives a range of activations. \n",
    "\n",
    "If you;ve read a bit about neural networks, you know gradient descent is how we're going to train. Then you notice that for this function, the derivative is a constant. The derivative with respect to $x$ is $c$. That means, the gradient has no relationship with $x$. It is a constant gradient and the descent is going to be on constant gradient. If there is an error in prediction, the changes made by back propagation are constant and do not depend on the change in input delta$(x)$.\n",
    "\n",
    "Hmm. Not good.\n",
    "\n",
    "There is another problem too. Think about connected layers. Each layer is activated by a linear function. That activation in turn goes into the next level as input and the second layer calculates weighted sum on that input and it in turn, fires based on another linear activation function.\n",
    "\n",
    "No matter how many layers we have, if all are linear in nature, the final activation function of last layer is nothing but just a linear function of the input of first layer! Pause for a bit and think about it.\n",
    "\n",
    "That means these two layers (or N layers) can be replaced by a single layer and we just lost the ability of stacking layers this way. No matter how we stack, the whole network is still equivalent to a single layer with linear activation (a combination of linear functions in a linear manner is still another linear function), just like our *winnow*. Not that we don't like our little caterpillar brain, but we came to class today to see if we can transform into something smarter, like a *butterfly*!\n",
    "\n",
    "So let's go looking for non-linear.\n",
    "\n",
    ">**Student**: Professor, we only studied *linear* algebra, are you saying we're going to have to study non-linear algebra, too? My math is not that good!\n",
    "\n",
    ">**Professor**: No worries, little caterpillar-soon-to-be-butterfly, this will be the only non-linear part in the whole story, you'll be able to handle it. But you *need to freshen up on how to [compute derivatives of functions](https://www.mathsisfun.com/calculus/derivatives-introduction.html)*.\n",
    "\n",
    "### The sigmoid\n",
    "Here is a non-linear function that looks a bit like $y = cx$. It's called the **sigmoid function**. It turns out to be a good approximation of the *real* activation function in animal neurons:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/sigmoid.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "What are its advantages? Combinations of this function are also nonlinear! Great. Now we can stack layers..\n",
    "\n",
    "Also, for values of $x$ between -2 and 2, $y$ values are very steep. Which means, any small changes in the value of $x$ in that region will cause the value of $y$ to change significantly. That means this function has a tendency to bring $y$ values to either end of the curve. Looks like it’s good for a classifier to me, aking clear distinctions on prediction!\n",
    "\n",
    "Another advantage of the sigmoid is, unlike linear functions, the output is always going to be in range $(0,1)$ compared to $(-\\infty, \\infty)$ of linear functions! And you know how much this professor loves his probabilities and absolutely adores the range $(0,1)$, right? \n",
    "\n",
    ">**NOTE**: There is a little problem, though. Towards either end of the sigmoid function, the $y$ values tend to respond very less to changes in $x$. What does that mean? The gradient in those regions is going to be small. This will give rise to a problem called [vanishing gradients](https://en.wikipedia.org/wiki/Vanishing_gradient_problem): When the activations reach the *near-horizontal* part of the curve, the gradient is small or has vanished and the network refuses to learn further or becomes drastically slow in its learning.\n",
    "\n",
    "We'll stick with the sigmoid for now, but let's briefly mention two other options:\n",
    "\n",
    "### The hyperbolic tangent\n",
    "\n",
    "The hyperbolic tangent is just a rescaled version of the simgoid:\n",
    "\n",
    "$$tanh(x) = 2\\sigma(2x) - 1$$\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/hyperbolic-tangent.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "### The Rectified Linear Unit (ReLU)\n",
    "\n",
    "The ReLU is almost linear, but not really:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/relu.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "The ReLU has some advantages, namey the sparsity of its activations. \n",
    "\n",
    "Imagine a big neural network with a lot of neurons. Using a sigmoid or tanh will cause almost all neurons to fire in an analog way. That means almost all activations will be processed to describe the output of a network. In other words the activation is dense. This costs energy, and mammals like to conserve energy (a nice way of saying *lazy*). Ideally, we want only a few neurons in the network to activate, thereby making the activations sparse and efficient.\n",
    "\n",
    "ReLu give us this benefit. Imagine a network with random initialized weights: Almost 50% of the network yields 0 activation because of the characteristic of ReLu of having output 0 for negative values of $x$! This means a fewer neurons are firing (sparse activation) and the network is light. Woah, nice! ReLu seems to be awesome! \n",
    "\n",
    ">**Leaky RELU**: The ReLU is not invincible, either. Because of the horizontal line for negative $x$, the gradient can go to 0. For activations in that region the gradient will be 0, so weights will not get adjusted during learning. That means, those neurons will stop responding to variations in error/input. This is called dying ReLu problem. This problem can cause several neurons to just die and not respond, making a substantial part of the network passive. That is why there are variations of ReLu to mitigate this issue: They simply turn the horizontal line into a non-horizontal line, for example $y = 0.01x$ for $x<0$ (slightly inclined). This is called *leaky ReLu*. The main idea is to let the gradient be non zero and recover during training.\n",
    "\n",
    "Moreover, ReLu is less computationally expensive than tanh and sigmoid because it involves *simpler* mathematical operations. Math takes energy, even for smart brains!\n",
    "\n",
    "\n",
    "### Back to the sigmoid\n",
    "Let's just stick with the sigmoid, for now! \n",
    "\n",
    ">**NOTE**L To be precise, **sigmoid** is a general class of curves that *are S-shaped*. That's the actual definition. We often use the term sigmoid to refer to the **logistic function**, but that's actually just a single example of a sigmoid. The hyperbolic function also defines a sigmoid curve! So we should really say: Here is the *logistic*. But unfortunately, the sigmoid has stuck to the logistic in Machine Learning. But be careful in math that is not correct!\n",
    "\n",
    "So here's the \"*sigmoid*\" (a.k.a. *logistic*) function in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is its derivative:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/sigmoid-derivative.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "We derive a formula for the derivative of the sigmoid:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\\n",
    "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\\n",
    "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\\n",
    "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}$$\n",
    "\n",
    "And we write the equivalent python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**NOTE**: Why do we need the derivative? Because we're going to improve our Caterpillar-brain learning so that now we increase or decrease the weights based on the *slope* of the input! Remember how Varsha asked \"professor, why don't we just *add* something rather than multiplying?\". Yes, we could add something, but what? The answer is somethign proportional to the slop of the input!\n",
    "\n",
    "## Learning (Training)\n",
    "Naturally, the values for the weights and biases determines the *strength* of the predictions. The process of fine-tuning the weights and biases from the input data is known as *learning*, or *training the neural network*, just like we trained our Caterpillar brain using the Winnow algorithm.\n",
    "\n",
    "Each iteration of the training process consists of the following steps:\n",
    "\n",
    "- Calculating the predicted output $\\hat{y}$, known as **feedforward**, in order to see how it compares to the *label*\n",
    "\n",
    "- Updating the weights and biases, known as **backpropagation** in order to bring the output *closer* to the *label*\n",
    "\n",
    "The sequential graph below illustrates the process:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/ff-bp.png\" width=1000 />\n",
    "</center>\n",
    "\n",
    "We can also represent our graph like this. This is called a 3-4-1 neural net architecture:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/3-4-1-ann.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "We use both subscripts and superscripts as indexes. The subscripts are the components of the vector, and the superscripts are the layer components. Superscripts are *not* powers!\n",
    "\n",
    "So $x$ is the input vector, no activation function $\\sigma$ is applied, and the activation vector $a^1 = x$. As we dot the vector $x$ with the Weights matrix $W^1$ just as we did with the Winnow ($x$@$W$), this produces the vector $z$, when mapped into the activation function $\\sigma$, it produces the activation vector $a^2$. Then, $a^2$ gets dotted with the Weights matrix $W^2$ ($a^2$@$W^2$), then passed into the last activation function to produce the scalar $a^3 = \\hat{y}$, which we compare with label $y$.\n",
    "\n",
    "Note we neglected the contribution of the bias $b$ in both layers!\n",
    "\n",
    "When we neglect $b$:\n",
    "\n",
    "$$z = W^1 \\;x$$\n",
    "\n",
    "and\n",
    "\n",
    "$$y = W^2\\;a^2 = W^2\\;\\sigma(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "As we’ve seen in the sequential graph above, feedforward is just simple calculus and for a basic 2-layer neural network, the output of the Neural Network is:\n",
    "\n",
    "$$\\hat{y} = \\sigma\\;(W_2\\sigma(W_1x + b_1) + b_2)$$\n",
    "\n",
    "So let’s add a feedforward function in our python code to do exactly that. \n",
    "\n",
    "For simplicity, let's assume we have no long-term memory and the biases $b$ are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Witness how simple and clean our formulas look, now that we've studied **linear algebra** we represent `self.input` as a *vector* and *self.weights* as a *vector*, so that a complex formula like so:\n",
    "\n",
    "$$\\sum_{i=\\text{features}} \\text{self.input}_i * \\text{self.weights1}_i$$\n",
    "\n",
    "is just simply:\n",
    "```(python)\n",
    "np.dot(self.input, self.weights1)\n",
    "```\n",
    "\n",
    "## Loss function\n",
    "\n",
    "We need a way to evaluate the ***goodness*** of our predictions (i.e. how far off are our predictions). With the Winnow, we just evaluated when `prediction != label`. The Loss Function allows us to do a more refined evaluation.\n",
    "\n",
    "There are many available loss functions, and the nature of our problem should dictate our choice of loss function. Here, we’ll use a simple sum-of-squares error as our loss function: The sum-of-squares error is simply the sum of the difference between each predicted value $\\hat{y}$ and the actual value $y$. The difference is squared so that we measure the *absolute value* of the difference:\n",
    "\n",
    "$$\\text{sum-of-squares-error} = \\sum_{j=\\text{observations}} (y_j - \\hat{y_j})^2$$\n",
    "\n",
    "Our goal in training is to find the best set of weights and biases that minimizes the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Now that we have a way to measured the error of our prediction (loss), we need to find a way to propagate the error back into layer 2, then layer 1, and update weights and biases in the process.\n",
    "\n",
    "In order to know the appropriate amount to adjust the weights and biases by, we will use some math: We'll compute the **derivative of the loss function with respect to the weights and biases**.\n",
    "\n",
    "Recall that the derivative of a function is simply the slope of the function.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/gradient-descent.png\" width=800 />\n",
    "</center>\n",
    "\n",
    "If we have the derivative, we can simply update the weights and biases by increasing/reducing with it. Remember our Winnow, where we divided by 2 or multiplied by 2? Now we're doind something more refined: If the slope is *big*, we'll increase or decrease by *more* than 2, and if the slope is *small*, by *less* than 2. In other words, we will increase or decrease weights not by 2, but based on the derivative of the output. This is known as [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "The derivative of a function measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). We already talked about this in the context of the sigmoid and the vanishing gradient problem. Here, if the function is the Loss function and the argument is the weights, the gradient shows ***how much the weights needs to change by (in positive or negative direction) to minimize the Loss function***.\n",
    "\n",
    "> Backpropagation aims to minimize the loss function by adjusting the network’s weights and biases. The level of adjustment is determined by the derivative (or **gradient**) of the loss function with respect to those parameters.\n",
    "\n",
    "So we will update our weights (and biases) thusly:\n",
    "\n",
    "$$w \\rightarrow w + \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "$$b \\rightarrow b + \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "But there's a problem! We can’t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function *does not contain the weights and biases*! \n",
    "\n",
    "Therefore, we need the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to help us calculate it. We call $z$ the intermediate output ($ x \\rightarrow z \\rightarrow \\hat{y}$) between the two layers. Now let's simplify our equations to get an idea of how the chain rule works:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/chain-rule.png\" width=800 />\n",
    "</center>\n",
    "\n",
    "Wow, that looks simple and that's what some textbooks will show you. but unfortunately, it's wrong! Here is one mistake, and it's not the only one: \n",
    "\n",
    "$$\\hat{y} = \\sigma(W^2 z)$$ \n",
    "\n",
    "So: \n",
    "\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial z} = W^2 * \\sigma'(W^2 z)$$\n",
    "\n",
    "We need to be *very careful* when doing math!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have *two* sets of weights, we need *two* derivations of $\\text{Loss}(y, \\hat{y})$:\n",
    "\n",
    ">**CLASS WORK**: Please derive the second equation! The first one only needed to go up to an equation that involves $W^1$. What do you think the second one needs to go up to? What other variables need to be involved? Please write down the markdown and share it with me on blackboard!\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^2} = \n",
    "\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial \\hat{y}} * \\frac{\\partial\\hat{y}}{\\partial W^2} =\n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\frac{\\partial\\hat{y}}{\\partial W^2}$$\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^1} = \n",
    "\\cdots$$\n",
    "\n",
    "$$z = \\sigma(W^1 x)$$\n",
    "\n",
    "$$\\hat{y} = \\sigma(W^2 z)$$\n",
    "\n",
    "Note that we carried out the chain rule a bit further in order to get to the first layer of weights with the second equation above.\n",
    "\n",
    "Plugging in the last two equations in the first two:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^2} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\frac{\\partial \\;\\sigma(W^2 z)}{\\partial W^2}$$\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^1} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\frac{\\partial\\;\\sigma(W^2 z)}{\\partial z} * \\frac{\\partial \\;\\sigma(W^1 x)}{\\partial W^1}$$\n",
    "\n",
    "Do you remember the function composition rule from math class: If $f(x) = h(g(x))$, then $f'(x) = h'(g(x)) * g'(x)$.\n",
    "\n",
    "So with the change of variable $W^2 \\rightarrow q$:\n",
    "\n",
    "$$\\frac{\\partial \\;\\sigma(W^2 z)}{\\partial W^2} = \\frac{\\partial \\;\\sigma(zq)}{\\partial q} = \n",
    "\\sigma'(zq) * \\frac{\\partial \\;(zq)}{\\partial q} = \\sigma'(zq) * z$$\n",
    "\n",
    "Similarly,\n",
    "\n",
    "$$\\frac{\\partial \\;\\sigma(W^1 x)}{\\partial W^1} = \\frac{\\partial \\;\\sigma(xq)}{\\partial q} = \n",
    "\\sigma'(xq) * \\frac{\\partial \\;(xq)}{\\partial q} = \\sigma'(xq) * x$$\n",
    "\n",
    "And also:\n",
    "\n",
    "$$\\frac{\\partial \\;\\sigma(W^2 z)}{\\partial z} = W^2 * \\sigma'(W^2 z)$$\n",
    "\n",
    "So the first equation becomes:\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^2} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\sigma'(W^2 z) * z$$\n",
    "\n",
    "And the second equation:\n",
    "\n",
    "$$\\frac{\\partial \\;\\text{Loss}(y, \\hat{y})}{\\partial W^1} = \n",
    "\\sum_i \\; 2(y - \\hat{y}) * \\sigma'(W^2 z) * W^2 * \\sigma'(W^1 x) * x$$\n",
    "\n",
    "Now that we have that, let’s add the backpropagation function into our python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        \n",
    "    def reload(self, x):\n",
    "        self.input = x\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.output\n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        #d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        #d_weights1 = np.dot(self.input.T,  \n",
    "        #                    np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * \n",
    "        #                    sigmoid_derivative(self.layer1))\n",
    "        \n",
    "        sigmoid_derivative_1 = sigmoid_derivative(np.dot(self.input, self.weights1))\n",
    "        sigmoid_derivative_2 = sigmoid_derivative(np.dot(self.layer1, self.weights2))\n",
    "        d_weights2 = np.dot(self.layer1.T, \n",
    "                            (2*(self.y - self.output) * sigmoid_derivative_2))\n",
    "        d_weights1 = np.dot(self.input.T,  \n",
    "                            np.dot(2*(self.y - self.output) * sigmoid_derivative_2, self.weights2.T) * \n",
    "                            sigmoid_derivative_1)\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now that we have our complete python code for doing feedforward and backpropagation, let’s apply our Neural Network on an example and see how well it does. Instead of a Caterpillar, let's assume we are R2D2, and we *only* have 3 feet:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/r2d2.gif\" width=200 />\n",
    "</center>\n",
    "\n",
    "This is the feet-sensory input for R2D2, and the forward/backward decision vector:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/ann-1-iniput.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "Our Neural Network should learn the ideal set of weights to represent this function. Note that it isn’t exactly trivial for us to work out the weights just by inspection alone.\n",
    "\n",
    "Let’s train the Neural Network for 1500 iterations and see what happens. Looking at the loss per iteration graph below, we can clearly see the loss monotonically decreasing towards a minimum. This is consistent with the gradient descent algorithm that we’ve discussed earlier.\n",
    "\n",
    "## Training R2D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "nn = NeuralNetwork(X,y)\n",
    "\n",
    "for i in range(1500):\n",
    "    nn.feedforward()\n",
    "    nn.backprop()\n",
    "\n",
    "print(nn.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take notice of the shape of our inputs. We'll need that for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "If we plot the Loss at each iteration, we find:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/ann-1-loss.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "Let’s look at the final prediction (output) from the Neural Network after 1500 iterations.\n",
    "Predictions after 1500 training iterations\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/ann-1-predictions.png\" width=200 />\n",
    "</center>\n",
    "\n",
    "Wow, not bad! Our feedforward and backpropagation algorithm trained the Neural Network successfully and the predictions converged on the true values. We taught R2D2 how to walk, yay!\n",
    "\n",
    "Note that we iterated 1500 times over our data though! Compare that to the Winnow, where we iterated how many times over our data?\n",
    "\n",
    "Note that the slight difference between the predictions and the actual values is *desirable*, as it prevents overfitting and allows our Neural Network to generalize better to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setosa, Versicolor, Virginica\n",
    "\n",
    "I feel so much smarter with that extra brain layer, dont you? I think we've transformed from little [caterpillars](https://i.redd.it/vsvvs1tlat021.gif) to beautiful [butterflies](https://steemitimages.com/0x0/https://2.bp.blogspot.com/-I40rsv5sDIs/V_jmR63qOuI/AAAAAAAAGZk/TTZsMLWQLE8TGTbMZEJSIA10KlYoo7utgCLcB/s1600/butterfly3.gif)!\n",
    "\n",
    "<center>\n",
    "<img src=\"ipynb.images/butterfly.gif\" width=400 />\n",
    "</center>\n",
    "\n",
    "Now that we're a butterfly, we're after flowers! So let's train our butterfly brain with the famous [Iris](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/) dataset. You may download it from UCI and load it into a pandas data frame, but my version of `iris.data` on blackboard includes legend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/iris.data\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on petal length and petal width.Sepals (collectively called the calyx) are modified leaves that encase the developing flower. Tough to see.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/Petal-sepal.jpg\" width=300 />\n",
    "</center>\n",
    "\n",
    "How about we prepare to add some *more* neurons in the hidden layer of the butterfly brain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2:\n",
    "    def __init__(self, x, y, n):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],n) \n",
    "        self.weights2   = np.random.rand(n,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        \n",
    "    def reload(self, x):\n",
    "        self.input = x\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.output\n",
    "    \n",
    "    def weights1(self):\n",
    "        return self.weights1\n",
    "    \n",
    "    def weights2(self):\n",
    "        return self.weights2\n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        #d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        #d_weights1 = np.dot(self.input.T,  \n",
    "        #                    np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * \n",
    "        #                    sigmoid_derivative(self.layer1))\n",
    "        \n",
    "        sigmoid_derivative_1 = sigmoid_derivative(np.dot(self.input, self.weights1))\n",
    "        sigmoid_derivative_2 = sigmoid_derivative(np.dot(self.layer1, self.weights2))\n",
    "        d_weights2 = np.dot(self.layer1.T, \n",
    "                            (2*(self.y - self.output) * sigmoid_derivative_2))\n",
    "        d_weights1 = np.dot(self.input.T,  \n",
    "                            np.dot(2*(self.y - self.output) * sigmoid_derivative_2, self.weights2.T) * \n",
    "                            sigmoid_derivative_1)\n",
    "\n",
    "        # The mod below fizes some strange uloop errors!\n",
    "        \n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        #print(self.weights1.shape, d_weights1.shape)\n",
    "        self.weights1 = self.weights1 + d_weights1\n",
    "        #print(self.weights2.shape, d_weights2.shape)\n",
    "        self.weights2 = self.weights2 + d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we added a parameter in our brain constructor to set the number of neurons in the hidden layer.\n",
    "\n",
    "Also note that I added some mods to our matrix algebra in `backprop()`, as `NumPy` trips a bit over itself with the matrix extension of the `+=` operator.\n",
    "\n",
    "Ok, let's peek at our data for a bit of EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = np.reshape(y, (y.size, 1))\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a *pair plot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that based on petal length and width, the Setosas and Versicolor/Virginica Irises are *easily separable*. This is why I picked these two kinds. Had I picked the Versicolor vs Virginica/Setosa Irises, learning to differentiate them would be tougher. One extra layer of brain *may not be enough*.\n",
    "\n",
    ">**CLASS WORK**: Please train/test split the data in the cell below into `X_train, X_test, y_train, y_test `:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = np.reshape(y_train, (y_train.size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mod will save you from a lot of headaches. Since we are dealing with vector and matrix quantities, sometimes we need to help along `NumPy`.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    try:\n",
    "        x = np.vectorize(round)(x)\n",
    "        return 1.0/(1+ np.exp(-x))\n",
    "    except:\n",
    "        print(x)\n",
    "        print(np.exp(-x))\n",
    "        return 1.0/(1+ np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**CLASS WORK**: Please rescale (normalize) `X_train` in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double[] vector\n",
    "def ShowVector(vector, decimals, valsPerRow, newLine):\n",
    "    frmt = '%.' + str(decimals) + 'f'\n",
    "    for i in range(len(vector)):\n",
    "        print(frmt % vector[i] + \" \", end='')\n",
    "    if (newLine): print(\"\")\n",
    "\n",
    "# int[][] matrix\n",
    "def ShowMatrix(matrix, decimals, numRows, indices):\n",
    "    frmt = '%.' + str(decimals) + 'f'\n",
    "    for i in range(numRows):\n",
    "        if (indices):\n",
    "            print(\"[\" + '%02d' % i + \"]   \", end='')\n",
    "        for j in range(len(matrix[i])):\n",
    "            print(frmt % matrix[i][j] + \" \", end='')\n",
    "        print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of training data are:\")\n",
    "ShowMatrix(X_train, 2, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of testing data are:\")\n",
    "ShowMatrix(X_test, 2, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how balanced our training data is.\n",
    "\n",
    ">**CLASS WORK**: Please write down a single like of code below that will reveal how \"*balanced*\" the data is. Why is that important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've come a *long way*, little caterpillar!\n",
    "<center>\n",
    "<img src=\"ipynb.images/butterfly2.gif\" width=400 />\n",
    "</center>\n",
    "\n",
    ">**CLASS WORK**\" Train our little butterfly brain to distinguish an Irish Setosa from other Irises. ***You*** decide how many neurons are required in the hiden layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork2(X_train2, y_train2, ?)\n",
    "\n",
    "for i in range(1500):\n",
    "    nn.feedforward()\n",
    "    nn.backprop()\n",
    "\n",
    "print(np.vectorize(round)(nn.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results impressive? Let's wait until we test on our test set, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = scaler.transform(X_test)\n",
    "print(X_test2)\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.reload(X_test2)\n",
    "nn.feedforward()\n",
    "prediction = nn.predict()\n",
    "print(np.vectorize(round)(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns double, int[][] trainData\n",
    "def Accuracy(result, label):\n",
    "    numCorrect = 0\n",
    "    numWrong = 0\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        if np.abs(result[i] - label[i]) < 0.01:\n",
    "            numCorrect += 1\n",
    "        else:\n",
    "            numWrong += 1\n",
    "\n",
    "    return (numCorrect * 1.0) / (numCorrect + numWrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAcc = Accuracy(np.vectorize(round)(prediction), y_test)\n",
    "print(\"Prediction accuracy on test data = \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job, little butterfly!\n",
    "<center>\n",
    "<img src=\"ipynb.images/cartoon-butterfly.png\" width=300 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning leveraging all 4 features\n",
    "\n",
    "We reload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data/iris.data\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove one of the 3 flower classes. That will leave us with two flower classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[' flower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**CLASS WORK**: Filter out `Iris-versicolor` rows using `pandas` (keeping only `Iris-setosa` and `Iris-virginica`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = ...\n",
    "data2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASS WORK**: Transform string labels to integer labels: `Iris-setosa` $\\rightarrow$ 0 and `Iris-virginica` $\\rightarrow$ 1, using a new column `[flower]` (note that the old column is `[ flower]`: extra leading space character!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['flower'] = ...\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**CLASS WORK**: Now drop label ` flower` and `flower` from the dataset (create a new dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = ...\n",
    "data3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test datasets using `sklearn`'s API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data3.values, data2['flower'].values, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of training data are:\")\n",
    "ShowMatrix(X_train, 2, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few rows of testing data are:\")\n",
    "ShowMatrix(X_test, 2, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how balanced our training data is.\n",
    "\n",
    ">Why is that important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabels_balance = sum(y_train)\n",
    "trainlabels_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else do you notice that we might want to address, with the data before we being learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**CLASS WORK**: Now reshape the data according to how our neural network expects it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(y_train, ...)\n",
    "y2 = np.reshape(y_test, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**CLASS WORK**: Now train our butterfly brain to distinguish an Irish Setosa from an Iris Virginica, using *all four* features sepal length, sepal width, petal length, and petal width, according to the neural network parameters *you* picked. Please report training accuracy and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAcc = Accuracy(nn.output, y)\n",
    "\n",
    "nn.reload(X_test2)\n",
    "nn.feedforward()\n",
    "prediction = nn.predict()\n",
    "testAcc = Accuracy(prediction, y2)\n",
    "\n",
    "print(\"Prediction accuracy on training data = \" + str(trainAcc))\n",
    "print(\"Prediction accuracy on test data = \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~Homework~~\n",
    "- Can you help the butterfly correctly differentiate Iris Setosa from Iris Virginica *and* Iris Versicolor?\n",
    "- What other activation function can you use besides the Sigmoid function?\n",
    "- Can you use a *learning rate* when training the Neural Network, to help lazy butterflies learn better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
