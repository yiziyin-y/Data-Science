{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 9 Day 2</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 12 March 2020, with material by Thomas Wiecki</div>\n",
    "\n",
    "\n",
    "# Poisson model\n",
    "\n",
    "Let's *warm up* with a simple Poisson model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np\n",
    "figsize(12.5, 4)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose this is the number of emails you receive in your inbox every day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rate = 10\n",
    "n_obs = 10000\n",
    "poisson_number_of_emails = np.random.poisson(true_rate, n_obs)\n",
    "plt.plot(poisson_number_of_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(poisson_number_of_emails, 30, density=True, histtype='stepfilled', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit a Poisson Model to get the expectation (how many emails to *expect* every day), and the error. From the histogram above, I could model my `lambda_prior` with a normal distribution with a mean close to 10. But let's assume that I am completely ignorant, and thnk that I'm equally likely to get *any* value from 0 to 20.\n",
    "\n",
    "Please build a model and run a sim with `PyMC3` here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "\n",
    "# fit a pymc3 model to get the rate\n",
    "with pm.Model() as email_model:\n",
    "    lambda_prior = pm.???('lambda_prior', 0, 20)\n",
    "\n",
    "    #likelihood\n",
    "    Y_obs = pm.Poisson('Y_obs', mu = lambda_prior, observed = poisson_number_of_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with email_model:\n",
    "    trace = pm.sample(2000, chains = 2, tune = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[\"lambda_prior\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we expect to get 9.99 emails a day. Pretty close!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mixture model\n",
    "\n",
    " Varsha requested a Mixture Model in a *simpler* way. The simplest ML library is *always* `scikit-learn`: Just one call to a `.fit()` function. So let's do a Gaussian Mixture Model with `scikit-learn`.\n",
    " \n",
    ">**NOTE**: It may be simple, but can you actually understand *how* the modeling is done? Not really, right? To understand what is going on under the hood, you need to turn to our good old (complex) friend `PyMC3`..\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://media.giphy.com/media/l2Sq8EYhA66vdOfAI/giphy.gif\" width=300 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np\n",
    "figsize(12.5, 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "\n",
    "# %load std_ipython_import.txt\n",
    "import pandas as pd\n",
    "import scipy as spy\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *three* different 可爱极了 models that we concatenate and then reshape into a 2D array, since that is what `scikit-learn`'s `.fit()` API expects.\n",
    "\n",
    ">**NOTE**: Use X.reshape(-1, 1) if your data has a single feature/column and X.reshape(1, -1) if it contains a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([random_state.normal(-1, 1.5, 350),\n",
    "                    random_state.normal(0, 1, 500),\n",
    "                    random_state.normal(3, 0.5, 150)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([random_state.normal(-1, 1.5, 350),\n",
    "                    random_state.normal(0, 1, 500),\n",
    "                    random_state.normal(3, 0.5, 150)]).reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([random_state.normal(-1, 1.5, 350),\n",
    "                    random_state.normal(0, 1, 500),\n",
    "                    random_state.normal(3, 0.5, 150)]).reshape(-1, 1)\n",
    "plt.plot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the data looks like, let's peek at its histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm... I see ***2*** peaks (*I don't know how the data was generated!*), so I will assume 2 gaussians generated my data. So I will build a **Gaussian Mixture** with 2 gaussians. \n",
    "\n",
    "`scikit-learn` does this for us automagically: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixm = GaussianMixture(n_components=2, random_state=1, verbose=1)\n",
    "mixm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot! \n",
    "\n",
    "It turns our that the `score_samples` API that allows us to produce *fake data* from the model yields actually the `log`arithm of the data (remember how the log is the math function that does the best **plastic surgery** to our data so that it looks 可爱极了？Many models do this, so when we generate fake data, we have to take the exponential of the data in order to get realistic-looking data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 1000)\n",
    "logprob = mixm.score_samples(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf)\n",
    "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... does not look great, does it?\n",
    "\n",
    "What are the two clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsibilities = mixm.predict_proba(x.reshape(-1, 1))\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "plt.plot(x, pdf_individual)\n",
    "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... Looking at this, it looks like there has to be an *extra* cluster to the right because the two peaks seem ot be offset from the real peaks! So, as good data scientists, we wiil try another model that contains ***3*** gaussians!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixm = GaussianMixture(n_components=3, random_state=1, verbose=1)\n",
    "mixm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 1000)\n",
    "logprob = mixm.score_samples(x.reshape(-1, 1))\n",
    "pdf = np.exp(logprob)\n",
    "plt.plot(x, pdf)\n",
    "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh.... So much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsibilities = mixm.predict_proba(x.reshape(-1, 1))\n",
    "pdf_individual = responsibilities * pdf[:, np.newaxis]\n",
    "plt.plot(x, pdf_individual)\n",
    "plt.hist(X, 30, density=True, histtype='stepfilled', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! That's it! What a beautiful model!\n",
    "\n",
    "But... what if I have ***no idea*** about the number of components? What if there are... 10!?\n",
    "\n",
    "Well, let's try them all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models with 1-10 components\n",
    "N = np.arange(1, 11)\n",
    "models = [None for i in range(len(N))]\n",
    "\n",
    "for i in range(len(N)):\n",
    "    models[i] = GaussianMixture(N[i]).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute some information criteria to help us pick the best possible model.\n",
    "\n",
    "The two most popular information criteria for statistical models are [Akaike's Information criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) (**AIC**), discovered by the statistician [Hirotugu Akaike](https://en.wikipedia.org/wiki/Hirotugu_Akaike), and the [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (**BIC**), discovered by Gideon E. Schwarz, both of which are included in `scikit-learn`!\n",
    "\n",
    "When a statistical model is used to represent a process that generated the data, some information will be lost by using the model to represent the data. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.\n",
    "\n",
    "Gideon uses some linear algebra in his reasoning, so I won't give you more details. Linear Algebra is next on our topics to discover! If you want to know more about it, it's [here](https://www.andrew.cmu.edu/user/kk3n/simplicity/schwarzbic.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the AIC and the BIC\n",
    "AIC = [m.aic(X) for m in models]\n",
    "BIC = [m.bic(X) for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(N, AIC, '-k', label='AIC')\n",
    "plt.plot(N, BIC, '--k', label='BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see immediately that the best possible model is for ***3*** gaussian clusters.\n",
    "\n",
    "What are the areas of responsibility of each cluster, as we did in our PyMC3 notebook (if a point is in a specific range, which cluster does it most probably belong to)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = responsibilities\n",
    "p = p[:, (1, 0, 2)]  # rearrange order so the plot looks better\n",
    "p = p.cumsum(1).T\n",
    "\n",
    "plt.fill_between(x, 0, p[0], color='gray', alpha=0.3)\n",
    "plt.fill_between(x, p[0], p[1], color='gray', alpha=0.5)\n",
    "plt.fill_between(x, p[1], 1, color='gray', alpha=0.7)\n",
    "plt.xlim(-6, 6)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$p({\\rm class}|x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that, dear class, is the process of data science! \n",
    "\n",
    ">**Data Science**: Go back in time to find the process that generated the data. Just like the source of a river, it's where it's the smallest and easiest to \"*remember*\". That is also where you'll find all of its gold!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/JyskkePkMKt7a/source.gif\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese restaurant process\n",
    "\n",
    "In probability theory, the [Chinese restaurant process](https://en.wikipedia.org/wiki/Chinese_restaurant_process) is a discrete-time stochastic process, analogous to seating customers at tables in a Chinese restaurant. \n",
    "\n",
    "Imagine a Chinese restaurant with an infinite number of circular tables, each with infinite capacity. \n",
    "\n",
    "Customer 1 sits at the first table. The next customer either sits at the same table as customer 1, or the next table. \n",
    "\n",
    "This continues, with each customer choosing to either sit at an occupied table with a probability proportional to the number of customers already there, or an unoccupied table. \n",
    "\n",
    "> Chinese people are more likely to sit at a table with many customers than a few! Is that true? I do not know for sure, but it makes sense! Western people are *selfish*: They want tables *for themselves*. 中国人 are less selfish and they like to *share* tables.\n",
    "\n",
    "At time n, the n customers have been partitioned among m ≤ n tables (or blocks of the partition). The results of this process are exchangeable, meaning the order in which the customers sit does not affect the probability of the final distribution. This property greatly simplifies a number of problems in population genetics, linguistic analysis, and image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet processes\n",
    "\n",
    "Dirichlet processes are used when modelling data that tends to repeat previous values. There are several equivalent views of the Dirichlet process. The Dirichlet process can be defined implicitly through the Chinese restaurant process. \n",
    "\n",
    "Because customers sit at a table with a probability proportional to the number of customers already sitting at the table, the Dirichlet process exhibits a **self-reinforcing** property: The more often a given value has been sampled in the past, the more likely it is to be sampled again.\n",
    "\n",
    "Let's demonstrate a Dirichlet process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "np.array([1.]*K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dirichlet process yields probabilities, so in \\[0, 1\\]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # numbers from the set {0, 1, 2}, where the values are sampled from a Dirichlet distribution with a=np.array([1.]*K)\n",
    "    category = pm.Dirichlet('category', a=np.array([1.]*K))\n",
    "    trace = pm.sample(100, step=pm.Metropolis())\n",
    "    #trace = pm.sample(100, step=pm.BinaryMetropolis())\n",
    "    \n",
    "print(trace['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(trace['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the repretitions?\n",
    "\n",
    "Now let's use a Dirichlet process to generate Categorical variables exhibiting chinese restaurant concentrations. Since the Dirichlet process is seeded with a 3D array of concentrations, we have 3 categories: {1, 2, 3}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "with pm.Model() as model:\n",
    "    # numbers from the set {0, 1, 2}, where the values are sampled from a Dirichlet distribution with a=np.array([1.]*K)\n",
    "    category = pm.Categorical(name='category', p=pm.Dirichlet('p', a=np.array([1.]*K)))\n",
    "    trace = pm.sample(100, step=pm.Metropolis())\n",
    "    #trace = pm.sample(100, step=pm.BinaryMetropolis())\n",
    "    \n",
    "print(trace['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the concentrations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join([str(x) for x in trace['category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty balanced, tough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(trace['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For kicks, from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "with pm.Model() as model:\n",
    "    # numbers from the set {0, 1, 2}, where the values are sampled from a Dirichlet distribution with a=np.array([1.]*K)\n",
    "    category = pm.Categorical(name='category', p=pm.Dirichlet('p', a=np.array([1.]*K)))\n",
    "    trace = pm.sample(100, step=pm.Metropolis(), chains=1)\n",
    "    #trace = pm.sample(100, step=pm.BinaryMetropolis())\n",
    "    \n",
    "','.join([str(x) for x in trace['category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with a Bernoulli distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # numbers from the set {0, 1, 2}, where the values are sampled from a Dirichlet distribution with a=np.array([1.]*K)\n",
    "    category = pm.Categorical(name='category', p=np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]))\n",
    "    trace = pm.sample(100, step=pm.Metropolis(), chains=1)\n",
    "    #trace = pm.sample(100, step=pm.BinaryMetropolis())\n",
    "    \n",
    "','.join([str(x) for x in trace['category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What't the moral of this story? Dirichlet distributions are actually often closer to what we observe in nature, where there is more repetition, and they also are a *much better* starting (prior belief) distribution for Bayesian estimation!\n",
    "\n",
    "So let's use them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model with PyMC3\n",
    "\n",
    "Now let's try a Gaussian mixture model with `PyMC3`!\n",
    "\n",
    "We simulate some data sourced from three *very simple* 可爱极了 gaussian clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm, theano.tensor as tt\n",
    "\n",
    "# simulate data from a known mixture distribution\n",
    "np.random.seed(12345) # set random seed for reproducibility\n",
    "\n",
    "k = 3\n",
    "ndata = 500\n",
    "spread = 5\n",
    "centers = np.array([-spread, 0, spread])\n",
    "\n",
    "# simulate data from mixture distribution\n",
    "v = np.random.randint(0, k, ndata)\n",
    "data = centers[v] + np.random.randn(ndata)\n",
    "\n",
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my data looks **trimodal**!\n",
    "\n",
    "Let's build a model! In the model below, I've commented out some code that *you* will comment back in and run the sim again, ***at home*** (not now). It fixes a little problem. \n",
    "\n",
    "But in any case, notice how we *start* with a Dirichlet distribution. It actually makes more sense that neighboring point belong to the same cluster, rather than that points are randonly assigned to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm, theano.tensor as tt\n",
    "\n",
    "with pm.Model() as trimodel:\n",
    "    # cluster sizes\n",
    "    p = pm.Dirichlet('p', a=np.array([1., 1., 1.]), shape=k)\n",
    "    \n",
    "    # ensure all clusters have some points\n",
    "    #p_min_potential = pm.Potential('p_min_potential', tt.switch(tt.min(p) < .1, -np.inf, 0))\n",
    "\n",
    "    # cluster centers\n",
    "    means = pm.Normal('means', mu=[0, 0, 0], sigma=15, shape=k)\n",
    "    \n",
    "    # break symmetry\n",
    "    #order_means_potential = pm.Potential('order_means_potential',\n",
    "    #                                     tt.switch(means[1]-means[0] < 0, -np.inf, 0)\n",
    "    #                                     + tt.switch(means[2]-means[1] < 0, -np.inf, 0))\n",
    "\n",
    "    # measurement error\n",
    "    sd = pm.Uniform('sd', lower=0, upper=20)\n",
    "\n",
    "    # latent cluster of each observation\n",
    "    category = pm.Categorical('category', p = p, shape = ndata)\n",
    "\n",
    "    # likelihood for each observed value\n",
    "    points = pm.Normal('obs', mu = means[category], sigma = sd, observed = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run this sim, open your laptop's Performance monitor (`Task Manager | Performance` on Windows), and see how fast your CPU will run. You will be *racing it* like a F1 car! When was the last time you did that on your laptop?\n",
    "\n",
    ">**Benchmark**: 5 minutes  on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with trimodel:\n",
    "    step1 = pm.Metropolis(vars = [p, sd, means])\n",
    "    step2 = pm.ElemwiseCategorical(vars = [category], values = [0, 1, 2])\n",
    "    tr = pm.sample(10000, step=[step1, step2], tune=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(tr, var_names=['p', 'sd', 'means']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(tr, var_names=['means']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After thinning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plots.traceplot(tr[::5], var_names=['p', 'sd', 'means']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we converge?\n",
    "\n",
    ">**Note**: Convergence does not mean that your model is the ***best***. It means that your model ***worked***, but there could be a better one.. You need more sims to figure out the best model, leveraging some kind of information criteria like AIC or BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocorrelation plots for serious confirmation of MCMC convergence\n",
    "pm.autocorrplot(tr[::5], var_names=['sd']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[\"p\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = tr[\"p\"][:,0]\n",
    "p2 = tr[\"p\"][:,1]\n",
    "p3 = tr[\"p\"][:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[\"means\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an *interesting* mysterious phenomenon happening: Every 5,000 timesteps, the model switches the 3 clusters :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr[\"means\"][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr[\"means\"][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr[\"means\"][:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr[\"means\"][:,0])\n",
    "plt.plot(tr[\"means\"][:,1])\n",
    "plt.plot(tr[\"means\"][:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sooo funny! The sim *warned me of non-convergence*. It's not that it's not converging, it's that it's oscillating clusters unpredictably. Other people have noticed that, [too](https://discourse.pymc.io/t/mixture-models-and-breaking-class-symmetry/208).  What I need to do is enforce a constraint so that means[0] < means[1] < means[2] or something similar. So I'm going to consider the last 10,000 timesteps, where there is no oscillation :-)\n",
    "\n",
    ">**NOTE**: The PyMC3 authors fixed this problem with the calls in my model that I have commented out above. For homework, rerun the sim with the commented out code commented back in, and see the difference: No more oscillations between mean assignments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr[\"means\"][30000:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_trace_1 = tr[\"means\"][:,0]\n",
    "center_trace_2 = tr[\"means\"][:,1]\n",
    "center_trace_3 = tr[\"means\"][:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_trace = tr[\"sd\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_trace_1 = tr[\"means\"][30000:,0]\n",
    "center_trace_2 = tr[\"means\"][30000:,1]\n",
    "center_trace_3 = tr[\"means\"][30000:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_trace = tr[\"sd\"][30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_trace.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty colors\n",
    "colors = [\"#348ABD\", \"#A60628\", \"#760628\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "norm = stats.norm\n",
    "figsize(15, 5)\n",
    "x = np.linspace(-8, 8, 500)\n",
    "\n",
    "posterior_center_means_1 = center_trace_1.mean(axis=0)\n",
    "posterior_center_means_2 = center_trace_2.mean(axis=0)\n",
    "posterior_center_means_3 = center_trace_3.mean(axis=0)\n",
    "posterior_std_means = std_trace.mean(axis=0)\n",
    "posterior_p1_mean = p1.mean()\n",
    "posterior_p2_mean = p2.mean()\n",
    "posterior_p3_mean = p3.mean()\n",
    "\n",
    "plt.hist(data, bins=30, histtype=\"step\", normed=True, color=\"k\", lw=2, label=\"data histogram\")\n",
    "\n",
    "y = posterior_p1_mean * norm.pdf(x, loc=posterior_center_means_1, scale=posterior_std_means)\n",
    "plt.plot(x, y, label=\"Cluster 0 (using posterior-mean parameters)\", lw=3)\n",
    "plt.fill_between(x, y, color=colors[2], alpha=0.3)\n",
    "\n",
    "y = posterior_p2_mean * norm.pdf(x, loc=posterior_center_means_2, scale=posterior_std_means)\n",
    "plt.plot(x, y, label=\"Cluster 1 (using posterior-mean parameters)\", lw=3)\n",
    "plt.fill_between(x, y, color=colors[1], alpha=0.3)\n",
    "\n",
    "y = posterior_p3_mean * norm.pdf(x, loc=posterior_center_means_3, scale=posterior_std_means)\n",
    "plt.plot(x, y, label=\"Cluster 2 (using posterior-mean parameters)\", lw=3)\n",
    "plt.fill_between(x, y, color=colors[0], alpha=0.3)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Visualizing Clusters using posterior-mean parameters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster prediction\n",
    "\n",
    "What cluster does the datapoint `4` most likely belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_posterior(i=0):\n",
    "    print('true cluster:', v[i])\n",
    "    print('  data value:', np.round(data[i],2))\n",
    "    plt.hist(tr['category'][::5, i], bins=[-.5, .5, 1.5, 2.5,], rwidth=.9)\n",
    "    plt.axis(xmin=-.5, xmax=2.5)\n",
    "    plt.xticks([0,1,2])\n",
    "    \n",
    "cluster_posterior(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"../styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
