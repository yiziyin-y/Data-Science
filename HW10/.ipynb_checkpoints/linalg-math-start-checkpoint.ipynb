{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 10 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 18 March 2020</div>\n",
    "\n",
    "# Introduction to *Linear Algebra* with NumPy and SciPy\n",
    "\n",
    "We're done with the **statistics** part of our class (does this mean no more stats? *Noooo*, this means from  now on you need to know *all* about stats and be ready to apply that knowledgeat any time), and now we're starting off on the **linear algebra** part.\n",
    "\n",
    "The great thing with computers is: you don't need to be that good at math. All you need to be good at is translating logic to predicates and then using libraries like `PyMC3` for probabilistic programs, and for linear algebra, `SciPy`. Buy you need to know *some* math, and today we'll cover that *some*.\n",
    "\n",
    "`NumPy` is for performance. `SciPy` contains all classic (non-probabilistic, which implies machine-learning) scientific algorithms.\n",
    "\n",
    "There's so much to look at in linear algebra. I picked 10 important topics, important stuff you need to make sure you understand, and then we apply it to do **ecology** by leveraging linear algebra to study **graphs**.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/ecology.jpg\" width=600 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math\n",
    "\n",
    "Linear algebra was founded around the same time as Calculus (think [Leibniz](https://en.wikipedia.org/wiki/Leibniz), circa 1700) solely for the purpose of solving general systems of linear equations. The coefficients of a system were written in a\n",
    "grid form, with rows corresponding to equations and columns to the unknown variables. \n",
    "\n",
    "> **NOTE**: Rows are *observations* and columns are *features*.\n",
    "\n",
    "Using a computational tool called the [determinant](https://en.wikipedia.org/wiki/Determinant), researchers were able to solve these systems, opening a world of information about the positions of celestial bodies and large-scale measurements (of geodesic arcs) on the surface of the earth.\n",
    "\n",
    "By the 1850’s, [Arthur Cayley](https://en.wikipedia.org/wiki/Arthur_Cayley) was representing matrices as abstract objects. He *defined* matrix multiplication and nurtured matrix theory as its own field, recognizing a vast wealth of theoretical\n",
    "knowledge underlying the theory of determinants. \n",
    "\n",
    "Around turn of the century, a formal system of vector algebra was invented which relied heavily on interpreting matrices as so-called [linear transformations](https://en.wikipedia.org/wiki/Linear_transformation). Linear algebra became a tool for *computation*.\n",
    "\n",
    "Linear transformations are intuitively those maps of everyday space (Euclidian space) which preserve \"*linear*\" things.\n",
    "Specifically, they send lines to lines, planes to planes, etc., and they preserve the origin. No squares, square roots, exponentials, logarithms, or crazay functions like `Beta` and `Gamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">1</div>\n",
    "\n",
    "## Vector spaces\n",
    "\n",
    "The setting for all of linear algebra is in some vector space. Intuitively this is just a collection of objects,\n",
    "which we call **vectors**, with some rules on how you can combine vectors to get other vectors.\n",
    "\n",
    ">**Definition**: A vector space is a quadruple $(V, F, +, .)$, where $V$ is a set of vectors (points in our space), $F$ is\n",
    "a scalar field (coefficients), the operation $+$: $V$ x $V$: --> $V$ is a commutative, associative **lambda** to combine vectors, and $.$ is an operation to \"*scale*\" vectors. \n",
    "\n",
    "In addition, we need the following properties to hold:\n",
    "- Addition and multiplication distribute (as we are used to with traditional algebra).\n",
    "- There must be an additive identity, which we call $0$, giving $0 + v = v$ for all vectors $v$.\n",
    "- Every vector must have an additive inverse (every vector $v$ has some vector $w$ such that $v + w = 0$).\n",
    "\n",
    "Linear algebra is the study of *linear transformations* on **vectors**, which represent points in a finite dimensional space. The matrix-vector product $y = A x$ is a linear combination of the columns of $A$.  The familiar definition,\n",
    "\n",
    "$$ y_i = \\sum_j A_{ij} x_j $$\n",
    "\n",
    "which says:\n",
    "\n",
    "$$ y_1 = A_{1,1} x_1 + \\cdots + A_{1,n} x_n \\\\\n",
    "   y_2 = A_{2,1} x_1 + \\cdots + A_{2,n} x_n \\\\\n",
    "   \\vdots \\\\\n",
    "   y_n = A_{n,1} x_1 + \\cdots + A_{n,n} x_n $$\n",
    "\n",
    "can also be viewed as\n",
    "\n",
    "$$ y = \\Bigg[ A_{:,0} \\Bigg| A_{:,1} \\Bigg| \\dotsm \\Bigg] \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\end{bmatrix}\n",
    "= \\Bigg[ A_{:,0} \\Bigg] x_0 + \\Bigg[ A_{:,1} \\Bigg] x_1 + \\dotsb . $$\n",
    "\n",
    "The notation $A_{i,j}$ corresponds to the python syntax `A[i,j]` and the colon `:` means the entire range (row or column).  So $A_{:,j}$ is the $j$th column, and $A_{i,:}$ is the $i$th row.  The corresponding python syntax is `A[:,j]` and `A[i,:]`.\n",
    "\n",
    "We may speak of the **span** of a set of vectors as the set of all possible linear combinations of those vectors (like all possible combinations of 3 blue, 2 white, and 1 red ball from an urn). Furthermore, we call a set of vectors **linearly independent** if no vector in the list is in the span of the others. For example, (1,0,0), (0,1,0), and (0,0,1) and are linearly independent in **R**$^3$. Specifically, (1,0,0) cannot be written as a(0,1,0) + b(0,0,1) = (0,a,0) + (0,0,b) for any scalars a and b, and the other two vectors similarly so.\n",
    "\n",
    "We may describe **subspaces** of a vector space, which are just subsets of which are themselves\n",
    "vector spaces with the inherited operations. The simplest examples of these are lines, planes, and\n",
    "hyperplanes through the origin in *R*$^3$. These subspaces are exactly like derives objects in Java, which inherit from \"*wider*\" objects.\n",
    "\n",
    ">**Definition**: A list of vectors is a **basis** for if its elements are **linearly independent**, and\n",
    "their span is the entire vector space (there is no hiddren region in that space that these vectors cannot reach, there is no place on the mountain that you cannot get to). The dimension of a vector space is the length of any basis.\n",
    "Further, *R*$^n$ admits a very natural basis, often called the standard basis:\n",
    "\n",
    "$$(1, 0, 0, \\cdots ,0)\\\\\n",
    "(1, 1, 0, \\cdots ,0)\\\\\n",
    "\\cdots\\\\\n",
    "(0, 0, 0 \\cdots ,1)$$\n",
    "\n",
    "### Linear Transformations and their Matrix Representations\n",
    "We may speak of linear transformations (linear maps, or linear **lambdas**) between two vector spaces:\n",
    "\n",
    ">**Definition**: A function $f$: V --> W is a **linear map** if it **preserves** the operations of **addition** and **scalar multiplication**. In other words, for all v, w in V and c in F:\n",
    "\n",
    "$$f(v+w) = f(v) + f(w)\\\\\n",
    "f(cw) = cf(w)$$\n",
    "\n",
    "One remarkable fact that helps us characterize linear maps is that every **linear map** is determined\n",
    "**completely** by what it does to a **basis**. Since every vector is a linear combination of basis elements,\n",
    "say $x = a_1v_1 + \\cdots a_nv_n$, we see that a linear map plays nicely:\n",
    "\n",
    "$$f(x) = f(a_1v_1 + \\cdots a_nv_n) = a_1f(v_1) + \\cdots a_nf(v_n)$$\n",
    "\n",
    "In other words, if we know what $f$ does to a basis, then we know everything about $f$! In order to aid our\n",
    "computations, we write what does to each basis vector in a tabular form:\n",
    "\n",
    "$$ \\Bigg[\\; f(v_1) \\;\\Bigg| \\;f(v_2) \\;\\Bigg|\\; \\dotsm f(v_n)\\; \\Bigg] $$\n",
    "\n",
    "The $j$th column corresponds to $f(v_j)$, and the $i$th row corresponds to the $i$th coefficient in the expansion\n",
    "of in $f(v_j)$ terms of the basis for W. Here the vertical bars indicate that each element is a column of\n",
    "scalars.\n",
    "\n",
    "The representation above is called the **matrix representation** of $f$.\n",
    "\n",
    "Consider the map (or **lambda**) $f$ on **R**$^3$ defined as $(x,y,z)$ --> $(y, x, 2z + y)$. Write down the lambda for this map here below and find the transformation of the vector (3, 1, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ...\n",
    "f(3,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write down the transformation through $f$ of the standard basis $(e_1, e_2, e_3)$ in **R**$^3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(1,0,0))\n",
    "print(f(0,1,0))\n",
    "print(f(0,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $f(e_1) = ??$, $f(e_2) = ??$, and $f(e_3) = ??$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write down the matrix representation of $f$ with respect to the standard basis as a `NumPy` array here below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that if we take a (column) vector $X$ in V, and multiply it on the left by our matrix above, the resulting\n",
    "vector is precisely the coordinate representation of $f(x)$ with respect to the basis for W. \n",
    "\n",
    ">**NOTE**: The rules for matrix multiplication were constructed very particularly so that this would be the case. In this way, we may arbitrarily switch between viewing $f$ as a transformation and a vector computation. Compositions of linear maps translate to multiplication of two matrices, and matrix inversion (if it exists) is precisely *function inversion*.\n",
    "\n",
    "\n",
    "Here's an elementary example of **function inversion**, related to the map $(x,y)$ --> $(3x - 2y, 5x + y)$:\n",
    "```python\n",
    "v = 3*x - 2*y\n",
    "w = 5*x + y\n",
    "```\n",
    "Given $v = 3$ and $w = 1$, what are $x$ and $y$?\n",
    "\n",
    "You have to solve 2 linear equations of 2 unknowns. We already did this with MOM. Solve the first equation for x (as a function of y). Then plug x into the second equation. Get the solution for y. Then plug in the value of y in the first equation. Get x. Simple!\n",
    "\n",
    "But there is a more mechanistic way of doing this, which works for 2 equations of 2 unknowns or 100 equations of 100 unknowns. And computers can do it really well, too!\n",
    "\n",
    "$$ \\begin{bmatrix} v \\\\ w \\end{bmatrix} = \\begin{bmatrix} 3 \\; 2 \\\\ 5 \\; 1 \\end{bmatrix} * \\begin{bmatrix} x \\\\ y \\end{bmatrix}$$\n",
    "\n",
    "Multiplying both sides by the inverse of the 2x2 matrix:\n",
    "\n",
    "$$ \\begin{bmatrix} 3 \\; 2 \\\\ 5 \\; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} v \\\\ w \\end{bmatrix} = \\begin{bmatrix} 3 \\; 2 \\\\ 5 \\; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} 3 \\; 2 \\\\ 5 \\; 1 \\end{bmatrix} * \\begin{bmatrix} x \\\\ y \\end{bmatrix}$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$ \\begin{bmatrix} 3 \\; 2 \\\\ 5 \\; 1 \\end{bmatrix}^{-1} \\begin{bmatrix} v \\\\ w \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$$\n",
    "\n",
    "Using numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "A = np.array([[3,2],[5,1]]) \n",
    "B = np.linalg.inv(A) \n",
    "print (A)\n",
    "print (B) \n",
    "print (np.dot(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so, for  𝑣=3  and  𝑤=1 , 𝑥  and  𝑦 are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw = np.array([3,1])\n",
    "xy = np.dot(B,vw)\n",
    "print (xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.dot(A,xy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there are many different bases we could have chosen. Even though we are going from **R**$^3$ to **R**$^3$, the column basis could be different from the row basis. Fortunately for our purposes, we are\n",
    "not going to consider what basis is appropriate to choose. All that matters is that fixing a basis, the\n",
    "matrix representation of a linear map is unique, and so we may interchange the notation freely. Even so,\n",
    "the truly interesting things about matrices are those properties which are true *no matter which basis we\n",
    "prefer to use*.\n",
    "\n",
    "### What do I need to remember, professor?\n",
    "A Matrix is a **spreadsheet** that describes **linear transformations**, and Matrix multiplication is ***how your transform (linearly)***. Think about transformations as **rigid movements in state space**. Finding the **inverse** of a transformation represents *going back in time* to find our starting point, and we all *know how statistics is aall bout building a time machine*, so we know how important matrix inversions are, and now you know how to do them!\n",
    "\n",
    ">**NOTE**: Remember, finding the solution to a computaional problem (like Bayesian estimation) is usually locating a global maximum (or minimum) in your state space that maximizes (or minimizes) a **cost function** (Metropolis' cost function is the probability of observing the data with the model at hand). The cost function is your ***mountain***, and you're climbing the mountain to try to get to its **peak**. At the peak, the derivative is zero, and the function is at its maximum. That is your solution.\n",
    "\n",
    "There are tons of different kinds of vector spaces. Many of these are surprising and counter-intuitive. For our purposes, however, we may stick with the nice, small vector spaces. So here is a simple vector space that we will use in class: Climbing mountains!\n",
    "\n",
    "\n",
    "### The mountain vector space\n",
    "\n",
    "How do you **move** from one point on the mountainside to another point on the mountainside? You get your location (it's a **vector**), you multiply by a **matrix** (which is a vector transformation), and you get to another location (a different vector). Each step is a matrix multiplication. Since each step can be different, each matrix can be different.\n",
    "\n",
    "In Metropolis, we moved along a **single axis**, so that was simple: add a delta. But if you move in 2D, 3D, or nD space, you move by taking your current coordinates in that space, mutliplying by a matrix, and getting to new coordinates.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/elCapitan.gif\" width=400 />\n",
    "</center>\n",
    "\n",
    "So, today we're going to learn how to move on mountains with python :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">2</div>\n",
    "\n",
    "## Multiplication and basic operations of Matrices\n",
    "\n",
    "Basic linear algebra is all about learning how spreadsheets *multiply*:\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/matrix-multiply-constant.gif\" width=400 />\n",
    "Multiplying a matrix by a constant\n",
    "</center>\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/matrix-multiply-a.svg\" width=400 />\n",
    "Multiplying a matrix with another, **first row, first column**\n",
    "</center>\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/matrix-multiply-b.svg\" width=400 />\n",
    "Multiplying a matrix with another, **first row, second column**\n",
    "</center>\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/matrix-multiply-c.svg\" width=400 />\n",
    "Multiplying a matrix with another, **final result**\n",
    "</center>\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/matrix-identity.gif\" width=200 />\n",
    "A Matrix that when multiplied by another, does not change it\n",
    "</center>\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/matrix-multiply-order.gif\" width=400 />\n",
    "Order matters\n",
    "</center>\n",
    "\n",
    "`numpy.zeros_like()` returns an array of zeros with the same shape and type as a given array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "pyplot.style.use('ggplot')\n",
    "\n",
    "def matmult1(A, x):\n",
    "    \"\"\"Entries of y are dot products of rows of A with x\"\"\"\n",
    "    y = np.zeros_like(A[:,0])\n",
    "    for i in range(len(A)):\n",
    "        row = A[i,:]\n",
    "        for j in range(len(row)):\n",
    "            y[i] += row[j] * x[j]\n",
    "    return y\n",
    "\n",
    "def matmult2(A, x):\n",
    "    \"\"\"Same idea, but more compactly\"\"\"\n",
    "    y = np.zeros_like(A[:,0])\n",
    "    for i,row in enumerate(A):\n",
    "        y[i] = row.dot(x)\n",
    "    return y\n",
    "\n",
    "def matmult3(A, x):\n",
    "    \"\"\"y is a linear expansion of the columns of A\"\"\"\n",
    "    y = np.zeros_like(A[:,0])\n",
    "    for j,col in enumerate(A.T):\n",
    "        y += col * x[j]\n",
    "    return y\n",
    "\n",
    "# Define matrix and vector\n",
    "x = np.array([0,1,2])\n",
    "A = np.array([[2,1,2],[3,2,0],[1,0,1]])\n",
    "\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print(\"x =\")\n",
    "print(x)\n",
    "print(\"Ax =\")\n",
    "print(matmult1(A, x))\n",
    "print(matmult2(A, x))\n",
    "print(matmult3(A, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use an earlier version of Python (than 3.5), the `@` operator isn't defined yet. To perform matrix multiplication, you need to use `numpy.dot(A,B)` in place of `A@B`. That will require you to start with `import numpy` before doing any of this. But hey, I think `@` rocks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# matrix-vector product\n",
    "y = A@x\n",
    "print(\"A =\")\n",
    "print(A)\n",
    "print(\"x =\")\n",
    "print(x)\n",
    "print(\"Ax =\")\n",
    "print(np.dot(A,x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise operations\n",
    "x[0] = 1\n",
    "A[0,1] = 2\n",
    "\n",
    "# extracing rows and columns\n",
    "print(\"A[1,:] = \")\n",
    "print(A[1,:])\n",
    "print(\"A[:,1] = \")\n",
    "print(A[:,1])\n",
    "\n",
    "# copy matrix\n",
    "B = A.copy()\n",
    "print(\"B = \")\n",
    "print(B)\n",
    "\n",
    "# transpose\n",
    "print(\"B.T = \")\n",
    "print(B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">3</div>\n",
    "\n",
    "## Mountain workout: For data science Tomb Raiders!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/tomb-raider.gif\" width=300 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define vector: position of mountain climber\n",
    "x = np.array([0,1,2])\n",
    "print(\"initial: \", x)\n",
    "print('---')\n",
    "\n",
    "# Define matrix: jump-twist-and-half-with-three-spins-and-one-amazing-handhold. Tomb raider style! \n",
    "Twist = np.array([[2,1,2],[3,2,0],[1,0,1]])\n",
    "print(\"Twist matrix: \")\n",
    "print(Twist)\n",
    "print('---')\n",
    "\n",
    "# Define matrix: rappel with rope over a dangerous cliff\n",
    "Rappel = np.array([[5,3,1],[4,0,1],[3,2,-1]])\n",
    "print(\"Rappel matrix: \")\n",
    "print(Rappel)\n",
    "print('---')\n",
    "\n",
    "# Position of mountain climber after jump-twist\n",
    "y = Twist @ x\n",
    "print(\"After jump-twist: \", y)\n",
    "print('---')\n",
    "\n",
    "# position of mountain climber after rappel following jump-twist\n",
    "z = Rappel @ y\n",
    "print(\"After rappel: \", z)\n",
    "print('---')\n",
    "\n",
    "# Combination jump-twist and rappel\n",
    "Combo = Rappel @ Twist\n",
    "print(\"Combo matrix: \")\n",
    "print(Combo)\n",
    "print('---')\n",
    "\n",
    "# position of climber from initial position after combination jump-twist and rappel\n",
    "z2 = Combo @ x\n",
    "print(\"After combo: \", z2)\n",
    "print(z == z2)\n",
    "print('---')\n",
    "\n",
    "# Inverse of Combo\n",
    "Cinv = np.linalg.inv(Combo)\n",
    "print(\"Inverse of Combo matrix: \")\n",
    "print(Cinv)\n",
    "print('---')\n",
    "\n",
    "# Initial position of climber:\n",
    "x2 = Cinv @ z2\n",
    "print(\"Initial position of climber: \", x2)\n",
    "print(x2 == x)\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">4</div>\n",
    "\n",
    "## Special operations with arrays\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/batgirl.gif\" width=360 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner products, orthogonality, and Matrix norms\n",
    "\n",
    "The **inner product**\n",
    "$$ x^T y = \\sum_i x_i y_i $$\n",
    "of vectors (or columns of a matrix) tell us about the **magnitude** and about the **angle** formed by the two vectors. Think about it this way: the magnitude, or **norm**, of a vector tells the mountain climber how far she is from her starting point (origin). The **angle** between two vectors tells someone at the origin what the viewing angle is between one climber (one vector), and the other climber (the other vector).\n",
    "\n",
    "The **norm** of a vector is induced by the inner product,\n",
    "$$ \\lVert x \\rVert = \\sqrt{x^T x} $$\n",
    "\n",
    "The **angle** $\\theta$ between the two vectors is defined by\n",
    "$$ \\cos \\theta = \\frac{x^T y}{\\lVert x \\rVert \\, \\lVert y \\rVert} . $$\n",
    "\n",
    "Inner products are **bilinear**, which means that they satisfy some convenient algebraic properties\n",
    "\n",
    "$$ \\begin{split}\n",
    "(x + y)^T z &= x^T z + y^T z \\\\\n",
    "x^T (y + z) &= x^T y + x^T z \\\\\n",
    "(\\alpha x)^T (\\beta y) &= \\alpha \\beta x^T y \\\\\n",
    "\\end{split}  $$\n",
    "\n",
    "The pairwise inner products between two sets of vectors can be expressed by collecting the sets as columns in matrices and writing $A = X^T Y$ where $A_{i,j} = x_i^T y_j$.\n",
    "It follows from this definition that\n",
    "$$ (X^T Y)^T = Y^T X $$\n",
    "\n",
    "\n",
    "If $x^T y = 0$ then we say $x$ and $y$ are **orthogonal** (or \"$x$ is orthogonal to $y$\"). The cosine of the angle between the two vectors is 0, which means the angle formed by the two vectors is a 90 degree angle. If you see one climber due north, the other climber is due east (or west)!\n",
    "\n",
    "A vector is said to be **normalized** if $\\lVert x \\rVert = 1$. It's as if the mountain climber needs to be exactly, say 1km from the origin, in whatever direction.\n",
    "\n",
    "If $x$ is orthogonal to $y$ and $\\lVert x \\rVert = \\lVert y \\rVert = 1$ then we say $x$ and $y$ are **orthonormal**. That's when two climbers are at distinct cardinal points (east, west, north, south), at exactly 1 km. from the origin.\n",
    "\n",
    "In 2D, there are 4 cardinal points, forming two directional axes. In $n$D, there are $2n$ cardinal points forming $n$ directional axes.\n",
    "\n",
    "A square matrix with orthonormal column vectors is said to be an **orthogonal matrix**. If you have $n$ mountain climbers, and your state space is $n$-dimensional, then an orthonormal square matrix is like observing your mountain climbers at each of the distinct $n$ axes of your state space. Together, they form an $n$-dimensional *unit* **hypercube**.\n",
    "\n",
    "We typically use $Q$ or $U$ and $V$ for matrices that are known/constructed to be orthogonal.\n",
    "Orthogonal matrices are always full rank -- the columns are **linearly independent**. That means no one column can be written as a linear combination of any of the other ones. No mountain climber's position is a linear combination of other mountain climber's positions. Which makes these columns a **base** of the underlying vector space: Any vector of that vector space can be written as a linear combination of these column vectors. Any $(n+1)$th mountain climber's position can be written as a linear combination of the position of the other mountain climbers.\n",
    "\n",
    "The inverse of an orthogonal matrix is its transpose:\n",
    "$$ Q^T Q = Q Q^T = I . $$\n",
    "\n",
    "That's nice because transposing a matrix is super easy. Finding the inverse of a non-orthogonal matrix is **hard**. The runtime complexity is bounded by $O(n^3)$. Orthogonal matrices are a powerful building block for robust numerical algorithms.\n",
    "\n",
    "`np.linalg` contains basic linear algebra operations, like norms and [Frobenius norm] (square root of the sum of the absolute squares of its elements). See [MathWorld](http://mathworld.wolfram.com/FrobeniusNorm.html).\n",
    "\n",
    "Vector norms are built into the linear space (and defined in term of the inner product).  Matrix norms are *induced* by vector norms, according to\n",
    "\n",
    "$$ \\lVert A \\rVert = \\max_{x \\ne 0} \\frac{\\lVert A x \\rVert}{\\lVert x \\rVert} . $$\n",
    "\n",
    "* Try to see how **far away** a mountain climber can move, from any starting position - That's the matrix norm and it's a function of the mountain, not of the climber!\n",
    "* This equation makes sense for non-square matrices -- the vector norms of the input and output spaces may differ.\n",
    "* Due to linearity, all that matters is direction of $x$, so it could equivalently be written\n",
    "\n",
    "$$ \\lVert A \\rVert = \\max_{\\lVert x \\rVert = 1} \\lVert A x \\rVert . $$\n",
    "\n",
    "Norms are the metrics that learning machines use to minimize learning constraints.\n",
    "\n",
    "Whew! That was a lot of math, right? But it was *fun*, when you think of it as **mountain climbers** :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special matrices\n",
    "Here we learn how to use `numpy` to write some very useful arrays we are going to need *all the time*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all ones or zeros\n",
    "y = np.zeros(4)\n",
    "print(y)\n",
    "F = np.ones((4,4))\n",
    "print(F)\n",
    "\n",
    "# normally distributed random elements\n",
    "#  array of shape (d0, d1, ..., d4), filled with random floats sampled from a univariate “normal” (Gaussian) distribution \n",
    "# of mean 0 and variance 1. A single float randomly sampled from the distribution is returned if no argument is provided.\n",
    "r = np.random.randn(2,2,2)\n",
    "print(r)\n",
    "\n",
    "# identity matrix\n",
    "I = np.eye(3)\n",
    "print(I)\n",
    "\n",
    "# banded matrix\n",
    "L = np.diag([1,1],k=-1)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a 4 x 4 matrix with $a_{ii} = 2$, $a_{i,j!=i} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "np.ones((4,4)) + np.eye(4)   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Create a 10 x 10 matrix with $a_{ii} = 2$, $a_{i,i+1} = a_{i,i−1=1} = 1$, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "np.diag(np.ones(9),k=-1) + np.diag(np.ones(9),k=1)  + 2 * np.eye(10)  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">5</div>\n",
    "\n",
    "## Inverting matrices\n",
    "\n",
    "Matrix multiplication is *not* **commutative**, as we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[2, 3],[0, 4]])\n",
    "print(B)\n",
    "print(B @ B.T)\n",
    "print(B.T @ B)\n",
    "print(B.T @ B == B @ B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, matrix multiplication is **associative**:\n",
    "\n",
    "$$ A. (B.C) == (A.B) .C$$\n",
    "\n",
    "When we write $x = A^{-1} y$, we mean that $x$ is the unique vector such that $A x = y$.\n",
    "\n",
    "We use `numpy.linalg.inv()` to evaluate the inverse of a matrix, as we saw above.\n",
    "\n",
    "Matrix multiplication is associative! And we can follow the actions of state machines on vector states with matrix multiplication, which is just a math operation that defines how we mutliply spreadsheets together!\n",
    "\n",
    "We can use [numpy.linalg.solve()](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.solve.html) to solve a linear system of equations.\n",
    "\n",
    "For example, what is a mountain climber's position $x$ such that $C$ $x$ = $z$ where $y$ is the column vector (33, 22, 17)?\n",
    "\n",
    "$$ C x = z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[20, 11, 11],[9, 4, 9], [11, 7, 5]])\n",
    "z = np.array([33, 22, 17])\n",
    "solution = np.linalg.solve(C, z)\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's the initial position of our mountain climber! So math is.. just mountain-climbing! And to think that I was scared of math once upon a time... \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/scared.jpg\" width=400 />\n",
    "(bad professors give you bad headaches)\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify\n",
    "C @ solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally\n",
    "print(Cinv @ C)\n",
    "print('---')\n",
    "print(C @ Cinv)\n",
    "print('---')\n",
    "print(np.linalg.solve(C, C))\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Solve the elementary linear algebra question at the introduction of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[3, -2],[5, 1]])\n",
    "z = np.array([3, 1])\n",
    "solution = np.linalg.solve(C, z)\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "import numpy as np\n",
    "\n",
    "C = np.array([[3, 2],[5, 1]])\n",
    "z = np.array([3, 1])\n",
    "solution = np.linalg.solve(C, z)\n",
    "solution\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">6</div>\n",
    "\n",
    "## Eigenvalues and eigenvectors\n",
    "\n",
    ">**Definition**: A scalar $\\lambda$ is an **eigenvalue** for the linear map A (A is the matrix of the linear map) if there exists a non-zero vector $V$ such that $Av = \\lambda v$. Any such vector which satisfies this equation is said to be an **eigenvector** of A, corresponding to the eigenvalue $\\lamda$.\n",
    "\n",
    "Eigenvectors and eigenvalues have a huge number of [applications](http://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Applications), including facial recognition\n",
    "software, geology, quantum mechanics, and web search. So being able to find them quickly is of great significance to researchers and engineers. What’s interesting is that while eigenvectors depend on a choice of basis, eigenvalues do not.\n",
    "\n",
    "### Again!\n",
    "\n",
    "* A Matrix is a mathematical object that acts on a (column) vector, resulting in a new vector, i.e. A**x**=**b**. It represents a **move**, in any direction, for a mountain-climber, such as Tomb Raider.\n",
    "\n",
    "* A vector is like an arrow pointing to a location on the mountain. An **eigenvector** $v$ is a **position on the mountain** $x$ such that ***for a class of mountain moves described by a matrix $C$***, the resulting position (or vector) $y$ following that mountain move is **parallel** to the original position $x$ (some multiple of $x$). \n",
    "\n",
    "It's as if the **new position** of the mountain climber is **in the same direction** as defined by her previous position and the origin (where she started her climb):\n",
    "\n",
    "$$ {C}\\underline{x}=\\lambda \\underline{x} $$\n",
    "\n",
    "$\\lambda$ is its **eigenvalue**. It's **how much** the mountain climber moved from the previous position.\n",
    "\n",
    "\n",
    "### What are eigenvalues?\n",
    "\n",
    "Given $A \\in \\mathbb{R}^{n\\times n}$, $\\lambda$ is the **eigenvalue** of $C$ if there is a non-zero vector $x$, the corresponding **eigenvector**, such that the following is true:\n",
    "\n",
    "$$Cx = \\lambda x, \\;\\; \\text{for} \\; x \\neq 0$$\n",
    "\n",
    "Is there a move that can take a mountain-climber to *some factor* times her original position? If so, the old position is an eigenvector, and the moving factor is its eigenvalue.\n",
    "\n",
    "Formally, given a square matrix $C \\in \\mathbb{R}^{n\\times n}$, we say that $\\lambda \\in \\mathbb{C}$ is an **eigenvalue** of $C$ and $x \\in \\mathbb{C}^n$ is the corresponding **eigenvector**.\n",
    "\n",
    "Intuitively, this definition means that multiplying $C$ by the vector $x$ results in a new vector that points in the same direction as $x$, but is scaled by a factor $\\lambda$.\n",
    "\n",
    "Also note that for any **eigenvector** $x \\in \\mathbb{C}^n$ **of $C$**, and scalar $k \\in \\mathbb{C}, C(kx) = kCx = k\\lambda x = \\lambda(kx)$, so $kx$ is also an **eigenvector of $C$**. For this reason when we talk about **“the” eigenvector** associated with $\\lambda$, we usually assume that the **eigenvector** is normalized to have length $1$ (this still creates some ambiguity, since $x$ and $−x$ will both be **eigenvectors**, but we will have to live with this).\n",
    "\n",
    "For any $\\lambda$ an eigenvalue of $C$, there is a vector space, the **eigenspace**, that corresponds to $\\lambda$:\n",
    "\n",
    "$$\\{ x : C x = \\lambda x \\}$$\n",
    "\n",
    "Any non-zero vector in this space is an eigenvector. One convenient requirement is that the eigenvector has norm $1$.\n",
    "\n",
    "\n",
    "### What are eigenvectors and eigenvalues for mountain climbers?\n",
    "\n",
    "Linear algebra is multiplying vectors (mountain climbers positions) with matrices (technical mountain climber moves), to get to a new mountain position. So what corresponds to an eigenvector, then?\n",
    "\n",
    "***For a specific technical move described by a matrix***, there is a mountain climber position such that her next position will be parallel to her previous position. In other words, an observer at the origin looking at his mountain climber girlfriend very far away through binoculars ***will not have to adjust his binoculars to continue to observe her***: She just either moved a bit closer or a bit further away, but remained in the same direction relative to the observer. If we call the factor by which she moved closer or further away $\\lambda$, her previous position $x$, and her next position $y$, and the technical mountain move $T$, then:\n",
    "\n",
    "$$ y = T \\; x = \\lambda x$$\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/binoculars.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "There is a theorem in linear algebra that says that (almost) all technical mountain moves have such mountain positions $x$. In fact, an $n$ x $n$ matrix $T$ has $n$ such vectors.\n",
    "\n",
    "What's the big deal? We'll see that soon.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">7</div>\n",
    "\n",
    "## Eigenvalues and eigenvectors in SciPy\n",
    "\n",
    "Turns out `scipy` also has a good amount of linear algebra functionality. Which library to pick, `numpy` or `scipy`? *Your* choice.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/hi.png\" width=200 />\n",
    "</center>\n",
    "\n",
    "### Exercise\n",
    "\n",
    "What are the eigenvalues and eigenvectors of our Combo mountain jump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "\n",
    "C = np.array([[20, 11, 11],[9, 4, 9], [11, 7, 5]]) \n",
    "\n",
    "(v, r) = la.eig(C, left = False) # You can read the help, buy the left eigenvectors don't get created without this. \n",
    "d = sp.diag(v)  # by default, eig puts the eigenvalues in a 1-D array. We will need a diagonal matrix in a moment.\n",
    "\n",
    "print(v)\n",
    "print(d)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.random.randn(4,4)\n",
    "(eigenvalues, eigenvectors) = la.eig(R, left = False)\n",
    "print(eigenvalues)\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the mountain climber starts at any of the three positions described by the eigenvectors, she will end up facing in the same direction (from the perspective of her observer boyfriend at the origin of the mountain), just slightly closer or further away from him, by a factor of the eigenvalue associated with the eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">8</div>\n",
    "\n",
    "## Application of Eigenvectors: Markov Chains\n",
    "\n",
    "A **Markov chain** is any *memorlyess* process that can be modeled with a *stochastic transition* matrix. It's like a mountain climber that always forgets her previous position, and only cares about the next (which is the right state of mind if you're climbing *El Capitan* in Yellowstone National Park! Did you see the movie *Free Solo*? Scariest movie *ever*!)\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/moody.png width = 600 />\n",
    "    Moody's credit rating transitions\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "### Repeated transitions\n",
    "Conjecture: If the transition matrix $P$ is the same after each step (time homogeneous), then the k-step transition probability is.. the k-th power of the transition matrix, $P^k$.\n",
    "\n",
    "I don't think I need to convince you of this, we illustrated this with the mountain climber and the Combo matrix!\n",
    "\n",
    "And so, the k-step transition is characterized by the matrix $A^k$.\n",
    "\n",
    "### Steady state limit\n",
    "Now what if, in the long term, after many powers of $k$, we suddenly transition to a condition where the states don't change very much? Well, we acheive what is called a **steady-state** condition: \n",
    "\n",
    "$$P^{\\;k+1} = P^{\\;k} \\;\\text{, for  very large} \\; k$$\n",
    "\n",
    "Stationary distributions are super-interesting to Wall Street, because they describe the ***future***! Suppose we call that future the state vector $\\pi$. Then:\n",
    "\n",
    "$$\\pi = P . \\pi$$\n",
    "\n",
    "Hey, wait a second, does that equation remind you of anything?\n",
    "\n",
    "\n",
    "### Eigenvectors and.. the *future*\n",
    "The eigenvectors $X_1, \\cdots X_n$ of $n$ x $n$ matrix $A$, and its associated eigenvalues $λ_i$, are such that:\n",
    "\n",
    "$$A X_i = λ_i X_i$$\n",
    "\n",
    "And so the **future**, described as state vector $\\pi$, is the ***eigenvector associated with the eigenvalue 1***!\n",
    "\n",
    "**Google** realized that this corresponds to the steady state condition of a sliver surfer surfing the Web through its hyperlinks. It descibes the probabilistic future state of the sliver surfer. If you rank all of its components from highest probability to lowest probability, it's the Google search page for your search query (corresponding to a silver surfer surfing for Web pages relevant to your query terms)!\n",
    "\n",
    "Assume the mountain climber climbs the entire mountain using just one type of jump, let's call this the **Twist** matrix jump.\n",
    "\n",
    "Imagine that we are able to represent any position on the mountain as a **linear combination of eigenvectors** of the **Twist** matrix. That, in fact, is a ***theorem*** that is true if the **Twist** matrix is **non-singular** (has a non-zero [determinant](https://en.wikipedia.org/wiki/Determinant)).\n",
    "\n",
    "To find the mountain climber's position after $n$ **Twist** jumps, we multiply her position vector by the **Twist** matrix, $n$ times, right? Which corresponds to a single multiplication by the matrix **Twist$^n$**, right? Each application of **Twist** adds a factor of the eigenvalue $λ$ to the coefficient of each eigenvector in the expansion of the state. Recall: The eigenvectors don’t transform much.. They just squish or expand (by a factor of $λ$). So after $n$ transitions, the coefficient of each eigenvector has gained a factor $λ^n$.\n",
    "\n",
    "If λ > 1, this factor would grow without bound as $n$ increases, leading to matrix elements larger than 1, which would catapult the mountain climber to the moon! So, there must not be any eigenvalue strictly larger than one!\n",
    "\n",
    "If λ < 1, this factor would suppress the contribution of the corresponding eigenvector as $n$ increases. It’s fine if this happens to some eigenvectors, but if it happens to *all of them*, the probability interpretation of our state vector would also be ruined, because all entries in our vector would be driven to zero, and the mountain climber would **never move**.\n",
    "\n",
    "So, **there must be at least one eigenvalue greater or equal to one!**\n",
    "\n",
    "The **dominant eigenvector** (eigenvector corresponding to the eigenvalue = 1) of any Markov chain *is* the **steady-state limit (the *long-term future*) of the Markov chain**!\n",
    "\n",
    "The dominant eigenvector will become the *only unsuppressed contribution* to the state vector as $n$ gets large.\n",
    "This eigenvector therefore represents the steady-state towards which every **linear system** tends to.\n",
    "\n",
    "The dominant eigenvector is the future of Markov chains. That is why we love Markov chains: With a bot of math, wwe can predict their future!\n",
    "\n",
    "So if we start with a random vector (random mountain climber position), and apply the **Twist** matrix a million times, we will ultimately converge to the dominant eigenvector, since all other eigenvalue-to-the-power-a-million contributions will become suppressed. \n",
    "\n",
    "It's the same method used by Google to rank the WWW!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Find the eigenvalues and eigenvectors of the square of the following matrix as well as the inverse of the matrix minus the identity matrix. Use `sympy`, a symbolic math manipulation program.\n",
    "```(python)\n",
    "pip install sympy\n",
    "```\n",
    "\n",
    "$$ {A}=\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & -2 \\\\ 0 & 1 & 4 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import Matrix\n",
    "A = Matrix([[1, 2, 3], [0, 1, -2], [0, 1, 4]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvalues\n",
    "A.eigenvals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "A.eigenvals()    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvectors\n",
    "A.eigenvects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "A.eigenvects()  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Compute the eigenvalues and eigenvectors of the following upper triangular matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Matrix([[3, 1], [0, 3]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.eigenvects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two eigenvalues, both equal to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This matrix has one eigenvector with multiplicity 2. The associated eigenvalue is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Look at the following upper triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Matrix([[3, 1, 1], [0, 3, 4], [0, 0, 3]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.eigenvects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix has one eigenvector with multiplicity 3. The associated eigenvalue is 3, as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Look at the following matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg as la\n",
    "\n",
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]])  # Defining the array \n",
    "\n",
    "(v, r) = la.eig(A, left = False) # You can read the help, buy the left eigenvectors don't get created without this. \n",
    "d = sp.diag(v)  # by default, eig puts the eigenvalues in a 1-D array. We will need a diagonal matrix in a moment.\n",
    "\n",
    "print(A)\n",
    "print(v)\n",
    "print(d)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(la.inv(r)@A@r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, one never looks for just the inverse matrix. What you typically want is the inverse of a matrix times another matrix or vector, which is the solution to a linear algebra problem. \n",
    "\n",
    "We can use `la.solve` to obtain this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.solve(r,A)@r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that `la.inv(r)@A =  la.solve(r, A)`.\n",
    "\n",
    "What is the real value of this matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(la.solve(r,A)@r).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to out previous result:\n",
    "```(python)\n",
    "la.inv(r)@A@r = D\n",
    "```\n",
    "where D is the diagonal matrix with the eigenvalues of A in its diagonal.\n",
    "\n",
    "That is a very important result, and this is why:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a little bit of math..\n",
    "\n",
    "Define the matrices $P$ and $D$ composed of eigenvectors and eigenvalues in the main diagonal/zero everywhere else:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/pd.png width = 400 />\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "Then:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/appd.png width = 370 />\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "And if:\n",
    "\n",
    "$$ A P = P D$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$ A = P D P^{-1}$$\n",
    "\n",
    "So, **any matrix** $A$ can be written as the product of a matrix $P$ whose columns are composed of the eigenvectors of $A$, times the matrix composed of the eigenvalues of $A$ in the main diagonal, times the inverse of $P$. That is exactly what our exercise above proved, for an example matrix `A = np.array([[1,2,3],[4,5,6],[7,8,9]])`.\n",
    "\n",
    "Neat, so what?\n",
    "\n",
    "### So What?\n",
    "\n",
    "Take the power of that equation:\n",
    "\n",
    "$$ A^2 = P D P^{-1} .  P D P^{-1} = P D . P^{-1} P . D P^{-1} = P D . D P^{-1} = P D^2 P^{-1} $$\n",
    "\n",
    "And if you keep doing this many times, you will see that:\n",
    "\n",
    "$$ A^n = P D^n P^{-1} $$\n",
    "\n",
    "So, just compute $D^n$ and ***that gives you the long-term (steady-state) transition matrix of any Markov chain*** described the transition matri A, assuming you can find all the eigevectors and eigenvalues of A!. \n",
    "\n",
    "It is easy to compute $D^n$: It is just the matrix with the n-powers of the eigenvalues in the main diagonal! \n",
    "\n",
    "$P$ and $P^{-1}$ have no powers, so they're *easy* to compute if you have the eigenvectors of $A$:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/dn.png width = 200 />\n",
    "</center>\n",
    "<br />\n",
    "\n",
    "### Revisiting the fixed-point theorem\n",
    "\n",
    "So, if $n$ is a large number, what happens to $\\lambda^n$? \n",
    "\n",
    "What happens if $\\lambda < 1$, and what happens if $\\lambda > 1$?\n",
    "\n",
    "Is it possible that $\\lambda$ is bigger than 1? Does $A^n$ remain *stochastic*?\n",
    "\n",
    "And what if all $\\lambda_i$ are less than one?\n",
    "\n",
    "So what does this tell you?\n",
    "\n",
    "Think about a Markov chain and its stochastic transition matrix $A$. That matrix has eigenvectors $V_i$.\n",
    "\n",
    "Imagine that we are able to represent the state of the system as a **linear combination of the eigenvectors** $V_i$ (we can, there is a theorem in linera algebra that says we can, it's just a change of bases).\n",
    "\n",
    "To find the probability distribution of states after $n$ transitions, we multiply this vector by $A$, $n$ times, right? Each application of $A$ adds a factor of the eigenvalue $λ$ to the coefficient of each eigenvector in the expansion of the state. Recall: The eigenvectors don’t transform much.. They just squish or expand (by a factor of $λ$). So after $n$ transitions, the coefficient of each eigenvector has gained a factor $λ^n$.\n",
    "\n",
    "If λ > 1, this factor would grow without bound as $n$ increases, leading to matrix elements larger than 1, which would ruin the probability interpretation of our state vector.. So, there must not be any eigenvalue strictly larger than one!\n",
    "\n",
    "If λ < 1, this factor would suppress the contribution of the corresponding eigenvector as $n$ increases. It’s fine if this happens to some eigenvectors, but if it happens to *all of them*, the probability interpretation of our state vector would also be ruined, because all entries in our vector would be driven to zero..\n",
    "\n",
    "So, ***there must be at least one eigenvalue equal to one***!\n",
    "\n",
    "### The dominant eigenvector\n",
    "\n",
    "The dominant eigenvector (eigenvector corresponding to the eigenvalue = 1) of any Markov chain *is* the **steady-state limit (the *long-term future*) of the Markov chain**!\n",
    "\n",
    "The dominant eigenvector will become the *only unsuppressed contribution* to the state vector as $n$ gets large.\n",
    "This eigenvector therefore represents the steady-state towards which every **linear system** tends to.\n",
    "\n",
    "Stochastic matrices are guaranteed to have eigenvalues not larger than one, and to have *at least* one eigenvalue equal to one. The components of the dominant eigenvector are proportional to the relative occupation probabilities of states in the long run.\n",
    "\n",
    "Now we didn't prove that a Markov chain *has* a steady state, or that it is unique. *Only* that *if* there is a steady state, *and* it is unique, then it is given by the dominant eigenvector. \n",
    "\n",
    "But actually, for Markov chains, there is indeed only one so-called *fixed point* (steady-state). That is why Markov chains are so interesting and sexy. And for the math, go [here](https://en.wikipedia.org/wiki/Fixed-point_theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">9</div>\n",
    "\n",
    "## Sparse matrix Representation\n",
    "\n",
    "**Sparse matrix** representation is very important for **Big Data**. If a matrix is made of mostly zero entries, we need to find more *compact* representations. `scipy.sparse` does this well.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/carcrusher.jpg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy module for sparse matrices\n",
    "import scipy.sparse as spp\n",
    "\n",
    "# sparse identity matrix\n",
    "D = spp.eye(100)\n",
    "\n",
    "# random sparse matrix\n",
    "R = spp.random(100,100,density=0.01)\n",
    "\n",
    "# plot a sparse matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.spy(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's about all the math you need to know :-)\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src = ipynb.images/Calvin_and_hobbes._teacher.gif width = 400 />\n",
    "</center>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: gray\">10</div>\n",
    "\n",
    "## Cooldown\n",
    "- Wow, this notebook was a real workout. My arms and fingers are hurting from all the climbing. Can we rest a little bit, professor?\n",
    "\n",
    "\n",
    "- Yes, let's cool down with some linear algebra. \n",
    "\n",
    "Linear algebra in n-dimensional space is all about climbing a mountain in n dimensions, and each step can be represented by multiplying the position vector of the mountain climber by a step-matrix, to move to a new position represented by a new position vector. The position vector is just the arrow from the origin (start of the climb) to the current position of the climber. So as the mountain climber moves, that vector essentially rotates around its axis (the origin), and thus matrix multiplication is essentially a **solid rotation**. A matrix describes a specific kind of mountain move. For that mountain move, the matrice's eigenvectors describe all possible position vectors that are parallel before and after the move, while its eigenvalues describe by how much the climber has actually moved in that direction.\n",
    "\n",
    "Wow, math is ***simple*** when explained right, right? \n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/warmup.png\" width=200 />\n",
    "</center>\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/bouldering.jpg\" width=600 />\n",
    "</center>\n",
    "\n",
    "In short, matrices represent **rigid transformations** in vector space, mapping one **state vector** to another state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = np.deg2rad(45)\n",
    "R = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "              [np.sin(theta),  np.cos(theta), 0],\n",
    "              [            0,              0, 1]])\n",
    "\n",
    "print(\"R times the x-axis:\", R @ [1, 0, 0])\n",
    "print(\"R times the y-axis:\", R @ [0, 1, 0])\n",
    "print(\"R times a 45 degree vector:\", R @ [1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since multiplying a vector by $R$ rotates it 45 degrees, multiplying the result\n",
    "by $R$ again should result in the original vector being rotated 90 degrees.\n",
    "Matrix multiplication is associative, which means that $R(Rv) = (RR)v$, so\n",
    "$S = RR$ should rotate vectors by 90 degrees around the z-axis. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = R @ R\n",
    "S @ [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R @ z-axis:\", R @ [0, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R rotates both the x and y axes, but not the z-axis.\n",
    "\n",
    "What does that make the z-axis? If you thought that's an **eigenvector** of R, that is the *right answer*! What is its eigenvalue?\n",
    "\n",
    "See, if you're a mountain climber, linear algebra and eigenvectors come naturally to you.\n",
    "\n",
    "If you've never climbed a mountain and linear algebra remains a little bit of challenge, you could pick up an introductory linear algebra textbook, e.g. Gil Strang's [Linear Algebra](http://math.mit.edu/~gs/linearalgebra/) and its Applications. It's free. \n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/wink.jpg\" width=100 />\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
