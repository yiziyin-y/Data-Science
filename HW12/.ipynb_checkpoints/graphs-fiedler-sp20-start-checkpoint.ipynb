{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 12 Day 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 30 March 2020, with material from J. Nunez-Iglesias, L.R. Varshney, and the LibIgl team</div>\n",
    "\n",
    "# What else can you do with Eigenvectors and Eigenvalues?\n",
    "\n",
    "I told you that graphs are just a low-dimensional way of representing surfaces, We will continue exploring this concept. We will also learn to draw **surfaces**. The math in differential geometry can put you to sleep faster than Nyquil can, so we need to make it fun by plotting neat models, so we'll do that in this notebook. \n",
    "\n",
    ">**NOTE**: Surfaces are nothing more than higher-dimensional versions of the same 1D curves wew started with in class. And you know what data science is all about: You get some data, you take its histogram, and you try to represent that graph with some predefined math functions (that have parameters). If you are successful, you went back in time and uncovered the process that generated the data. Good job! Its parameters are all you need to go to the future and get wicked-low dimensional representations of your data. You can now predict a bunch of things. In 3D and above, it's much tougher to find mathematical functions to represent these surfaces. That is why we turn to graphs to draw them. The graphs are *not* the surface, they are a *way* to *draw* the surface. That's what your brain does. And that's what Neural Networks do.\n",
    "\n",
    "When we're done learning to draw surfaces, we reproduce results from a scientific paper exploring the [connectome](https://en.wikipedia.org/wiki/Connectome) of a (small) brain, using the linear algebra tools we learned. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/frankenstein.jpg\" width=300 />\n",
    "    Prometheus Frankenstein\n",
    "</center>\n",
    "\n",
    "<br />\n",
    "The graph below is the graph of Wikipedia. Looks a bit like a brain!\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/90/Visualization_of_wiki_structure_using_prefuse_visualization_package.png\" width=500>\n",
    "<!-- caption text=\"Visualization of wikipedia structure. Created by Chris Davis and released under CC-BY-SA-3.0 (https://commons.wikimedia.org/wiki/GNU_Free_Documentation_License).\" -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed formats for NumPy arrays\n",
    "\n",
    "We learned how to save big arrays as sparse arrays. But what if we have dense arrays? We need to learn how to save NumPy data in compressed formats. First we learn how to save in a **.CSV** File (ASCII), then in an **.NPY** File (binary), and then in an **.NPZ File** (compressed).\n",
    "\n",
    "In .csv format, the array has a single row of data with 10 columns. We would expect this data to be saved to a CSV file as a single row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as csv file\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "# define data\n",
    "data = asarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "# save to csv file\n",
    "savetxt('data/mysimpledata.csv', data, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how we load it back in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy array from csv file\n",
    "from numpy import loadtxt\n",
    "# load array\n",
    "data = loadtxt('data/mysimpledata.csv', delimiter=',')\n",
    "# print the array\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have a lot of data in NumPy arrays that we wish to save efficiently, but which we only need to use in another Python program. Therefore, we can save the NumPy arrays into a native binary format that is efficient to both save and load.\n",
    "\n",
    "This is common for input data that has been prepared, such as transformed data, that will need to be used as the basis for testing a range of machine learning models in the future or running many experiments. The .npy file format is appropriate for this use case and is referred You can load this file as a NumPy array later using the load() function.to as simply **NumPy format**.\n",
    "\n",
    "This can be achieved using the `save()` NumPy function and specifying the filename and the array that is to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "# define data\n",
    "data = asarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "# save to npy file\n",
    "save('data/mysimpledata.npy', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load this file as a NumPy array later using the `load()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy array from npy file\n",
    "from numpy import load\n",
    "# load array\n",
    "data = load('data/mysimpledata.npy')\n",
    "# print the array\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we prepare data for modeling that needs to be reused across multiple experiments, but the data is large. This might be pre-processed NumPy arrays like a corpus of text (integers) or a collection of rescaled image data (pixels). In these cases, it is desirable to both save the data to file, but also in a compressed format. This allows gigabytes of data to be reduced to hundreds of megabytes and allows easy transmission to other servers of cloud computing for long algorithm runs.\n",
    "\n",
    "The .npz file format is appropriate for this case and supports a compressed version of the native NumPy file format. The `savez_compressed()` NumPy function allows multiple NumPy arrays to be saved to a single compressed .npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as npz file\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "# define data\n",
    "data = asarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "# save to npy file\n",
    "savez_compressed('data/mysimpledata.npz', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load this file later using the same load() function from the previous section.\n",
    "\n",
    "In this case, the `savez_compressed()` function supports saving multiple arrays to a single file. Therefore, the `load()` function may load multiple arrays.\n",
    "\n",
    "The loaded arrays are returned from the `load()` function in a dict with the names ‘arr_0’ for the first array, ‘arr_1’ for the second, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy array from npz file\n",
    "from numpy import load\n",
    "# load dict of arrays\n",
    "dict_data = load('data.npz')\n",
    "# extract the first array\n",
    "data = dict_data['arr_0']\n",
    "# print the array\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know how to read and save data, let's plot it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libigl\n",
    "\n",
    "We need to learn how to plot graphs and manifolds!\n",
    "\n",
    "`Libigl` is an open source C++ library for geometry processing research and development. Let's install the python bindings:\n",
    "\n",
    "`Libigl` can be downloaded from [Conda forge](https://anaconda.org/conda-forge/igl):\n",
    "```\n",
    "conda install -c conda-forge igl \n",
    "```\n",
    "\n",
    "All of `libigl` functionality depends only on `numpy` and `scipy`. I like simple!\n",
    "\n",
    "For visualization, we will use [meshplot](https://anaconda.org/conda-forge/meshplot) which can also be easily installed from Conda:\n",
    "```\n",
    "conda install -c conda-forge meshplot \n",
    "```\n",
    "Libigl uses `numpy` to encode vectors and matrices and `scipy` for sparse matrices. And we already know everything about both!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangular Meshes with meshplot\n",
    "\n",
    "A triangular [mesh](https://en.wikipedia.org/wiki/Types_of_mesh) is encoded as a pair of numpy matrices:\n",
    "\n",
    "`v` is an $N$ by 3 matrix which stores the coordinates of the nodes (or vertices) of a graph. \n",
    "Each row stores the coordinate of a vertex, with its x, y and z coordinates in the first,\n",
    "second and third column, respectively. \n",
    "\n",
    "The matrix `f` stores the triangle connectivity: each line of `f` denotes a triangle whose 3 vertices are\n",
    "represented as indices pointing to rows of `f`.\n",
    "\n",
    "Clear?\n",
    "\n",
    "For example, here are two faces of a pyramid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import meshplot as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.array([\n",
    "    [0., 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [2, 1, 1]\n",
    "])\n",
    "\n",
    "F = np.array([\n",
    "    [0, 1, 2],\n",
    "    [1, 3, 2]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(V, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can zoom in and out by using your mouse's scroll bar, and rotate the manifold using your mouse! If you don't have a mouse, that will teach you to buy a mouse. You cannot do data science with a touch pad!\n",
    "\n",
    "And here is a bunny as a bunch of triangles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v: np.array\n",
    "f: np.array\n",
    "    \n",
    "data = np.load('data/data.npz')\n",
    "v, f, n, fs = data[\"v\"], data[\"f\"], data[\"n\"], data[\"fs\"]\n",
    "v1, f1, v2, f2 = data[\"v1\"], data[\"f1\"], data[\"v2\"], data[\"f2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar field visualization\n",
    "\n",
    "Colors and normals can be associated to faces or vertices using the same `plot` function with three parameters.\n",
    "\n",
    "The key parameter `c` represents the vertex or face colors and can be one of the following:\n",
    "\n",
    "1. A #v by 1 vector with one function value per vertex, which gets normalized and converted into vertex color values using the [viridis](https://matplotlib.org/examples/color/colormaps_reference.html) colormap.\n",
    "2. A #v by 3 vector with RGB color values per vertex. The color values should be in the range 0.0-1.0.\n",
    "3. A single color value for all vertices in the form of a numpy array [R, G, B] in the range 0.0-1.0.\n",
    "4. A #f by 1 vector with one function value per face, which gets normalized and converted into face color values using the [viridis](https://matplotlib.org/examples/color/colormaps_reference.html) colormap.\n",
    "5. A #f by 3 vector with RGB color values per face. The color values should be in the range 0.0-1.0.\n",
    "\n",
    "The following four examples show vertex function colors (in this case just the y-coordinate), vertex normals as colors per vertex, random colors per face and face function colors (in this case the size of the faces):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = mp.subplot(v, f, c=v[:, 1], s=[2, 2, 0])\n",
    "mp.subplot(v, f, c=n, s=[2, 2, 1], data=d)\n",
    "mp.subplot(v, f, c=np.random.rand(*f.shape), s=[2, 2, 2], data=d)\n",
    "mp.subplot(v, f, c=fs, s=[2, 2, 3], data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Point Clouds\n",
    "We can also visualize point clouds, their properties and additional debugging information through the `plot` function, by just leaving the faces array empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.plot(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the surface plot, we can also set color values for all points in the point cloud. This can be done either by passing function values or directly by passing colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = mp.subplot(v, c=v[:, 1], s=[1, 2, 0], shading={\"point_size\": 0.03})\n",
    "mp.subplot(v, c=np.random.rand(*v.shape), s=[1, 2, 1], data=d, shading={\"point_size\": 0.03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlays, Textures and Shading\n",
    "\n",
    "In addition to plotting the surface, the viewer supports the visualization of bounding boxes, points and lines. These overlays can be very helpful while developing geometric processing algorithms to plot debug information.\n",
    "\n",
    "The following example draws a point of a given color for each row of `v_box`. The point is placed at the coordinates specified in each row of `v_box`, which is a #v_box by 3 matrix.\n",
    "In addition, edges of a given color are drawn for the vertices `v_box` with the indices `f_box`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.min(v, axis=0)\n",
    "ma = np.max(v, axis=0)\n",
    "\n",
    "# Corners of the bounding box\n",
    "v_box = np.array([[m[0], m[1], m[2]], [ma[0], m[1], m[2]], [ma[0], ma[1], m[2]], [m[0], ma[1], m[2]],\n",
    "                  [m[0], m[1], ma[2]], [ma[0], m[1], ma[2]], [ma[0], ma[1], ma[2]], [m[0], ma[1], ma[2]]])\n",
    "\n",
    "# Edges of the bounding box\n",
    "f_box = np.array([[0, 1], [1, 2], [2, 3], [3, 0], [4, 5], [5, 6], [6, 7], \n",
    "                  [7, 4], [0, 4], [1, 5], [2, 6], [7, 3]], dtype=np.int)\n",
    "\n",
    "p = mp.plot(v, f, return_plot=True)\n",
    "\n",
    "p.add_edges(v_box, f_box, shading={\"line_color\": \"red\"});\n",
    "p.add_points(v_box, shading={\"point_color\": \"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events and Widgets\n",
    "\n",
    "The viewer supports to use interactive widgets from the [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/user_guide.html) package to manipulate the plot. \n",
    "\n",
    "Remember the `v1`'s and `f1`'s we loaded with the bunny data? We actually loaded a bunch of other pictures in the same compressed file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v = [v1, v2]\n",
    "f = [f1, f2]\n",
    "p = mp.plot(v1, f1, return_plot=True)\n",
    "\n",
    "@mp.interact(mesh=[('bump', 0), ('fertility', 1)])\n",
    "def ff(mesh):\n",
    "    mp.plot(v[mesh], f[mesh], plot=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Plotting\n",
    "\n",
    "Besides interactive plotting in Jupyter Notebooks, `meshplot` supports to plot objects in offline html pages. The offline mode is automatically selected, if `meshplot` is run outside of a Jupyter Notebook. Within Jupyter Notebooks, one can manually switch to offline mode as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.offline()\n",
    "mp.plot(v1, f1, c=v1[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without parameters, the plot is stored with the name `<UUID>.html`. It is possible to save Jupyter plots after they are generated and to chose the filename as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.jupyter()\n",
    "p = mp.plot(v1, f1, c=np.random.rand(*f1.shape), return_plot=True)\n",
    "p.add_mesh(v1 + 5, f1, c=v1[:,1]);\n",
    "p.add_points(v1 - 5, c=v1[:,2], shading={\"point_size\": 1.0})\n",
    "p.save(\"test.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More refined mesh data with igl\n",
    "\n",
    "`Libigl` provides input and output functions to read and write many common mesh formats.\n",
    "The IO functions are `igl.read_\\*` and `igl.write_\\*`.\n",
    "\n",
    "Reading a mesh from a file requires a single libigl function call. Here, we read in our bunny in `off` format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igl as ig\n",
    "\n",
    "## Load a mesh in OFF format\n",
    "v, f = ig.read_triangle_mesh('data/bunny.off')\n",
    "\n",
    "## Print the vertices and faces matrices \n",
    "#print(\"Vertices: \", len(v))\n",
    "#print(\"Faces: \", len(f))\n",
    "\n",
    "mp.plot(v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a cow if you don‘t like bunnies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a mesh in OFF format\n",
    "v, f = ig.read_triangle_mesh('data/cow.off')\n",
    "\n",
    "## Print the vertices and faces matrices \n",
    "#print(\"Vertices: \", len(v))\n",
    "#print(\"Faces: \", len(f))\n",
    "\n",
    "mp.plot(v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a teapot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a mesh in OFF format\n",
    "v, f = ig.read_triangle_mesh('data/teapot.off')\n",
    "\n",
    "## Print the vertices and faces matrices \n",
    "#print(\"Vertices: \", len(v))\n",
    "#print(\"Faces: \", len(f))\n",
    "\n",
    "mp.plot(v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teapots are very famous in graphics processing. Computer graphic processing was invented using [teapots](https://en.wikipedia.org/wiki/Utah_teapot). It is the `Hello World` of graphics programming!\n",
    "\n",
    "This igl function reads the mesh bumpy.off and returns the `v` and `f` matrices.\n",
    "Similarly, a mesh can be written to an OBJ file using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mesh in OBJ format\n",
    "ret = ig.write_triangle_mesh(os.path.join(root_folder, \"data\", \"bunny_out.obj\"), v, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Differential Geometric Quantities and Operators\n",
    "\n",
    "### Gaussian curvature\n",
    "\n",
    "[Gaussian curvature](https://en.wikipedia.org/wiki/Gaussian_curvature) on a continuous surface is defined as the **product** of the [principal curvatures](https://en.wikipedia.org/wiki/Principal_curvature):\n",
    "\n",
    " $k_G = k_1 k_2.$\n",
    "\n",
    "As an _intrinsic_ measure, it depends on the metric and not the surface's embedding.\n",
    "\n",
    ">**DEFINITION**: In differential geometry, the two principal curvatures at a given point of a surface are the **eigenvalues** of the [shape operator](https://en.wikipedia.org/wiki/Differential_geometry_of_surfaces#Shape_operator) at the point and the principal directions are its **eigenvectors**. They measure how the surface bends by different amounts in different directions at that point.\n",
    "\n",
    "Intuitively, Gaussian curvature tells how locally spherical or _elliptic_ (`yellow`) the\n",
    "surface is ( $k_G>0$ ), how locally saddle-shaped or _hyperbolic_ the surface\n",
    "is ( $k_G<0$ ), or how locally cylindrical or _parabolic_ (`blue`) ( $k_G=0$ ) the\n",
    "surface is.\n",
    "\n",
    "Think yellow for [convex](https://en.wikipedia.org/wiki/Convex_set) shapes, and blue for [concave](https://en.wikipedia.org/wiki/Convex_set#Non-convex_set) shapes.\n",
    "\n",
    "In the discrete setting, one definition for a \"discrete Gaussian curvature\"\n",
    "on a triangle mesh is via a vertex's _angular deficit_:\n",
    "\n",
    " $k_G(v_i) = 2π - \\sum\\limits_{j\\in N(i)}θ_{ij},$\n",
    "\n",
    "where $N(i)$ are the triangles incident on vertex $i$ and $θ_{ij}$ is the angle\n",
    "at vertex $i$ in triangle $j$ <cite data-cite=\"meyer2003\">(Meyer, 2003)</cite>.\n",
    "\n",
    "Just like the continuous analog, our discrete Gaussian curvature reveals\n",
    "elliptic, hyperbolic and parabolic vertices on the domain.\n",
    "\n",
    "Let's compute Gaussian curvature and visualize it in pseudocolor. First, calculate the curvature with libigl and then plot it in pseudocolors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, f = ig.read_triangle_mesh('data/bumpy.off')\n",
    "k = ig.gaussian_curvature(v, f)\n",
    "mp.plot(v, f, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean curvature\n",
    "The two principal curvatures $(k_1,k_2)$ at a point on a surface measure how\n",
    "much the surface bends in different directions. The directions of maximum and\n",
    "minimum (signed) bending are called principal directions and are always\n",
    "orthogonal.\n",
    "\n",
    "[Mean curvature](https://en.wikipedia.org/wiki/Mean_curvature) is defined as the **average** of the [principal curvatures](https://en.wikipedia.org/wiki/Principal_curvature):\n",
    "\n",
    " $H = \\frac{1}{2}(k_1 + k_2).$\n",
    "\n",
    "One way to extract mean curvature is by examining the Laplace-Beltrami operator\n",
    "applied to the surface positions. The result is a so-called mean-curvature\n",
    "normal:\n",
    "\n",
    "  $-\\Delta \\mathbf{x} = H \\mathbf{n}.$\n",
    "\n",
    "It is easy to compute this on a discrete triangle mesh in libigl using the\n",
    "cotangent Laplace-Beltrami operator <cite data-cite=\"meyer2003\">(Meyer, 2003)</cite>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "l = ig.cotmatrix(v, f)\n",
    "m = ig.massmatrix(v, f, ig.MASSMATRIX_TYPE_VORONOI)\n",
    "\n",
    "minv = sp.sparse.diags(1 / m.diagonal())\n",
    "\n",
    "hn = -minv.dot(l.dot(v))\n",
    "h = np.linalg.norm(hn, axis=1)\n",
    "mp. plot(v, f, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean curvature captures areas of extreme curvature more than gaussian curvature. Gaussian curavgure captures convexithyh and concavity better.\n",
    "\n",
    "## Professor, why are we studying curvature in a data science class?\n",
    "\n",
    "Because curvature is one way your brain can store knowledge, as geometry (using graphs to build the geometry). That geometry is probably scale-invariant. So think scalable vector graphics, not raster graphics. Your brain's surfaces  represents memory and models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal directions\n",
    "\n",
    "A robust method for determining principal curvatures is via quadric fitting  <cite data-cite=\"panozzo2010\">(Panozzo, 2010)</cite>. In the neighborhood around every vertex, a best-fit quadric is found and principal curvature values and directions are analytically computed on this quadric.\n",
    "\n",
    "Of course, when you draw surfaces with triangular meshes, the principal directions are very straightforward: They are the edges of every vertex. If the vertex connects to more than one triangle, it will have many principal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2, k1, k2 = ig.principal_curvature(v, f)\n",
    "h2 = 0.5 * (k1 + k2)\n",
    "p = mp.plot(v, f, h2, shading={\"wireframe\": False}, return_plot=True)\n",
    "\n",
    "avg = ig.avg_edge_length(v, f) / 2.0\n",
    "p.add_lines(v + v1 * avg, v - v1 * avg, shading={\"line_color\": \"red\"})\n",
    "p.add_lines(v + v2 * avg, v - v2 * avg, shading={\"line_color\": \"green\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the creator of the character, Eduard Uspensky (1965), [Cheburashka](https://en.wikipedia.org/wiki/Cheburashka) is an *animal unknown to science*, with large monkey-like ears and a body resembling that of a cub, who lives in a tropical forest. He accidentally gets into a crate of oranges, eats his fill, and falls asleep. \n",
    "\n",
    "Of course, Cheburashka is not very unknown to *us*. We know him well as a cross between `Pikachu` and `Gengar` from [Pokemon]((https://www.ranker.com/list/complete-list-of-all-pokemon-characters/video-game-info)). What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, f = ig.read_triangle_mesh('data/cheburashka.off')\n",
    "u = ig.read_dmat('data/cheburashka-scalar.dmat')\n",
    "\n",
    "g = ig.grad(v, f)\n",
    "gu = g.dot(u).reshape(f.shape, order=\"F\")\n",
    "\n",
    "gu_mag = np.linalg.norm(gu, axis=1)\n",
    "p = mp.plot(v, f, u, shading={\"wireframe\":False}, return_plot=True)\n",
    "\n",
    "max_size = ig.avg_edge_length(v, f) / np.mean(gu_mag)\n",
    "bc = ig.barycenter(v, f)\n",
    "bcn = bc + max_size * gu\n",
    "p.add_lines(bc, bcn, shading={\"line_color\": \"black\"});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "How did we give Cheburashka hairs? Using gradients!\n",
    "\n",
    "Scalar functions on a surface can be discretized as a piecewise linear function\n",
    "with values defined at each mesh vertex:\n",
    "\n",
    "$f(\\mathbf{x}) \\approx \\sum\\limits_{i=1}^n \\phi_i(\\mathbf{x})\\, f_i,$\n",
    "\n",
    "where $\\phi_i$ is a piecewise linear hat function defined by the mesh so that\n",
    "for each triangle $\\phi_i$ is _the_ linear function which is one only at\n",
    "vertex $i$ and zero at the other corners.\n",
    "\n",
    "![Hat function $\\phi_i$ is one at vertex $i$, zero at all other vertices, and linear on incident triangles.](ipynb.images/hat-function.jpg)\n",
    "\n",
    "Thus gradients of such piecewise linear functions are simply sums of gradients\n",
    "of the hat functions:\n",
    "\n",
    " $\\nabla f(\\mathbf{x}) \\approx\n",
    " \\nabla \\sum\\limits_{i=1}^n \\phi_i(\\mathbf{x})\\, f_i =\n",
    " \\sum\\limits_{i=1}^n \\nabla \\phi_i(\\mathbf{x})\\, f_i.$\n",
    "\n",
    "This reveals that the gradient is a linear function of the vector of $f_i$\n",
    "values. Because the $\\phi_i$ are linear in each triangle, their gradients are\n",
    "_constant_ in each triangle. Thus our discrete gradient operator can be written\n",
    "as a matrix multiplication taking vertex values to triangle values:\n",
    "\n",
    " $\\nabla f \\approx \\mathbf{G}\\,\\mathbf{f},$\n",
    "\n",
    "where $\\mathbf{f}$ is $n\\times 1$ and $\\mathbf{G}$ is an $md\\times n$ sparse\n",
    "matrix. This matrix $\\mathbf{G}$ can be derived geometrically <cite data-cite=\"jacobson2013\">(Jacobson, 2013)</cite>.\n",
    "\n",
    "Libigl's `grad` function computes $\\mathbf{G}$ for\n",
    "triangle and tetrahedral meshes. \n",
    "Let's see how this works. First load a mesh and a corresponding surface function.\n",
    "Next, compute the gradient operator g (#F*3 x #V) on the triangle mesh, apply it to the surface function and extract the magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian\n",
    "\n",
    "And now we're ready for the [Laplacian](https://en.wikipedia.org/wiki/Laplace_operator)!\n",
    "\n",
    "The discrete Laplacian is an essential geometry processing tool. Many\n",
    "interpretations and flavors of the Laplace and Laplace-Beltrami operator exist.\n",
    "\n",
    "In open Euclidean space, the _Laplace_ operator is the usual divergence of\n",
    "gradient (or equivalently the Laplacian of a function is the trace of its\n",
    "Hessian):\n",
    "\n",
    " $\\Delta f =\n",
    " \\frac{\\partial^2 f}{\\partial x^2} +\n",
    " \\frac{\\partial^2 f}{\\partial y^2} +\n",
    " \\frac{\\partial^2 f}{\\partial z^2}.$\n",
    "\n",
    "The _Laplace-Beltrami_ operator generalizes this to surfaces.\n",
    "\n",
    "When considering piecewise-linear functions on a triangle mesh, a discrete\n",
    "Laplacian may be derived in a variety of ways. The most popular in geometry\n",
    "processing is the so-called \"cotangent Laplacian\" $\\mathbf{L}$, arising\n",
    "simultaneously from [FEM](https://en.wikipedia.org/wiki/Finite_element_method), [DEC](https://en.wikipedia.org/wiki/Discrete_exterior_calculus) and applying divergence theorem to vertex\n",
    "one-rings. As a linear operator taking vertex values to vertex values, the\n",
    "Laplacian $\\mathbf{L}$ is a $n\\times n$ matrix with elements:\n",
    "\n",
    "$L_{ij} = \\begin{cases}j \\in N(i) &\\cot \\alpha_{ij} + \\cot \\beta_{ij},\\\\\n",
    "j \\notin N(i) & 0,\\\\\n",
    "i = j & -\\sum\\limits_{k\\neq i} L_{ik},\n",
    "\\end{cases}$\n",
    "\n",
    "where $N(i)$ are the vertices adjacent to (neighboring) vertex $i$, and\n",
    "$\\alpha_{ij},\\beta_{ij}$ are the angles opposite to edge ${ij}$.\n",
    "\n",
    "Libigl implements discrete \"cotangent Laplacians\" for triangles meshes and\n",
    "tetrahedral meshes, building both with fast geometric rules rather than \"by the\n",
    "book\" FEM construction which involves many (small) matrix inversions <cite data-cite=\"sharf_2007\">(Sharf, 2007)</cite>.\n",
    "\n",
    "Here, I load a triangle mesh and then calculate the Laplace-Beltrami operator and visualize the normals as pseudocolors. But I have some bugs and have not been able to finish this on time for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "v, f = ig.read_triangle_mesh('data/cow.off')\n",
    "l = ig.cotmatrix(v, f)\n",
    "\n",
    "n = ig.per_vertex_normals(v, f)*0.5+0.5\n",
    "c = np.linalg.norm(n, axis=1)\n",
    "p = mp.plot(v, f, c, shading={\"wireframe\": False}, return_plot=True)\n",
    "\n",
    "vs = [v]\n",
    "cs = [c]\n",
    "for i in range(10):\n",
    "    m = ig.massmatrix(v, f, ig.MASSMATRIX_TYPE_BARYCENTRIC)\n",
    "    s = (m - 0.001 * l)\n",
    "    b = m.dot(v)\n",
    "    v = spsolve(s, m.dot(v))\n",
    "    n = ig.per_vertex_normals(v, f)*0.5+0.5\n",
    "    c = np.linalg.norm(n, axis=1)\n",
    "    vs.append(v)\n",
    "    cs.append(c)\n",
    "\n",
    "@mp.interact(level=(0, 9))\n",
    "def mcf(level=0):\n",
    "    p.update_object(vertices=vs[level], colors=cs[level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "v, f = ig.read_triangle_mesh('data/cow.off')\n",
    "\n",
    "## Find boundary vertices\n",
    "e = ig.boundary_facets(f)\n",
    "v_b = np.unique(e)\n",
    "\n",
    "## List of all vertex indices\n",
    "v_all = np.arange(v.shape[0])\n",
    "\n",
    "## List of interior indices\n",
    "v_in = np.setdiff1d(v_all, v_b)\n",
    "\n",
    "## Construct and slice up Laplacian\n",
    "l = ig.cotmatrix(v, f)\n",
    "l_ii = l[v_in, :]\n",
    "l_ii = l_ii[:, v_in]\n",
    "\n",
    "l_ib = l[v_in, :]\n",
    "l_ib = l_ib[:, v_b]\n",
    "\n",
    "## Dirichlet boundary conditions from z-coordinate\n",
    "z = v[:, 2]\n",
    "bc = z[v_b]\n",
    "\n",
    "## Solve PDE\n",
    "z_in = spsolve(-l_ii, l_ib.dot(bc))\n",
    "\n",
    "mp.plot(v, f, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, f = ig.read_triangle_mesh('data/cheburashka.off')\n",
    "\n",
    "## Two fixed points: Left hand, left foot should have values 1 and -1\n",
    "b = np.array([4331, 5957])\n",
    "bc = np.array([1., -1.])\n",
    "B = np.zeros((v.shape[0], 1))\n",
    "\n",
    "## Construct Laplacian and mass matrix\n",
    "L = ig.cotmatrix(v, f)\n",
    "M = ig.massmatrix(v, f, ig.MASSMATRIX_TYPE_VORONOI)\n",
    "Minv = sp.sparse.diags(1 / M.diagonal())\n",
    "\n",
    "## Bi-Laplacian\n",
    "Q = L @ (Minv @ L)\n",
    "\n",
    "## Solve with only equality constraints\n",
    "Aeq = sp.sparse.csc_matrix((0, 0))\n",
    "Beq = np.array([])\n",
    "_, z1 = ig.min_quad_with_fixed(Q, B, b, bc, Aeq, Beq, True)\n",
    "\n",
    "## Solve with equality and linear constraints\n",
    "Aeq = sp.sparse.csc_matrix((1, v.shape[0]))\n",
    "Aeq[0,6074] = 1\n",
    "Aeq[0, 6523] = -1\n",
    "Beq = np.array([0.])\n",
    "_, z2 = ig.min_quad_with_fixed(Q, B, b, bc, Aeq, Beq, True)\n",
    "\n",
    "## Normalize colors to same range\n",
    "min_z = min(np.min(z1), np.min(z2))\n",
    "max_z = max(np.max(z1), np.max(z2))\n",
    "z = [(z1 - min_z) / (max_z - min_z), (z2 - min_z) / (max_z - min_z)]\n",
    "\n",
    "## Plot the functions\n",
    "p = mp.plot(v, f, z1, shading={\"wireframe\":False}, return_plot=True)\n",
    "\n",
    "@mp.interact(function=[('z0', 0), ('z1', 1)])\n",
    "def sf(function):\n",
    "    p.update_object(colors=z[function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Decomposition\n",
    "\n",
    "Libigl has rudimentary support for extracting eigen pairs of a generalized\n",
    "eigen value problem:\n",
    "\n",
    " $Ax = \\lambda B x$\n",
    "\n",
    "where $A$ is a sparse symmetric matrix and $B$ is a sparse positive definite\n",
    "matrix. Most commonly in geometry processing, we let $A=L$ the cotangent\n",
    "Laplacian and $B=M$ the per-vertex mass matrix <cite data-cite=\"vallet_2008\">(Vallet, 2008)</cite>.\n",
    "Typically applications will make use of the _low frequency_ eigen modes.\n",
    "Analogous to the Fourier decomposition, a function $f$ on a surface can be\n",
    "represented via its spectral decomposition of the eigen modes of the\n",
    "Laplace-Beltrami:\n",
    "\n",
    " $f = \\sum\\limits_{i=1}^\\infty a_i \\phi_i$\n",
    "\n",
    "where each $\\phi_i$ is an eigen function satisfying: $\\Delta \\phi_i = \\lambda_i\n",
    "\\phi_i$ and $a_i$ are scalar coefficients. For a discrete triangle mesh, a\n",
    "completely analogous decomposition exists, albeit with finite sum:\n",
    "\n",
    " $\\mathbf{f} = \\sum\\limits_{i=1}^n a_i \\phi_i$\n",
    "\n",
    "where now a column vector of values at vertices $\\mathbf{f} \\in \\mathcal{R}^n$\n",
    "specifies a piecewise linear function and $\\phi_i \\in \\mathcal{R}^n$ is an\n",
    "eigen vector satisfying:\n",
    "\n",
    "$\\mathbf{L} \\phi_i = \\lambda_i \\mathbf{M} \\phi_i$.\n",
    "\n",
    "Note that Vallet &amp; Levy <cite data-cite=\"vallet_2008\">(Vallet, 2008)</cite> propose solving a symmetrized\n",
    "_standard_ eigen problem $\\mathbf{M}^{-1/2}\\mathbf{L}\\mathbf{M}^{-1/2} \\phi_i\n",
    "= \\lambda_i \\phi_i$. Libigl implements a generalized eigen problem solver so\n",
    "this unnecessary symmetrization can be avoided.\n",
    "\n",
    "Often the sum above is _truncated_ to the first $k$ eigen vectors. If the low\n",
    "frequency modes are chosen, i.e. those corresponding to small $\\lambda_i$\n",
    "values, then this truncation effectively _regularizes_ $\\mathbf{f}$ to smooth,\n",
    "slowly changing functions over the mesh <cite data-cite=\"hildebrandt_2011\">(Hildebrandt, 2011)</cite>. Modal\n",
    "analysis and model subspaces have been used frequently in real-time deformation\n",
    "<cite data-cite=\"barbic_2012\">(Barbic, 2005)</cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the first k eigen vectors of the discrete Laplace-Beltrami operator are computed and displayed in\n",
    "pseudocolors atop our cow. \n",
    "\n",
    "Low frequency eigenvectors of the discrete Laplace-Beltrami operator vary smoothly and slowly over the model.\n",
    "At first, calculate the Laplace-Betrami operator and solve the generalized Eigenproblem with scipy/arpack. \n",
    "Then, rescale the Eigenvectors and visualize them. We do this on Cheburashka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, f = ig.read_triangle_mesh('data/cheburashka.off')\n",
    "l = -ig.cotmatrix(v, f)\n",
    "m = ig.massmatrix(v, f, ig.MASSMATRIX_TYPE_VORONOI)\n",
    "\n",
    "k = 10\n",
    "d, u = sp.sparse.linalg.eigsh(l, k, m, sigma=0, which=\"LM\")\n",
    "\n",
    "u = (u - np.min(u)) / (np.max(u) - np.min(u))\n",
    "bbd = 0.5 * np.linalg.norm(np.max(v, axis=0) - np.min(v, axis=0))\n",
    "\n",
    "p = mp.plot(v, f, bbd * u[:, 0], shading={\"wireframe\":False, \"flat\": False}, return_plot=True)\n",
    "\n",
    "@mp.interact(ev=[(\"EV %i\"%i, i) for i in range(k)])\n",
    "def sf(ev):\n",
    "    p.update_object(colors=u[:, ev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These demos are meant to show you that eigenvectors can actually be *viewed*.\n",
    "\n",
    "# Advanced Linear Algebra: Laplacian matrix of a graph\n",
    "\n",
    "Remember what an **adjacency matrix** ise? If you number the nodes of the graph from $0$\n",
    "to $n-1$, and place a 1 in row $i$, column $j$ of the matrix whenever there is\n",
    "an edge from node $i$ to node $j$, that's your graph's adjacency matrix. In other words, if we call the adjacency\n",
    "matrix $A$, then $A_{i, j} = 1$ if and only if the edge $(i, j)$ is in $G$.\n",
    "\n",
    "The *degree* of a node is the number of edges touching it. For\n",
    "example, if a node is connected to five other nodes in a graph, its degree\n",
    "is 5. In adjacency matrix terms, the degree corresponds to the *sum*\n",
    "of the values in a row or column. If the graph is not directed, the adjacency matrix is symmetrix (its transpose is the matrix itself). If the graph is directed, then you have **in-degrees** and **out-degrees**.\n",
    "\n",
    "Now, for some advanced linear algebra. This is the kind of math you study to build the most advanced Deep Learning models.\n",
    "\n",
    "The [**Laplacian**](https://en.wikipedia.org/wiki/Laplacian_matrix) matrix of a graph (insteadof a differentiable manifold) is more siply defined as the **degree matrix**, $D$, which contains the degree of each node along the diagonal and zero everywhere else, minus the adjacency matrix $A$:\n",
    "\n",
    "$\n",
    "L = D - A\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fiedler vector\n",
    "\n",
    "Let's start with the **blueprint** for a graph. How do you draw nodes and edges in such a way that you don't get a complete\n",
    "mess? If you try plotting it with `networkx`, guaranteed you get a complete mess every time you redraw.\n",
    "\n",
    "One way is to ***put nodes that share many edges close together***. And that is essentially what the paper that we are going to study does.\n",
    "\n",
    "It turns out\n",
    "that this can be done by using the *second-smallest eigenvalue* of the Laplacian\n",
    "matrix, and its corresponding eigenvector, which is so important it has its\n",
    "own name: the [Fiedler vector](https://en.wikipedia.org/wiki/Algebraic_connectivity#The_Fiedler_vector). You see, the dominant eigenvector is not the only important eigenvector of a matrix. \n",
    "\n",
    "The magnitude of the eigenvalue of the Fielder eigenvector ***reflects how well connected overall the graph is***.\n",
    "\n",
    "Specifically:\n",
    "* This eigenvalue is greater than 0 if and only if G is a [connected graph](https://en.wikipedia.org/wiki/Connectivity_(graph_theory). This is a corollary to the fact that ***the number of times 0 appears as an eigenvalue in the Laplacian is the number of connected components in the graph***\n",
    "* The magnitude of this value reflects how well connected the overall graph is. \n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/lemur.png\" width=200 />\n",
    "    Really?\n",
    "</center>\n",
    "\n",
    "Let's use a minimal network to illustrate this. We start by creating an adjacency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "A = np.array([[0, 1, 1, 0, 0, 0],\n",
    "              [1, 0, 1, 0, 0, 0],\n",
    "              [1, 1, 0, 1, 0, 0],\n",
    "              [0, 0, 1, 0, 1, 1],\n",
    "              [0, 0, 0, 1, 0, 1],\n",
    "              [0, 0, 0, 1, 1, 0]], dtype=float)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please output it here below in 0s and 1s with no space in between (hint: use code from last lecture):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:none;\">\n",
    "for i in range(6):\n",
    "    row = []\n",
    "    for el in A[i]:\n",
    "        row.append(int(round(el)))\n",
    "    deg = np.sum(row)\n",
    "    print(''.join(str(i) for i in row).replace(' ', '').replace('\\n', '') + ' ' + str(deg))\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `networkx` to draw this network. First, we initialize matplotlib as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots appear inline, set custom plotting style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('style/elegant.mplstyle')\n",
    "plt.figure(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot it with `nx.spring_layout`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g = nx.from_numpy_matrix(A)\n",
    "layout = nx.spring_layout(g, pos=nx.circular_layout(g))\n",
    "nx.draw(g, pos=layout,\n",
    "        with_labels=True, node_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the nodes fall naturally into two groups: (0, 1, 2) and (3, 4, 5).\n",
    "Can the Fiedler vector tell us this? First, we must compute the **degree matrix**\n",
    "and the **Laplacian**. \n",
    "\n",
    "We first get the degrees by summing along either axis of $A$.\n",
    "(Either axis works because $A$ is symmetric.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.sum(A, axis=0)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put those degrees into a diagonal matrix of the same shape\n",
    "as A, the *degree matrix*. We can use the `np.diag` function to do this.\n",
    "\n",
    "We did this exercise in our ecology graph, remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.diag(d)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get the Laplacian from the definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = D - A\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $L$ is symmetric, we can use the `np.linalg.eigh` function (returns the eigenvalues and eigenvectors of a Hermitian or symmetric matrix, see [here](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.eigh.html)) to compute\n",
    "the eigenvalues and eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, Vec = np.linalg.eigh(L)\n",
    "val, Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the values returned satisfy the definition of eigenvalues\n",
    "and eigenvectors. For example, one of the eigenvalues is 3.\n",
    "\n",
    "- Marvel at this ***neat*** pythonic expression for verifying that a list contains a value very close to something we're after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isn't this a neat pythonic expression for verifying that a list contains a value very close to something we know?\n",
    "np.any(np.isclose(val, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check that multiplying the matrix $L$ by the corresponding eigenvector\n",
    "does indeed multiply the vector by 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_lambda3 = np.argmin(np.abs(val - 3))\n",
    "print(idx_lambda3)\n",
    "v3 = Vec[:, idx_lambda3]\n",
    "\n",
    "print(v3)\n",
    "print(L @ v3)\n",
    "print(3 * v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_lambda3)\n",
    "v3 = Vec[:, idx_lambda3]\n",
    "print(v3)\n",
    "print(Vec[0, idx_lambda3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great.\n",
    "\n",
    "As mentioned above, the **Fiedler vector** is the vector corresponding to the\n",
    "second-smallest eigenvalue of $L$. Sorting the eigenvalues tells us which one\n",
    "is the *second-smallest*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2 = np.sort(val)\n",
    "print(val2)\n",
    "plt.plot(val2, linestyle='-', marker='o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- caption text=\"Eigenvalues of $L$\" -->\n",
    "\n",
    "It's the first non-zero eigenvalue, close to 0.4. The Fiedler vector is the\n",
    "corresponding eigenvector. These are its components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Vec[:, np.argsort(val)[1]]\n",
    "print(f)\n",
    "plt.plot(f, linestyle='-', marker='o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- caption text=\"Fiedler vector of $L$\" -->\n",
    "\n",
    "Just by looking at the *sign* of the elements of the Fiedler\n",
    "vector, we can separate the nodes into the two groups we identified in the\n",
    "drawing. \n",
    "\n",
    "That's pretty remarkable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['orange' if eigv > 0 else 'gray' for eigv in f]\n",
    "nx.draw(g, pos=layout, with_labels=True, node_color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/lemur.png\" width=200 />\n",
    "    (speechless)\n",
    "</center>\n",
    "\n",
    "Right?!\n",
    "\n",
    "Now, ready for a *real* network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- caption text=\"Nodes colored by their sign in the Fiedler vector of $L$\" -->\n",
    "\n",
    "# Laplacians with brain data\n",
    "\n",
    "Armed with this newly acquired knowledge, and since we're getting close to building artificial neural networks with `scikit-learn`, we need to learn a bit more about how neurons are ***configured***. \n",
    "\n",
    "Let's demonstrate this process in a real-world example by laying out a worm's brain cells (one of the simplest brains in our animal kingdom), as shown in Figure 2\n",
    "from the\n",
    "[Varshney *et al* paper](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066)\n",
    "(information on\n",
    "how to do this is in the\n",
    "[supplementary material](http://journals.plos.org/ploscompbiol/article/asset?unique&id=info:doi/10.1371/journal.pcbi.1001066.s001)\n",
    "for the paper).\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/worm.png\" width=300 />\n",
    "</center>\n",
    "\n",
    "\n",
    "To obtain Varshney's layout of the worm brain neurons, they used a related matrix, the\n",
    "**degree-normalized Laplacian**.\n",
    "\n",
    "Because the order of the neurons is important in this analysis, we will use a\n",
    "preprocessed dataset, rather than do complex data cleansing. The original data is on Varshney's\n",
    "[website](http://www.ifp.illinois.edu/~varshney/elegans),\n",
    "and the processed data is on blackboard.\n",
    "\n",
    "First, load the data. There are four components:\n",
    "- The network of chemical synapses, through which a *pre-synaptic neuron*\n",
    "  sends a **chemical** signal to a *post-synaptic* neuron (chem-network.npy),\n",
    "- The gap junction network, which contains direct **electrical** contacts between\n",
    "  neurons) (gap-network.npy),\n",
    "- The neuron IDs (names) (neurons.npy), and\n",
    "- The three neuron types (neuron-types.npy):\n",
    "  - *sensory neurons*, those that detect signals coming from the outside world,\n",
    "    encoded as 0;\n",
    "  - *motor neurons*, those that activate muscles, enabling the worm to move,\n",
    "    encoded as 2; and\n",
    "  - *interneurons*, the neurons in between, which enable complex signal processing\n",
    "    to occur between sensory neurons and motor neurons, encoded as 1.\n",
    "    \n",
    "These are the most important figures in the paper. Here is Figure 1, scatter pairs plots:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/worm-fig1.png\" width=900 />\n",
    "    Figure 1. Adjacency matrices for the gap junction network (blue circles) and the chemical synapse network (red points) with neurons grouped by category (sensory neurons, interneurons, motor neurons)\n",
    "</center>\n",
    "\n",
    "<br />\n",
    "And here are the two views of Figure 2:\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/worm-fig2.png\" width=900 />\n",
    "    Figure 2. The C. elegans wiring diagram is a network of identifiable, labeled neurons connected by chemical and electrical synapses\n",
    "</center>\n",
    "\n",
    "<br />\n",
    "\n",
    "Here's a [neuron](https://en.wikipedia.org/wiki/Neuron). Chemical transmission happens in the synapses. Electrical transmission happens in the axon. For worms and humans alike!\n",
    "    \n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/neuron.png\" width=500 />\n",
    "</center>\n",
    "\n",
    "<br />\n",
    "Let's load the data.\n",
    "\n",
    "A `.npy` file is the `NumPy` **array file** we already know about. We know that we can load the array in a `.npy` file by using \n",
    "```python\n",
    "np.load('filename.npy')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Chem = np.load('data/chem-network.npy')\n",
    "Gap = np.load('data/gap-network.npy')\n",
    "neuron_ids = np.load('data/neurons.npy')\n",
    "neuron_types = np.load('data/neuron-types.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the network, adding the two kinds of connections together,\n",
    "and removing the directionality of the network by taking the average of\n",
    "in-connections and out-connections of neurons (adding by the transpose of the adjacency matrix, and then dividing by 2 achieves this):\n",
    "```python\n",
    "(A + A.T) / 2\n",
    "```\n",
    "\n",
    "Why? Since we are only looking for the *layout* of the neurons on a graph, we\n",
    "only care about *whether* neurons are connected, not in which direction.\n",
    "\n",
    "We are going to call the resulting matrix the **connectivity matrix**, $C$, which\n",
    "is just a different kind of **adjacency matrix**. Note that the adjacency matrix A is **weighted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Chem + Gap\n",
    "C = (A + A.T) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the Laplacian matrix $L$, we need the degree matrix $D$, which contains\n",
    "the degree of node i at position [i, i], and zeros everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.sum(C, axis=0)\n",
    "D = np.diag(degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get the Laplacian just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = D - C\n",
    "np.shape(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some **exploratory data analysis** (EDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def plot_coo_matrix(m):\n",
    "    if not isinstance(m, coo_matrix):\n",
    "        m = coo_matrix(m)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    #fig = plt.figure()\n",
    "    #fig = plt.figure(figsize=(2,1))\n",
    "    # you may have to replace 'axisbg' below with 'facecolor', depending on the version of your matplotlib\n",
    "    ax = fig.add_subplot(111, facecolor='black') #axisbg='b')\n",
    "    ax.plot(m.col, m.row, 's', color='white', ms=1)\n",
    "    ax.set_xlim(0, m.shape[1])\n",
    "    ax.set_ylim(0, m.shape[0])\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_coo_matrix(C)\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what are some of the conclusions you can make by looking at this brain? How about this:\n",
    "\n",
    "* Neurons appear to like to connect to nearby neurons (dense diagonal)\n",
    "* But when a neuron connects to far-away neurons, it connects *a ton* to all other neurons (horizontal and vertical lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some virtual surgery and take some brain slices.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/virtual-brain-surgery.jpg\" width=500 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.util.shape import view_as_blocks\n",
    "B = view_as_blocks(C, block_shape=(31, 31))  #why 31? Ask the authors..\n",
    "\n",
    "B00 = B[0, 0] #the top-left 31x31 block\n",
    "ax = plot_coo_matrix(B00)\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from last lecture\n",
    "for i in range(31):\n",
    "    row = []\n",
    "    for el in B00[i]:\n",
    "        row.append(int(round(el)))\n",
    "    deg = np.sum(row)\n",
    "    print(''.join(str(i) for i in row).replace(' ', '').replace('\\n', '') + ' ' + str(deg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rearrange nodes such that, on average, neurons are as close as possible to \"just above\" their downstream (connected)\n",
    "neighbors. Varshney _et al_ call this measure **processing depth**, and it's\n",
    "obtained by solving a linear equation involving the Laplacian. We use\n",
    "`scipy.linalg.pinv`, the\n",
    "[pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse),\n",
    "to solve it. \n",
    "\n",
    "That is because when the dimensions get high (we have 279 here), there is a ***high probability of matrix degeneracy***. Calculating a pseudo-inverse is safer and faster!\n",
    "\n",
    "If we denote by $A^+$ the pseudo-inverse (versus $A^{-1}$ for the real inverse), $AA^+$ need not be the general identity matrix, but it maps all column vectors of A to themselves: \n",
    "\n",
    "$$AA^+ = A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the pseudo-inverse `linalg.pinv` to compute a\n",
    "vector $z$ that satisfies $L z = b$,\n",
    "where $b = C \\odot \\textrm{sign}\\left(A - A^T\\right) \\mathbf{1}$. \n",
    "\n",
    "- Why, is in the supplementary material for Varshney *et al*.\n",
    "\n",
    "So, $z = L^+b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "b = np.sum(C * np.sign(A - A.T), axis=1) \n",
    "z = linalg.pinv(L) @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the degree-normalized Laplacian, $Q$, we need the inverse\n",
    "square root of the matrix $D$ formed by putting the degree for each node on its diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dinv2 = np.diag(1 / np.sqrt(degrees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are able to extract the $x$ coordinates (so-called **affinity coordinates**) of the neurons to ensure that\n",
    "highly-connected neurons remain close: the eigenvector of $Q$ corresponding to\n",
    "its second-smallest eigenvalue (**Fiedler vector**), normalized by the degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Dinv2 @ L @ Dinv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, Vec = linalg.eig(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from the documentation of `numpy.linalg.eig`:\n",
    "\n",
    "> \"The eigenvalues are not necessarily ordered.\"\n",
    "\n",
    "We must therefore sort the eigenvalues and the corresponding eigenvector columns ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallest_first = np.argsort(val)\n",
    "val = val[smallest_first]\n",
    "Vec = Vec[:, smallest_first]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we found the Fielder eigenvector, we compute the **affinity coordinates**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dinv2 @ Vec[:, 1]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reasons for using this vector are complex, but appear in\n",
    "the paper's supplementary material. The important result is that\n",
    "***choosing this vector minimizes the total length of the links between neurons, and yields pretty graphs that are easier to analyze***.\n",
    "\n",
    "There is one small kink that the authors address before proceeding: eigenvectors\n",
    "are only defined up to a multiplicative constant. This follows simply from the\n",
    "definition of an eigenvector: suppose $v$ is an eigenvector of the matrix $M$,\n",
    "with corresponding eigenvalue $\\lambda$. Then $\\alpha v$ is also an eigenvector\n",
    "of $M$ for any scalar number $\\alpha$,\n",
    "because $Mv = \\lambda v$ implies $M(\\alpha v) = \\lambda (\\alpha v)$.\n",
    "\n",
    "So, it is\n",
    "arbitrary whether a software package returns $v$ or $-v$ when asked for the\n",
    "eigenvectors of $M$. In order to make sure we reproduce the layout from the\n",
    "Varshney *et al.* paper, we must make sure that the vector is pointing in the\n",
    "same direction as theirs, rather than the opposite direction. \n",
    "\n",
    "So we choose an arbitrary neuron from their Figure 2, and check the sign of `x`\n",
    "at that position. Reverse the vector it if it doesn't match its sign in Figure 2\n",
    "of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc2_index = np.argwhere(neuron_ids == 'VC02')\n",
    "if x[vc2_index] < 0:\n",
    "    x = -x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's just a matter of drawing the nodes and the edges. We color them\n",
    "according to the type stored in `neuron_types`, using the appealing and\n",
    "functional \"colorblind\"\n",
    "[colorbrewer palette](http://chrisalbon.com/python/seaborn_color_palettes.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "\n",
    "def plot_connectome(x_coords, y_coords, conn_matrix, *,\n",
    "                    labels=(), types=None, type_names=('',),\n",
    "                    xlabel='', ylabel=''):\n",
    "    \"\"\"Plot neurons as points connected by lines.\n",
    "\n",
    "    Neurons can have different types (up to 6 distinct colors).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_coords, y_coords : array of float, shape (N,)\n",
    "        The x-coordinates and y-coordinates of the neurons.\n",
    "    conn_matrix : array or sparse matrix of float, shape (N, N)\n",
    "        The connectivity matrix, with non-zero entry (i, j) if and only\n",
    "        if node i and node j are connected.\n",
    "    labels : array-like of string, shape (N,), optional\n",
    "        The names of the nodes.\n",
    "    types : array of int, shape (N,), optional\n",
    "        The type (e.g. sensory neuron, interneuron) of each node.\n",
    "    type_names : array-like of string, optional\n",
    "        The name of each value of `types`. For example, if a 0 in\n",
    "        `types` means \"sensory neuron\", then `type_names[0]` should\n",
    "        be \"sensory neuron\".\n",
    "    xlabel, ylabel : str, optional\n",
    "        Labels for the axes.\n",
    "    \"\"\"\n",
    "    if types is None:\n",
    "        types = np.zeros(x_coords.shape, dtype=int)\n",
    "    ntypes = len(np.unique(types))\n",
    "    colors = plt.rcParams['axes.prop_cycle'][:ntypes].by_key()['color']\n",
    "    cmap = ListedColormap(colors)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plot neuron locations:\n",
    "    for neuron_type in range(ntypes):\n",
    "        plotting = (types == neuron_type)\n",
    "        pts = ax.scatter(x_coords[plotting], y_coords[plotting],\n",
    "                         c=cmap(neuron_type), s=4, zorder=1)\n",
    "        pts.set_label(type_names[neuron_type])\n",
    "\n",
    "    # add text labels:\n",
    "    for x, y, label in zip(x_coords, y_coords, labels):\n",
    "        ax.text(x, y, '   ' + label,\n",
    "                verticalalignment='center', fontsize=3, zorder=2)\n",
    "\n",
    "    # plot edges\n",
    "    pre, post = np.nonzero(conn_matrix)\n",
    "    links = np.array([[x_coords[pre], x_coords[post]],\n",
    "                      [y_coords[pre], y_coords[post]]]).T\n",
    "    ax.add_collection(LineCollection(links, color='lightgray',\n",
    "                                     lw=0.3, alpha=0.5, zorder=0))\n",
    "\n",
    "    ax.legend(scatterpoints=3, fontsize=6)\n",
    "\n",
    "    ax.set_xlabel(xlabel, fontsize=8)\n",
    "    ax.set_ylabel(ylabel, fontsize=8)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use that function to plot the neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral layout of the neurons of a nematode worm\n",
    "plot_connectome(x, z, C, labels=neuron_ids, types=neuron_types,\n",
    "                type_names=['sensory neurons', 'interneurons',\n",
    "                            'motor neurons'],\n",
    "                xlabel='Affinity eigenvector 1', ylabel='Processing depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a worm brain!\n",
    "\n",
    "As discussed in Varshney's original paper, you can see the top-down processing from\n",
    "sensory neurons to motor neurons through a network of interneurons. You can\n",
    "also see two distinct groups of motor neurons: these correspond to the neck\n",
    "(left) and body (right) body segments of the worm.\n",
    "\n",
    "How to modify the above code to show the affinity view in Figure 2B from the paper?\n",
    "\n",
    "In the affinity view, instead of using the processing depth on the y-axis,\n",
    "we use the normalized third eigenvector of Q, just like we did with x (\n",
    "invert if necessary, just like with x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Dinv2 @ Vec[:, 2]\n",
    "asjl_index = np.argwhere(neuron_ids == 'ASJL')\n",
    "if y[asjl_index] < 0:\n",
    "    y = -y\n",
    "\n",
    "# Spectral layout of the neurons of a nematode worm, using two spectral dimensions\n",
    "plot_connectome(x, y, C, labels=neuron_ids, types=neuron_types,\n",
    "                type_names=['sensory neurons', 'interneurons',\n",
    "                            'motor neurons'],\n",
    "                xlabel='Affinity eigenvector 1',\n",
    "                ylabel='Affinity eigenvector 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sparse matrix calculus\n",
    "\n",
    "Let's repeat these operations with sparse matrix calculus.\n",
    "\n",
    "First, we start with the adjacency matrix, A, in a sparse matrix format, in\n",
    "this case, **CSR** (compressed sparse row), which is the most common format for linear algebra. \n",
    "\n",
    "We'll append `s` to the names of all the matrices to indicate that they are **sparse**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "As = sparse.csr_matrix(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our connectivity matrix in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = (As + As.T) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the degrees matrix, we use the \"diags\" sparse format, which\n",
    "stores diagonal and off-diagonal matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.ravel(Cs.sum(axis=0))\n",
    "Ds = sparse.diags(degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Laplacian is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = Ds - Cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the processing depth. Remember that getting the\n",
    "pseudo-inverse of the Laplacian matrix for large connectomes is out of the question, because it will\n",
    "be a dense matrix (the inverse of a sparse matrix is not generally sparse\n",
    "itself). \n",
    "\n",
    "For the nematode worm, we used the pseudo-inverse to compute a\n",
    "vector $z$ that satisfies $L z = b$,\n",
    "where $b = C \\odot \\textrm{sign}\\left(A - A^T\\right) \\mathbf{1}$.\n",
    "\n",
    "With\n",
    "dense matrices, we can simply use $z = L^+b$. With sparse ones, we can\n",
    "use one of the *solvers* (see sidebox, \"Solvers\") in `sparse.linalg.isolve` to get the `z` vector after\n",
    "providing `L` and `b`, with no inversion required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Cs.multiply((As - As.T).sign()).sum(axis=1)\n",
    "z, error = sparse.linalg.isolve.cg(Ls, b, maxiter=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we must find the eigenvectors of $Q$, the degree-normalized Laplacian,\n",
    "corresponding to its second and third smallest eigenvalues.\n",
    "\n",
    "Numerical data in sparse matrices is\n",
    "in the `.data` attribute. We use that to invert the degrees matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dsinv2 = Ds.copy()\n",
    "Dsinv2.data = 1 / np.sqrt(Ds.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `scipy`'s sparse linear algebra functions to find the desired\n",
    "eigenvectors. \n",
    "\n",
    "The $Q$ matrix is symmetric, so we can use the `eigsh` function,\n",
    "specialized for symmetric matrices, to compute them. \n",
    "\n",
    "We use the `which` keyword\n",
    "argument to specify that we want the eigenvectors corresponding to the smallest\n",
    "eigenvalues, and `k` to specify that we need the 3 smallest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs = Dsinv2 @ Ls @ Dsinv2\n",
    "vals, Vecs = sparse.linalg.eigsh(Qs, k=3, which='SM')\n",
    "sorted_indices = np.argsort(vals)\n",
    "Vecs = Vecs[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we normalize the eigenvectors to get the x and y coordinates\n",
    "(and flip these if necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dsinv, x, y = (Dsinv2 @ Vecs).T\n",
    "if x[vc2_index] < 0:\n",
    "    x = -x\n",
    "if y[asjl_index] < 0:\n",
    "    y = -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the eigenvector corresponding to the smallest eigenvalue is always a\n",
    "vector of all ones. Why? Study [this](http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian/) paper!\n",
    "\n",
    "We can now reproduce the above plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral layout of a nematode brain, computed using sparse matrices\n",
    "plot_connectome(x, z, C, labels=neuron_ids, types=neuron_types,\n",
    "                type_names=['sensory neurons', 'interneurons',\n",
    "                            'motor neurons'],\n",
    "                xlabel='Affinity eigenvector 1', ylabel='Processing depth')\n",
    "\n",
    "plot_connectome(x, y, C, labels=neuron_ids, types=neuron_types,\n",
    "                type_names=['sensory neurons', 'interneurons',\n",
    "                            'motor neurons'],\n",
    "                xlabel='Affinity eigenvector 1',\n",
    "                ylabel='Affinity eigenvector 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy has several sparse iterative solvers available, and it is not always\n",
    "obvious which to use.  Unfortunately, that question also has no easy answer:\n",
    "different algorithms have different strengths in terms of speed of\n",
    "convergence, stability, accuracy, and memory use (amongst others).  It is also\n",
    "not possible to predict, by looking at the input data, which algorithm will\n",
    "perform best.\n",
    "\n",
    "Here is a rough guideline for choosing an iterative solver:\n",
    "\n",
    "> - If A, the input matrix, is symmetric and positive definite, use the\n",
    ">   Conjugate Gradient solver `cg`.  If A is symmetric, but\n",
    ">   near-singular or indefinite, try the Minimum Residual iteration\n",
    ">   method `minres`.\n",
    "> \n",
    "> - For non-symmetric systems, try the Biconjugate Gradient Stabilized\n",
    ">   method, `bicgstab`.  The Conjugate Gradient Squared method, `cgs`,\n",
    ">   is a bit faster, but has more erratic convergence.\n",
    "> \n",
    "> - If you need to solve many similar systems, use the LGMRES algorithm `lgmres`.\n",
    "> \n",
    "> - If A is not square, use the least squares algorithm `lsmr`.\n",
    "> \n",
    "\n",
    "For further reading, see:\n",
    " \n",
    "**How Fast are Nonsymmetric Matrix Iterations?**,\n",
    "Noël M. Nachtigal, Satish C. Reddy, and Lloyd N. Trefethen\n",
    "SIAM Journal on Matrix Analysis and Applications 1992 13:3, 778-795. https://pdfs.semanticscholar.org/899b/d31c4b9a29b593c31ccbcd4497684eeea864.pdf\n",
    " \n",
    "**Survey of recent Krylov methods**, Jack Dongarra,\n",
    "http://www.netlib.org/linalg/html_templates/node50.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word about [Krylov](https://en.wikipedia.org/wiki/Krylov_subspace) methods\n",
    "\n",
    "An intuitive method for finding an eigenvalue (specifically the largest eigenvalue) of a given m × m matrix $A$ is the power iteration that we used last lecture. \n",
    "\n",
    "Starting with a **random initial vector** $b$, this method calculates $Ab, \\; A^2b, \\; A^3b$,… iteratively storing and normalizing the result into $b$ on every turn. This sequence **converges** to the eigenvector corresponding to the largest eigenvalue,$ \\lambda _{{1}}$: the **dominant eigenvector**. However, much potentially useful computation is wasted by using only the final result, $A^{n-1}b, \\;  A^{{n-1}}b, \\; \\cdots$. \n",
    "\n",
    "If instead, we form the so-called Krylov matrix:\n",
    "\n",
    "$$K_{n}= \\begin{bmatrix}b&Ab&A^{2}b&\\cdots &A^{n-1}b\\end{bmatrix}$$\n",
    "\n",
    "The columns of this matrix are not orthogonal, but in principle, we can extract an orthogonal basis, via a method such as [Gram–Schmidt orthogonalization](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process). \n",
    "\n",
    "The resulting set of vectors is an orthogonal basis of the Krylov subspace, $K_{n}$. We may expect the vectors of this basis to give good approximations of the eigenvectors corresponding to the $n$ largest eigenvalues, for the same reason that $A^{{n-1}}b$ approximates the dominant eigenvector.\n",
    "\n",
    "The process described above is intuitive. Unfortunately, it is also unstable. This is where iterations such as the [Arnoldi iteration](https://en.wikipedia.org/wiki/Arnoldi_iteration) is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/fun.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "I had an idea to underscore the utility of eigenvectors, introduce graph and surface visualizations, and showcase the [graph Laplacian](https://samidavies.wordpress.com/2016/09/20/whats-up-with-the-graph-laplacian/) because it is very much in vogue in deep learning today, and a good application of linear algebra.\n",
    "\n",
    "Please read [this](https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf) concise introduction for an idea, and [this](http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian/) blog for the restricted results we studied in this notebook. \n",
    "\n",
    "How valuable are these methods? Look [here](https://www.ibm.com/us-en/marketplace/analysts-notebook).\n",
    "\n",
    "I also wanted to show you how sparse matrix calculus is done, and visualization tools for `scikit-learn` and artifical neural networks.\n",
    "\n",
    "Also, to underscore that solutions to data science problems are not software packages. Instead, it is the math that solves the problem. And then you look for software packages implementing the related math. Hint: most of them are in `scipy`. And the probabilistic ones in `scikit-learn` or `pymc3`.\n",
    "\n",
    "Now you know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
