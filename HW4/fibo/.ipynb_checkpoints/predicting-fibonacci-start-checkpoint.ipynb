{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Sci Engineering Methods and Tools, Lecture 4</div>\n",
    "<div style=\"text-align: right\">Prof. Dino Konstantopoulos, 27 January 2020</div>\n",
    "\n",
    "# Math and the Scientific Method\n",
    "\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/john-stuart-mill.jpg\" width=400 />\n",
    "    John Stuart Mill (1806 - 1873)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fibonacci numbers\n",
    "\n",
    "What's the *interesting* feature of Fibonacci numbers? The *future* ***is*** encoded in the *past*!\n",
    "\n",
    "Please complete the fibonacci generator below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"visibility: hidden\">\n",
    "    def fib(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b\n",
    "\n",
    "my_fibs = list(fib(80))\n",
    "print(my_fibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.Series(my_fibs)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(my_fibs, columns = ['Fibonacci'])\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(data2.values)\n",
    "plt.title('Fibonacci numbers up to fib(80)')\n",
    "plt.ylabel('Fib')\n",
    "plt.xlabel('Integers')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's *normalize*: Computers *hate* big numbers, but they love *very small* numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "data3 = sc.fit_transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(data3)\n",
    "plt.title('Fibonacci numbers up to fib(80), normalized')\n",
    "plt.ylabel('Fib')\n",
    "plt.xlabel('Integers')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is normalized, let's return it to a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.DataFrame(data3, columns=data2.columns, index=data2.index)\n",
    "data4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a random training/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(my_fibs, columns = ['Fibonacci'])\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ***only one*** of the following two cells:\n",
    "\n",
    "Run this cell to work with potentially ***huge*** numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to work with ***small*** numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to convert to a pandas **dataframe** if you picked the 2nd option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = pd.DataFrame(data5, columns=['Fibonacci'], index=data2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only need the previous ***two*** values to evaluate the next value, but just for kicks, to see if the model correctly picks the right columns, i'll also add the the previous ***three*** values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s in range(1,10):\n",
    "for s in range(1,4):\n",
    "    data5['Fibonacci_{}'.format(s)] = data5['Fibonacci'].shift(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5['Fibonacci'] - data5['Fibonacci_1'] - data5['Fibonacci_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! What's that *last* row saying??!\n",
    "\n",
    "Let's get rid of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = data5[:-1]\n",
    "data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5['Fibonacci'] - data5['Fibonacci_1'] - data5['Fibonacci_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data5.dropna().drop('Fibonacci', axis=1)\n",
    "y = data5.dropna().drop(['Fibonacci_'+str(i) for i in range(1,4)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model \n",
    "rf_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 measure (indicator of accuracy 1 is perfect, 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretty good!\n",
    "\n",
    "Which columns did the algorithm pick to determine the target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(X.columns, rf_model.feature_importances_),\n",
    "        key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm correctly determined that the previous and next-to-previous columns are the right ones to prioritize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred, columns=['Fibonacci'], index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test.values)\n",
    "plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML is Function Approximation Theory\n",
    "\n",
    "Sometimes **linear**, sometimes **non-linear**, depending on the algorithm!\n",
    "\n",
    "Let's prove this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "x = np.random.uniform(low=0.5, high=20, size=(1000,))\n",
    "y = np.random.uniform(low=0.5, high=20, size=(1000,))\n",
    "df = pd.DataFrame({'x':x, 'y':y})\n",
    "df.plot('x', 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['z'] = 5.* df['x'] + 0.2 * df['y']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "threedee = plt.figure().gca(projection='3d')\n",
    "threedee.scatter(df['x'], df['y'], df['z'])\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.dropna().drop('z', axis=1)\n",
    "y = df.dropna().drop(['x', 'y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a model \n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get R2 measure (indicator of accuracy 1 is perfect, 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv = pd.DataFrame({'y_test':np.squeeze(y_test).values, 'y_pred':y_pred})\n",
    "dfv.plot('y_test', 'y_pred', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convinced?\n",
    "\n",
    "Now let's try something (mildly) non-linear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['z'] = 5.* df['x']**2 + 0.2 * df['y']**3 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d')\n",
    "threedee.scatter(df['x'], df['y'], df['z'])\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.dropna().drop('z', axis=1)\n",
    "y = df.dropna().drop(['x', 'y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a model \n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get R2 measure (indicator of accuracy 1 is perfect, 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "dfv = pd.DataFrame({'y_test':np.squeeze(y_test).values, 'y_pred':y_pred})\n",
    "dfv.plot('y_test', 'y_pred', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try something *highly* non-linear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.linspace(-10,10,1000)\n",
    "#y = np.linspace(-10,10,1000)\n",
    "x = np.random.uniform(low=-10, high=10, size=(1000,))\n",
    "y = np.random.uniform(low=-10, high=10, size=(1000,))\n",
    "df = pd.DataFrame({'x':x, 'y':y})\n",
    "df.plot('x', 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['z'] = 5.* df['x']**2 + 0.2 * df['y']**3 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d')\n",
    "threedee.scatter(df['x'], df['y'], df['z'])\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.dropna().drop('z', axis=1)\n",
    "y = df.dropna().drop(['x', 'y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a model \n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get R2 measure (indicator of accuracy 1 is perfect, 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "dfv = pd.DataFrame({'y_test':np.squeeze(y_test).values, 'y_pred':y_pred})\n",
    "dfv.plot('y_test', 'y_pred', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still good!\n",
    "\n",
    "So regression forests (decision trees) pick up on **non-linear** relationships, too!\n",
    "\n",
    "Or do they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A different split\n",
    "\n",
    "What if we do not use `sklearn`'s train/test split, and instead use our own to predict **intervals** instead of isolated datapoints (e.g. the *future* from the *past*)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x = np.random.uniform(low=-10, high=10, size=(1000,))\n",
    "y = np.random.uniform(low=-10, high=10, size=(1000,))\n",
    "df = pd.DataFrame({'x':x, 'y':y})\n",
    "df.plot('x', 'y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['z'] = (5.* df['x']**2 + 0.2 * df['y']**3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "threedee = plt.figure().gca(projection='3d')\n",
    "threedee.scatter(df['x'], df['y'], df['z'])\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.dropna().drop('z', axis=1)\n",
    "y = df.dropna().drop(['x', 'y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the interval \\[0, 800\\] as the *past* (to train with), and \\[800, 1000\\] as the *future* (to predict or test the training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:800]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.drop(X.index[800:])\n",
    "X_test = X.drop(X.index[0:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a model \n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get R2 measure (indicator of accuracy 1 is perfect, 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "dfv = pd.DataFrame({'y_test':np.squeeze(y_test).values, 'y_pred':y_pred})\n",
    "dfv.plot('y_test', 'y_pred', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yahhhh! Still great prediction! So it's not like we're predicting individual datapoints because of smoothness and good linear approximations. We are predicting entire intervals (\\[800, \\1000\\])!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More wildly non-linear\n",
    "\n",
    "Really, can ML algorithms pick up *all* non-linearities? How about we use the same non-linear `z = f(x,y)` as right above, except let's *wiggle* the hell out of it with trigonometric functions!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/shar-pei.jpg\" width=400 />\n",
    "Shar-Pei breed\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['z'] = (5.* df['x']**2 + 0.2 * df['y']**3) * np.sin(df['x']) * np.cos(df['y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "threedee = plt.figure().gca(projection='3d')\n",
    "threedee.scatter(df['x'], df['y'], df['z'])\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.dropna().drop('z', axis=1)\n",
    "y = df.dropna().drop(['x', 'y'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try *both* types of splits! Run one ***or the other*** cell below, *not both*! Then come back, and run the other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:800]\n",
    "y_test = y[800:]\n",
    "X_train = X.drop(X.index[800:])\n",
    "X_test = X.drop(X.index[0:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a model \n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get R2 measure (indicator of accuracy 1 is perfect, 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "dfv = pd.DataFrame({'y_test':np.squeeze(y_test).values, 'y_pred':y_pred})\n",
    "dfv.plot('y_test', 'y_pred', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohhh. Not that good anymore, right?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/home-alone.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "ML algorithms are *guaranteed* to work when there is a *linear relationship* between the independent variables ($X$) and the dependent variable ($y$), and they may even work on *some* non-linear relationships between $X$ and $y$. But if the non-linearity is *too strong*, they may fail quite dramatically, and you need to work hard to be able to model it. And when there *is no relationship*, as in attempting to predict the future from the past, they are *guaranteed to fail*.\n",
    "\n",
    "In other words, it's what's professor told you:\n",
    "\n",
    ">Machine Learning is (linear) function approximation theory\n",
    "\n",
    "And that is what your brain does as well! Your brain's reasoning nucleus is made out of high-dimensional *surfaces* that capture the *models* that you've built to subsume your life'e *experiences*. But like our [Shar-pei](https://en.wikipedia.org/wiki/Shar_Pei) data above, your models may *fail* you when the experience becomes highly non-linear! \n",
    "\n",
    "So now that you know how *easy* it is to make mistakes my dear students, you can officially consider yourself *indoctrinated* to Western's civilization [scientific approach]((https://partiallyexaminedlife.com/2015/03/06/science-technology-and-society-ii-j-s-mill-on-scientific-method/)), best described by [John Stuart Mill](https://en.wikipedia.org/wiki/John_Stuart_Mill).\n",
    "\n",
    ">**The Scientific Approach**: In A System of Logic (1843) Mill proposed what has since become the standard description of a scientific explanation, called the Covering Law Model. According to Mill, science is concerned with the discovery of regular patterns in experience (laws), and a scientific explanation of a fact is one that fixes its relationship to such laws. As we gain experience in detecting these laws, we observe that certain features of investigation are more conducive to discovery than others. We might, in other words, propose a law about the discovery of laws â€“ the Scientific Method. This method is, simply, to use inference and inductive reason to create a set of hypotheses, and then to use deductive reason to derive from them likely consequences. We then perform an experiment, and on that basis we eliminate or revise our theories until we arrive at the true explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "Can you change one thing from the problem above to make the prediction *successful*, assuming ***you cannot change the `z` equation***?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/funny-fish.gif\" width=400 />\n",
    "    The End\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
