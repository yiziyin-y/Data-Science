{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Sci Engineering Methods and Tools, Lecture 2 Day 2</div>\n",
    "<div style=\"text-align: right\">Prof. Dino Konstantopoulos, 15 January 2020</div>\n",
    "\n",
    "# Data Wrangling with Pandas and a *practical* introduction to statistics\n",
    "\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/kungfu-panda.png\" width=600 />\n",
    "</center>\n",
    "\n",
    "`pandas` is a Python package providing fast, flexible, and expressive data structures designed to work with relational or labeled (hierarchical) data or both. It is a fundamental high-level building block for doing practical, real world, **scientific data analysis** in Python.\n",
    "\n",
    "![Protein Strucure with Sugar](ipynb.images/LysozymeRock.gif)\n",
    "\n",
    "`pandas` is well suited for:\n",
    "\n",
    "- Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "\n",
    "- Ordered and unordered (not necessarily fixed-frequency) *time series* (1D) data\n",
    "\n",
    "- Arbitrary *matrix* (2 and higher D) data (homogeneously typed or heterogeneous) with row and column labels\n",
    "\n",
    "- Any other form of observational / statistical data sets. The data actually need not be labeled at all to be placed into a pandas data structure\n",
    "\n",
    "Key features are:\n",
    "\n",
    "- Shape mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects\n",
    "\n",
    "- Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the data can be aligned automatically\n",
    "\n",
    "- Intuitive merging and joining of data sets\n",
    "\n",
    "- Flexible reshaping and pivoting of data sets\n",
    "\n",
    "- Robust IO tools for loading data from flat files, Excel files, databases, HDF5, etc.\n",
    "\n",
    "- Build-in statistics and linear regressions, extensible with additional packages (e.g. statsmodels)\n",
    "\n",
    "After this lecture, chances are you will never use Excel functions to analyze data anymore, and instead you will import your excel spreadsheet into a notebook and use pandas & friends for your data analysis :-) (sometimes you might use RStudio).\n",
    "\n",
    "We'll do some simple statistics in this notebook. Do not be afraid if the statistics are not familiar to you, we will do a much better introduction to statistics *next week*. This notebook is to introduce `pandas`.\n",
    "\n",
    "The most important data structures in `pandas` are 1D **Series** (vectors) and 2D **dataframes** (matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to pandas *Series*\n",
    "\n",
    "`pandas` Series are **vectors**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Series Lab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.Series([10,20,30,40,50])\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see how the series automatically gets its own index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify an index that we pick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.Series([10,20,30,40,50], index = ['a', 'b', 'c', 'd', 'e'])\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query data with predicates as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj[obj > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyze NBA games, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = {'Celtics': 3, \"Warriors\": 4}\n",
    "data = pd.Series(nba)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. I think we need more than one dimension to analyze the NBA. Once we increase the number of dimensions, we move on pandas' `DataFrame`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to pandas *DataFrames*\n",
    "\n",
    "`pandas` dataframes are excel spreadsheets, also known mathematically as **matrices**!\n",
    "\n",
    "Let's import data as a dictionary structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba = {\"east\": ['Celtics', \"Cavs\", \"76ers\"], \"west\": [\"Warriors\", \"Lakers\", \"Chicago\"]}\n",
    "nbadf = pd.DataFrame(nba)\n",
    "nbadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But.. does this work?\n",
    "nba = {\"east\": ['Celtics', \"Cavs\"], \"west\": [\"Warriors\", \"Lakers\", \"Chicago\"]}\n",
    "nbadf = pd.DataFrame(nba)\n",
    "nbadf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! What to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..so do this instead, and pandas adds 'None' or 'NaN' where data's missing\n",
    "nbadf = pd.DataFrame.from_dict(nba, orient='index')\n",
    "nbadf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the `east` row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.loc['east']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.ix[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add dictionaries as the data itself, this gives pandas the opportunity to add specified indexes as rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba2 = {\"east\": {'MA': 'Celtics', 'IN': \"Cavs\"}, \"west\": {\"CA\": \"Warriors\", \"CAS\": \"Lakers\", \"IL\": \"Chicago\"}}\n",
    "nbadf = pd.DataFrame(nba2)\n",
    "nbadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reindex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = nbadf.reindex(['MA', 'IN', 'IL', 'CA', 'CAS'])\n",
    "obj2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we work with data independently of indexes? Yes, using **numpy matrices**!\n",
    "\n",
    "Let's use python's `arange` to create a range of numbers, and `reshape` to shape the dimensions of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nba3 = np.arange(0,9).reshape(3,3)\n",
    "nba3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's attach the data to rows and columns to give it a context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3 = pd.DataFrame(nba3, index=['MA', 'NY', 'TX'], columns=['Celtics', 'Knicks', 'Rockets'])\n",
    "nbadf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks.. fake. Let's use random number generator in matrix to make the data look more real!\n",
    "nbadf3 = pd.DataFrame(np.random.randn(3,3), index=['MA', 'NY', 'TX'], columns=['Celtics', 'Knicks', 'Rockets'])\n",
    "nbadf3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we apply math formulas to the data like we do it in excel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula as lambda\n",
    "f = lambda x: x.max() - x.min()\n",
    "nbadf3.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbadf3.apply(f, axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Finance with pandas\n",
    "\n",
    "Finance is cool. You can do all your financial strategizing from within a notebook with the help of pandas! I cannot *not show* you how to do this! Finance is the perfect application for `pandas` (time) **Series**.\n",
    "\n",
    "Let's install a package to read financial data.\n",
    "\n",
    "Always try `conda install` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better do this in an anaconda terminal on windows or bash shell on mac\n",
    "!conda install pandas_datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that does not work, try `pip install`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better do this in an anaconda terminal on windows or bash shell on mac\n",
    "!pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's *use* the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pandas_datareader import data, wb\n",
    "import pandas as pd\n",
    "# the line below is the fix for is_list_like lub\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "\n",
    "import pandas_datareader as web\n",
    "import datetime\n",
    "start = datetime.datetime(2016, 1, 1)\n",
    "end = datetime.datetime(2019, 1, 30)\n",
    "aapl = web.DataReader('AAPL', 'yahoo', start, end)\n",
    "aapl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "vol = aapl['High']\n",
    "vol.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = aapl['Close']\n",
    "close.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the n-th (n=1 is the default) discrete difference along a given axis to find out about gains/losses, using numpy's [diff](https://docs.scipy.org/doc/numpy/reference/generated/numpy.diff.html) API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "returns = np.diff(close)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can take the logarithm to squash big growths. In this case, does not make a big difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "returns = np.diff(np.log(close))\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol.corr(close)\n",
    "# a value of 0.3 essentially means little correlation.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, I can analyze the stock market with pandas, numpy, and matplotlib!\n",
    "\n",
    "Yes, for free! Or you can pay lots of money to Fidelity Investments, who will turn to their programmers to give you the same tools you can use in a python notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Exploratory Data Analysis (EDA)\n",
    "\n",
    "A time series is simply a series of data points ordered in time. In a time series, time is often the independent variable and the goal is usually to make a **forecast** for the future.\n",
    "\n",
    "However, there are other aspects that come into play when dealing with time series. Namely:\n",
    "- Is it **stationary**? Stationarity is an important characteristic of time series. A time series is said to be\n",
    "stationary if its statistical properties do not change over time. In other words, it has\n",
    "constant mean and variance, and covariance is independent of time. We'll study what these concepts represent when we get into statistics. For now just think of them as point estimates of a distribution of numbers. Often, stock prices are ***not a stationary process***, since we might see a growing trend, or\n",
    "its volatility might increase over time (meaning that variance is changing). Ideally, we want to have a stationary time series for modelling. Of course, not all of them are stationary, but we can often make different transformations to make them stationary. [Dickey-Fuller](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test) is the statistical test\n",
    "that we run to determine if a time series is stationary or not. If you Coop for Wall Street or get a job as quant, you'll be running this test *all the time*.\n",
    "\n",
    "\n",
    "- Is the target variable **autocorrelated**? Autocorrelation is the similarity between observations as a function of the\n",
    "time lag between them\n",
    "\n",
    "\n",
    "- Is there a **seasonality**? Seasonality refers to periodic fluctuations. For example, electricity consumption is high\n",
    "during the day and low during night, or online sales increase during Christmas before slowing down again. seasonality can also be derived from an autocorrelation plot if it has a\n",
    "sinusoidal shape. Simply look at the period, and it gives the length of the season\n",
    "\n",
    "First, we import libraries that will be helpful throughout our analysis. \n",
    "\n",
    "Then, we import a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('data/stock_prices_sample.csv', index_col=['DATE'], parse_dates=['DATE'])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we only want end of day (EOD) information, and no `GEF` or `Intraday` tickers.\n",
    "\n",
    "So let's clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.TICKER != 'GEF']\n",
    "data = data[data.TYPE != 'Intraday']\n",
    "drop_cols = ['SPLIT_RATIO', 'EX_DIVIDEND', 'ADJ_FACTOR', 'ADJ_VOLUME', 'ADJ_CLOSE', 'ADJ_LOW', \n",
    "             'ADJ_HIGH', 'ADJ_OPEN', 'VOLUME', 'FREQUENCY', 'TYPE', 'FIGI']\n",
    "\n",
    "data.drop(drop_cols, axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the closing price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot closing price\n",
    "\n",
    "plt.figure(figsize=(17, 8))\n",
    "plt.plot(data.CLOSE)\n",
    "plt.title('Closing price of New Germany Fund Inc (GF)')\n",
    "plt.ylabel('Closing price ($)')\n",
    "plt.xlabel('Trading day')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, you see that this is not a stationary process, and it is hard to tell if there is some\n",
    "kind of seasonality.\n",
    "\n",
    "### Moving average\n",
    "The moving average model is probably the most naive approach to time series\n",
    "modelling. This model simply states that the next observation is the mean of all past\n",
    "observations.\n",
    "\n",
    "Although simple, this model might be surprisingly good and it represents a good starting\n",
    "point. \n",
    "\n",
    "Otherwise, the moving average can be used to identify interesting trends in the data. We\n",
    "can define a window to apply the moving average model to smooth the time series, and\n",
    "highlight different trends.\n",
    "\n",
    "Let’s use the moving average model to smooth our time series. For that, we will use a\n",
    "helper function that will run the moving average model on a specified time window and\n",
    "it will plot the result smoothed curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_moving_average(series, window, plot_intervals=False, scale=1.96):\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    " \n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.title('Moving average\\n window size = {}'.format(window))\n",
    "    plt.plot(rolling_mean, 'g', label='Rolling mean trend')\n",
    "\n",
    "    #Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bound = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bound = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bound, 'r--', label='Upper bound / Lower bound')\n",
    "        plt.plot(lower_bound, 'r--')\n",
    "\n",
    "    plt.plot(series[window:], label='Actual values')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "\n",
    "#Smooth by the previous 5 days (by week)\n",
    "plot_moving_average(data.CLOSE, 5)\n",
    "\n",
    "#Smooth by the previous month (30 days)\n",
    "#plot_moving_average(data.CLOSE, 30)\n",
    "\n",
    "#Smooth by previous quarter (90 days)\n",
    "#plot_moving_average(data.CLOSE, 90, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can hardly see a trend, because it is too close to actual curve. Let’s see\n",
    "the result of smoothing by the previous month, and previous quarter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(data.CLOSE, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(data.CLOSE, 90, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trends are easier to spot now. Notice how the 30-day and 90-day trend show a\n",
    "downward curve at the end. This might mean that the stock is likely to go down in the\n",
    "following days.\n",
    "\n",
    "### Exponential smoothing\n",
    "**Exponential smoothing** uses a similar logic to moving average, but this time, a different\n",
    "decreasing weight is assigned to each observations. In other words, less importance is\n",
    "given to observations as we move further from the present (very old observations become less important).\n",
    "\n",
    "**Double exponential smoothing** is used when there is a trend in the time series. In that\n",
    "case, we use this technique, which is simply a recursive use of exponential smoothing\n",
    "twice.\n",
    "\n",
    "**Triple exponential smoothing** extends double exponential smoothing, by adding a seasonal smoothing\n",
    "factor. Of course, this is useful if you notice seasonality in your time series.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/exponential-smoothing.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "$\\alpha$ is a smoothing factor that takes values between 0 and 1. It determines how\n",
    "fast the weight decreases for previous observations.\n",
    "\n",
    "$\\beta$ is the trend smoothing factor, and it takes values between 0 and 1.\n",
    "\n",
    "$\\gamma$ is the seasonal smoothing factor and L is the length of the season.\n",
    "\n",
    "Let's do exponential smoothing and use 0.05 and 0.3 as values for the smoothing factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "def plot_exponential_smoothing(series, alphas):\n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing\")\n",
    "    plt.grid(True);\n",
    "        \n",
    "plot_exponential_smoothing(data.CLOSE, [0.05, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, an alpha value of 0.05 smoothed the curve while picking up most of the\n",
    "upward and downward trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "    result = [series[0]]\n",
    "    \n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        \n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return result\n",
    "\n",
    "def plot_double_exponential_smoothing(series, alphas, betas):\n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(series.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_double_exponential_smoothing(data.CLOSE, alphas=[0.9, 0.02], betas=[0.9, 0.02])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "We must turn our series into a **stationary process** in order to\n",
    "model it. Therefore, let’s apply the Dickey-Fuller test to see if it is a stationary process. The Dickey-Fuller test is a point estimate, when it is tiny (say less than 0.01), we can safely say that the time series is stationary. Otherwise, *not*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n",
    "    \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "tsplot(data.CLOSE, lags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Dickey-Fuller test, the time series is non-stationary. \n",
    "\n",
    "Also, looking at the autocorrelation plot, we see that it is very high, and it seems that there is no clear seasonality.\n",
    "\n",
    "Therefore, to get rid of the high autocorrelation and to make the process stationary, let’s take the first difference: We simply subtract the time series from itself with a lag of one day, and we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first difference to remove to make the process stationary\n",
    "data_diff = data.CLOSE - data.CLOSE.shift(1)\n",
    "\n",
    "tsplot(data_diff[1:], lags=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our series is now stationary and we can start modelling! Does all this math make you want to work for Wall Street?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/wolf-wall-street.jpg\" width=400 />\n",
    "</center>\n",
    "\n",
    "### Homework: \n",
    "Study the stationarity of **three** stocks from three different industries. Plot closing prices, model them with either simple or double exponential smoothing, and test to see if they're stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Interactive learning with pandas\n",
    "\n",
    "If you think  that your python professor really sucks, then you can google for good python videos on youtube, view them in a notebook, and do the computations while the instructor is talking!\n",
    "\n",
    "Now that's what I call **accelerated learning**! Here are some cool ones on **Python**, mixing *motivation* with *deep content*.\n",
    "\n",
    "By the way, that is also how scientific papers should be ***written***, with code up on github, active code within the notebook in cells that demonstrate your basic proofs, and detailed experiment setup up on youtube videos, referenced in the paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('T5pRlIbr6gg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('uYjRzbP5aZs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('7lmCu8wz8ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here for **pandas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('B42n3Pc-N2A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('vmEHCJofslg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's one if you like, *like me*, to sing karaoke in chinese (也 你 爱  韩红）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('npQSre0yLSI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I add all songs I likee in a python notebook, keep the music and notes underneath (even download python DAW programs and run them in my notebook), and I have myself a personal KTV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data aggregation & pivoting with pandas\n",
    "\n",
    "Here we learn how to project and pivot tabular data with pandas.\n",
    "\n",
    "Let's load some gaming data and do *very basic* statistical analysis. This should remind you of similar operations with did in out **R** lab.\n",
    "\n",
    "This is a famous dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pokemon_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reindex by sorting on `Type 1` and `HP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values(['Type 1', 'HP'])\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column that is the **sum** of columns `HP` through `Speed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total'] = df.iloc[:, 4:10].sum(axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rearrange columns to move our `Total` column (the *last* one) closer to the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "df = df[cols[0:4] + [cols[-1]]+cols[4:12]]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, better than excel!\n",
    "\n",
    "In fact, let's save our data as a spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('data/pokemon00.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter our data for `Type 1` == Grass, `Type 2` == Poison, and `HP` > 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.loc[(df['Type 1'] == 'Grass') & (df['Type 2'] == 'Poison') & (df['HP'] > 70)]\n",
    "df3.reset_index(drop=True, inplace=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do aggregate statistics with `pandas`' GROUP_BY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.groupby(['Type 1', 'Type 2']).count()\n",
    "df4.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see, for example, that there are 12 rows of `Type 1` == Bug, `Type 2` == Poison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.groupby(['Type 1', 'Type 2']).sum()\n",
    "df4.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.groupby(['Type 1', 'Type 2']).mean()\n",
    "df4.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the number of columns to just `HP`, `Attack`, and `Defense`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.head(20)[cols[4:7]]\n",
    "df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot (Wow! Plotting is built into `pandas`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a **lambda function** to compute a formula for each row that takes into account column-level statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "df5.apply(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Statistics with pandas & friends\n",
    "\n",
    "`statsmodels` is a cool statistics library that also includes sample datasets for us to play with. Now *that's* accelerated learning!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/sunspots.jpg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "data_loader = sm.datasets.sunspots.load_pandas()\n",
    "df = data_loader.data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's sunspots data, from the 18th century to.. 2008! in fact, [Galileo](https://en.wikipedia.org/wiki/Galileo_Galilei) first started documenting sunspots in the 1600s, using his newly invented telescope, and reliable sunspot observations begin in about 1700.\n",
    "\n",
    "If you look closer at the dataset, you will find fractional years. When did fractional years start appearing? Take the modulo of each value with 1 to get the fractional part (lambda x: x % 1), using the `.apply()` API on the `df['SUNACTIVITY']` column. Then take the fractional part and mltiply by 1.2 to get months!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractional_nums = df['SUNACTIVITY'].apply(lambda x: x % 1) #Take the modulo of each value with 1 to get the fractional part\n",
    "fractional_nums[fractional_nums > 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a sense of the [distribution](https://en.wikipedia.org/wiki/Probability_distribution) of the values.\n",
    "\n",
    ">**DEFINITION**: A **probability distribution** is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment. It is a description of a random phenomenon in terms of the probabilities of events. Random phenomena may not be as random as you think, as we will see this semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SUNACTIVITY'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='YEAR', y='SUNACTIVITY', xlim=(1700,2008))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's a **seasonality**! In other words, a cycle of sorts (increase --> decrease --> increase --> decrease --> etc.). What is its [period](https://en.wikipedia.org/wiki/Periodic_function) (or [frequency](https://en.wikipedia.org/wiki/Frequency))?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.tools.plotting.autocorrelation_plot(df['SUNACTIVITY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation clearly shows that there are two periods in the time series: A short one, and a longer one.\n",
    "\n",
    "Let's find these periods using some... math magic. This is just to show off the power of math algorithms, which are all contained in one neat package called `SciPy`, and which we leverage with the help of a few otehr tools (`numpy`, `statsmodels`, `matplotlib`, ..).\n",
    "\n",
    "We'll talk more at length about SciPy, the most beautiful of python libraries, in future notebooks.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/will-smith-aladin.jpg\" width=600 />\n",
    "Will Smith, a.k.a Alladin, masquerading as SciPy\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "N2 = df.shape[0] / 2\n",
    "freqs = np.linspace(0, 0.5, num=N2, endpoint=False)[1:] #Nyquist range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "periodogram = sp.signal.lombscargle(df['YEAR'], df['SUNACTIVITY'], freqs * 2 * np.pi)\n",
    "plt.plot(freqs, periodogram, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_index_at_max_power = np.argmax(periodogram)\n",
    "print('Frequency and corresponding time in years at max power: %.2f, %.1f' % (freqs[freq_index_at_max_power], 1 / freqs[freq_index_at_max_power]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the major cycle is about 11 years, and the minor cycle at a just over a month.\n",
    "\n",
    "Wow, **autocorrelation** is really cool. It allows us to find *repeating* **patterns** in data. In fact, it's an underlying paradigm in Machine Learning: \n",
    "\n",
    ">**Paradigm of Machine Learning**: Machine learning is function *approximation*. A *function* is a dimensionality reduction of *data*. ***Intelligence*** is how many functions you can build in your lifetime.\n",
    "\n",
    "Can we find any correlations in our gaming data above (not neceassarily *auto*, but of one column against another)? It all starts with a *plot*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/pokemon_data.csv')\n",
    "attack = df['Attack']\n",
    "defense = df['Defense']\n",
    "plt.plot(attack, defense, 'g+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about for really good players?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g = df.loc[(df['Speed'] > 60) & (df['HP'] > 70)]\n",
    "df_g.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_g = df_g['Attack']\n",
    "defense_g = df_g['Defense']\n",
    "plt.plot(attack, defense, 'r.', label='all')\n",
    "plt.plot(attack_g, defense_g, 'b.', label='good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Outliers](https://en.wikipedia.org/wiki/Outlier) can really ***skew*** our data. \n",
    "\n",
    "In statistics, an outlier is a data point that differs significantly from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error.\n",
    "\n",
    "We need a way to find out if there are outliers in our data, remove them, and then look again at our correlations!\n",
    "\n",
    "Outliers are often taken to be any data points that are ***two standard deviations*** removed from the **mean**.\n",
    "\n",
    "Let's look at outliers for different columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of original df\n",
    "newdf = df.copy()\n",
    "\n",
    "newdf['x-Mean'] = abs(newdf['Attack'] - newdf['Attack'].mean())\n",
    "newdf['1.96*std'] = 1.96*newdf['Attack'].std()  \n",
    "newdf['Outlier'] = abs(newdf['Attack'] - newdf['Attack'].mean()) > 1.96*newdf['Attack'].std()\n",
    "newdf.loc[(newdf['Outlier'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.loc[(newdf['Outlier'] == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prettypandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettypandas import PrettyPandas\n",
    "PrettyPandas(newdf                                   # Prettyprints pandas dataframes\n",
    "    .head(10)                                    # Sample the first 10 rows\n",
    "    [[c for c in newdf.columns if \"Sp.\" not in c]] # Ignore columns with \"Sp.\" in the name (don't know what it means)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrettyPandas(newdf[newdf[\"Attack\"]>=180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of original df\n",
    "newdf2 = df.copy()\n",
    "\n",
    "Attack = newdf.groupby('Attack')\n",
    "\n",
    "newdf['Outlier'] = Attack.transform( lambda x: abs(x-x.mean()) > 1.96*x.std() )\n",
    "newdf['x-Mean'] = Attack.transform( lambda x: abs(x-x.mean()) )\n",
    "newdf['1.96*std'] = Attack.transform( lambda x: 1.96*x.std() )\n",
    "newdf.loc[(newdf['Outlier'] == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But Average and Standard Deviation are really only valid for [gaussian distributions](https://en.wikipedia.org/wiki/Normal_distribution) (also called *normal* distributions), as we will see later on in class.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of original df\n",
    "newdf3 = df.copy()\n",
    "\n",
    "Attack = newdf3.groupby('Attack')\n",
    "\n",
    "newdf3['Lower'] = Attack['Attack'].transform( lambda x: x.quantile(q=.25) - (1.5*(x.quantile(q=.75)-x.quantile(q=.25))) )\n",
    "newdf3['Upper'] = Attack['Attack'].transform( lambda x: x.quantile(q=.75) + (1.5*(x.quantile(q=.75)-x.quantile(q=.25))) )\n",
    "newdf3['Outlier'] = (newdf3['Attack'] < newdf3['Lower']) | (newdf3['Attack'] > newdf3['Upper']) \n",
    "newdf3.loc[(newdf3['Outlier'] == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "[Histograms](https://en.wikipedia.org/wiki/Histogram) are essential to data science, because understanding *how* your data is distributed is **key** to being able to fit it to a **model**.\n",
    "\n",
    "[Here](https://www.mathsisfun.com/data/quincunx.html) is how histograms are built.\n",
    "\n",
    ">**DEFINITION**: A **model** for your data is a function that ***autoencodes your data*** (possibly as an [approximation](https://en.wikipedia.org/wiki/Approximation_theory)). In other words, it is able to learn all your data ***and*** be able to output your data by remembering a much smaller amount of data. That way, you ***keep your model and you can throw away the data***.\n",
    "\n",
    "Your brain is, in fact, a ***model builder***. It learns all your past interactions between you and your parents, brother, husband, best friend, etc., and enables you to sidestep all future conflict (well, for some of us, our brain does *not* work that well ;-). How does it do that? Given the input of a potentially conflict-causing conditions, it is able to ***predict*** the future conflict, and thus devise strategies to avoid the conflict. *That* is why humans have conquered the planet. They are ***best*** at avoiding conflict.\n",
    "\n",
    "What's your most basic example of a ***model***? We talked about this... Generators!\n",
    "\n",
    "So, let's plot histograms. `seaborn` is the \\#2 visualization library, after `matplotlib`, but it has a great histogram API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(10,3),sharey=True,sharex=True)\n",
    "sns.distplot(newdf3[\"Attack\"], ax=ax1,\n",
    "             bins=range(newdf3[\"Attack\"].min(), newdf3[\"Attack\"].max()),\n",
    "             kde=False,\n",
    "             color=\"b\")\n",
    "ax1.set_title(\"Attack\")\n",
    "sns.distplot(newdf3[\"Defense\"], ax=ax2,\n",
    "             bins=range(newdf3[\"Defense\"].min(), newdf3[\"Defense\"].max()),\n",
    "             kde=False,\n",
    "             color=\"r\")\n",
    "ax2.set_title(\"Defense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even merge the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(nrows=1,figsize=(10,6),sharex=True)\n",
    "\n",
    "sns.distplot(newdf3[\"Attack\"], ax=ax1,\n",
    "             bins=range(newdf3[\"Attack\"].min(), newdf3[\"Attack\"].max()),\n",
    "             kde=False,\n",
    "             color=\"b\",\n",
    "             label=\"Attack\")\n",
    "sns.distplot(newdf3[\"Defense\"], ax=ax1,\n",
    "             bins=range(newdf3[\"Defense\"].min(), newdf3[\"Defense\"].max()),\n",
    "             kde=False,\n",
    "             color=\"r\",\n",
    "             label=\"Defense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Matrix visualizations with pandas & friends\n",
    "\n",
    "`Clustergrammer` is the ***coolest*** web-based tool for visualizing high-dimensional data (e.g. a matrix) as an interactive and shareable hierarchically clustered heatmap. I think a plot is worth a thousand tables, and so I wanted to show you what the tool is.\n",
    "\n",
    "Developed by the Ma'ayan lab at the Icahn School of Medicine In New York City. Clustergrammer's front end (Clustergrammer-JS) is built using [D3.js](https://d3js.org/) and its back-end (Clustergrammer-PY) is built using Python. Clustergrammer produces highly interactive visualizations that enable intuitive exploration of high-dimensional data and has several biology-specific features to facilitate the exploration of gene-level biological data.\n",
    "\n",
    "Fernandez, N. F. et al. Clustergrammer, a web-based heatmap visualization and analysis tool for high-dimensional biological data. Sci. Data 4:170151 doi: [10.1038/sdata.2017.151 (2017)](https://www.nature.com/articles/sdata2017151)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clustergrammer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py --sys-prefix clustergrammer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import widget classes and instantiate Network instance\n",
    "from clustergrammer_widget import *\n",
    "net = Network(clustergrammer_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import widget classes and instantiate Network instance\n",
    "from clustergrammer_widget import *\n",
    "net = Network(clustergrammer_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create random data using `numpy`'s `random` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate random matrix\n",
    "num_rows = 10\n",
    "num_cols = 10\n",
    "np.random.seed(seed=100)\n",
    "mat = np.random.rand(num_rows, num_cols)\n",
    "\n",
    "# make row and col labels\n",
    "rows = range(num_rows)\n",
    "cols = range(num_cols)\n",
    "rows = [str(i) for i in rows]\n",
    "cols = [str(i) for i in cols]\n",
    "\n",
    "# make pandas dataframe \n",
    "df6 = pd.DataFrame(data=mat, columns=cols, index=rows)\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_df(df6)\n",
    "net.cluster(enrichrgram=False)\n",
    "net.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matrix file\n",
    "net.load_file('data/rc_two_cats.txt')\n",
    "\n",
    "# cluster using default parameters\n",
    "net.cluster(enrichrgram=True)\n",
    "\n",
    "# make interactive widget\n",
    "net.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Machine Learning with pandas & friends\n",
    "\n",
    "Here we show an example of how well `Scikit-learn` and `pandas` integrate to do Machine Learning (`Scikit-learn`) from data (`pandas`). Don't worry about the algorithm yet ([random forest](https://en.wikipedia.org/wiki/Random_forest)), or Machine Learning (ML) in general. We'll study all this  in class. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/pretty-forest.jpeg\" width=600 />\n",
    "</center>\n",
    "\n",
    "But random forests are so cool. In opinion, the most generally successful algorithm in ML. Invented in the 1950s (as a **decision tree** algorithm) and optimized using [ensembling](https://en.wikipedia.org/wiki/Ensemble_learning) in modern libraries like `Scikit-Learn`. Just be amazed ;-)\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/minions-amazed.jpg\" width=300 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in meterological data as a `pandas` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nyc = pd.read_csv('data/central-park-raw.csv', parse_dates=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at our data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit of EDA, we decide to do some data *healing* and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data transformations\n",
    "nyc.columns = [x.strip() for x in nyc.columns]\n",
    "nyc.columns = [x.replace(' ', '_') for x in nyc.columns]\n",
    "nyc.PrecipitationIn.replace(\"T\", '0.001')\n",
    "nyc.PrecipitationIn = pd.to_numeric(nyc.PrecipitationIn.replace(\"T\", '0.001'))\n",
    "nyc['Events'] = nyc.Events.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` gives us easy integration with the `sklearn` library. \n",
    "\n",
    "Let's see if we can predict humidity (y) from the other columns (X).\n",
    "\n",
    "Any column we want to *predict* is a ***y*** (*dependent* variable). The coluumns that help us predict it are ***x***'s (*independent* variables). And I use a capital ***X*** to denote that there are many such columns.\n",
    "\n",
    "We will train a Random Forest with a sample of our data, then test it with another sample to see how it performs. Let's import the `Scikit-learn` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the attributes (columns) of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " nyc.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can make mean **humidity** the *dependent variable*, and predict it using other columns as *independent variables*. \n",
    "\n",
    "Let's look at mean humidity and also shift mean humidity shifted by one row ***up***, so that we may use information from the **previous day** to predict the ***next day***'s mean humidity value (isn't that what meteorologists try to do?). Indeed, each row in our spreadsheet represents meteorological data for ***one day***.\n",
    "\n",
    "`pandas` has a very neat API to do this. This is what a row-up-shifted column looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Shift Humidity down to predict next day\n",
    "pd.concat([nyc.Mean_Humidity, nyc.Mean_Humidity.shift(-1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the up-shifted column as training data and test data!\n",
    "\n",
    "Notice that the `Events` column is **categorical** (a set of different string values), so we need to turn it into **numbers**. Pandas has a great API for this: `get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make \"Dummy\" variables from Events column\n",
    "nyc_dummy = pd.get_dummies(nyc, columns=['Events'])\n",
    "nyc_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_dummy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our **training** *independent variables* (X_train) and **test** *independent variables* (X_test), and our **training* *dependent variable* (y_train) and **test** *dependent variable* (y_test). Notice how I use an uppercase X to denote that X is multi-dimensional, while I use a lowercase y to denote that y is one-dimensional.\n",
    "\n",
    "We need to remove the timestamp from the training data, as it is not relevant to our model. Each different timestamped row is just another ***observation***. We also need to disregard the `Events_` column. Where did that come from?\n",
    "\n",
    "We also need to drop `N/A`s from all cells.\n",
    "\n",
    "All that is called **EDA**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove NA\n",
    "# Regression - Try to predict Mean_Humidity (y) from non humidity columns (X)\n",
    "# Get training set (X_train)\n",
    "def valid(col):\n",
    "    return 'Humid' not in col and 'EST' not in col and col != 'Events_'\n",
    "nyc_dummy = nyc_dummy.dropna()\n",
    "X = nyc_dummy[[x for x in nyc_dummy.columns if valid(x)]].iloc[1:]\n",
    "y = nyc_dummy.Mean_Humidity.shift(-1).dropna()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a ML model, using `RandomForestRegressor` from the `sk-learn` package. Do not worry if you don't understand how this is done. You will soon enough! \n",
    "\n",
    "That's *one* line of code in `Scikit-learn`.\n",
    "\n",
    "Then, we fit the model to the training data (that's the ***training step***). That is where we fit a **curve** to the **datapoints**. This is where we do function approximation. We try to find the function that fits through our data.\n",
    "\n",
    "\n",
    "That's *another* line of code in `Scikit-learn`.\n",
    "\n",
    "Here are your two lines of code that do *everything*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model \n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's score our model at how good it is in predicting tomorrow's mean humidity from today's weather variables (all those that *do not* relate to humidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R2 measure (indicator of accuracy 1 is perfect 0 is horrible)\n",
    "rf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, close to 90%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.Series(rf_model.predict(X_test)), y_test.reset_index(\n",
    "drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most important features in predicting mean humidity for tomorrow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(X.columns, rf_model.feature_importances_),\n",
    "        key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Due next week, teams of two. *Must be* different team than from last homework.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/will-smith-aladin-2.jpg\" width=400 />\n",
    "    different :-)\n",
    "</center>\n",
    "\n",
    "You are already looking at financial time series as homework. See *how far* into the future you can predict stock prices for your three chosen stocks, using a random forest from `Scikit-learn`. Assume you want a prediction score above 70%. Then, start investing, and become..\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/wolf-wall-street.jpg\" width=400 />\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
